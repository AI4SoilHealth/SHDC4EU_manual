[
  {
    "objectID": "1-specifications.html",
    "href": "1-specifications.html",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "In a nutshell, Soil Health Data Cube is a building block and infrastructure for monitoring soil health across EU. The general idea of the SHDC4EU is that it serves some standard processes that help inform land owners / land managers about the soil health state of their land using open data. These processes are meant to help future users of the AI4SoilHealth app to quickly find out about the site, explore land potentials based on the state-of-the-art models and start taking concrete actions towards restoring land / improving soil health. This is closely connected with the main expected impacts of the AI4SoilHealth project (to make soil health data easier to access and provide tools and standards for soil health monitorng). In simple terms, the 7–step soil health assessment framework (Hengl 2024) includes:\n\nAssess the history of a site (last 30+ years) provide answers to questions e.g.:\n\nHow much SOC has been gained / lost over the last 30 years?\nWhich chemical and physical soil properties changed the most?\nIs there any land/soil degradation and how significant is it?\nHow has agronomic management impacted soil health?\nHow much did the land cover / vegetation cover change? What is the main change class?\nHow much did the landscape heterogeneity change over time?\n\nDetermine current state of soil (actual year):\n\nWhat is the current state of physical and chemical soil properties?\nWhat is the current (WRB) soil type / main soil forming processes and soil states?\nWhat are the current derived soil properties?\nWhat are the current macronutrient stocks in the soil?\n\nDetermine soil potential (next 10, 20, 30 yrs):\n\nWhat is the SOC sequestration potential of this soil?\nWhat is the agricultural potential of this soil?\nWhat are the other potential ecosystem functions provided some land use change or mitigations?\n\nCollect extra soil samples, send to lab, and add to training points:\n\nGenerate a sampling design that helps increase mapping accuracy. Optimize the number of samples while taking into account the costs of sampling.\nCollect new samples and send them to the lab.\n\nImport new local data then re-analyze and re-asses points 1, 2 and 3.\n\nImport new local samples / results and add to the training points. Re-assess local prediction accuracy.\nRepredict layers in SHDC4EU for steps #1, #2 and #3.\n\nSuggest concrete KPI’s (per farm / per land unit):\n\nProvided that soil potential assessment reveals a significant gap between potential and actual.\nEither directly provide concrete measures to help increase soil health and/or provide links to documents / organizations that can directly help increase soil health.\n\nTrack progress per farm and re-assess if required:\n\nEvery 1–2 years update the indicators (pan-EU).\nCompare with the planned transition in #6. If necessary recommend re-assessment.\n\n\nThis framework can be either apply to assess health of a specific site (either in the field using a mobile phone app, or in the office using a desktop app), or complete farms / administrative units. We, thus, aim to complete the data cube with primary and derived soil variables that can directly serve this framework, and especially so that can be worked alongside other Work Packages in the project in their testing of methodology.\n\n\n\nWP5 — with the tasks T5.2 Development of AI4SoilHealth computing engine and T5.3 Development of AI4SoilHealth Indicators data cube) — aims at implementing fully automated predictive soil mapping at high spatial resolution using large datasets (e.g. whole of Europe at 30-m spatial resolution). Prepared harmonized point datasets will are overlaid against time-series of EO images and/or static terrain-based or similar indices. These are then be used to predict values of target variables in 2D, 3D and 2D+T, 3D+T. The general workflow for predictive soil mapping based on Spatiotemporal Machine Learning and High Performance Computing (HPC) and assumes that all point data is analysis-ready, representative and correlates with the EO / covariate layers i.e. can be used to produce usable spatial predictions. Automated predictive soil mapping is currently implemented via the Python library scikit-map using OpenGeoHub’s High Performance Computing infrastructure.\nThe key interest of WP5 is to produce pan-EU predictions at the highest possible spatial resolution and that can directly serve soil health assessment at continental scale. Some variables are, however, not available for the whole of pan-EU, and some point datasets will be produced experimentally and used for testing purposes only. Hence, within the WP5, all soil variables are modeled / mapped under one the following 3 tiers:\n\nTier 1: Pan-EU, production ready variables: usually based on LUCAS + national soil datasets;\nTier 2 (internal, under construction): National / in-situ data: Tested locally, then upscaled to whole of EU; usually based on collaboration of WP3,4,5,6;\nTier 3 (internal, under construction): Pan-EU, new experimental variables (currently not in the LUCAS soil and most likely can not be modeled/mapped across EU, but can only be used to assess soil health at site); based on the collaboration of WP3,4,5;\n\nWe expect that project partners + pilots will feed data for Tier 2 and Tier 3. However, not everything can be mapped across the EU, hence some variables will eventually stay available only locally and a small selection of variables will be available in-situ only i.e. these will not be mapped at all.\n\n\n\nQuality of the outputs would most likely be dictated by the following four key aspects of modeling:\n\nThe general modeling design i.e. how appropriate the statistical / machine learning methods are. For example: are all casualties considered, are all distributions represented, are all statistical assumptions met?\nPredictive performance of the algorithms used i.e. is the best / fastest algorithm used for modeling? Is the algorithm robust, noise-proof, artifacts-proof etc?\nQuality and diversity of the covariate layers used.\nQuality and spatial and feature space representation of training points.\n\nAssuming that #1 and #2 are optimized, then the only remaining factors that control success of the modeling / mapping processes are aspects #3 and #4 i.e. the quality of covariate layers and quality of training points. In the past (prior to 2010), it was relatively difficult to produce pan-EU maps of soil health indicators as there was only limited training point data. Thanks to the European Commission’s LUCAS soil project, we now have 3–4 repetitions of ground measurements of soil properties of highest quality (about 22,000 sites are revisited per period) (see figure below). This is a unique opportunity to test building high resolution predictions, including dynamic soil property predictions. Ideally, such testing should be a joint effort of the AI4SoilHealth consortium, in an open collaboration with JRC and members of other sister projects funded by the same Soil mission.\nNevertheless, LUCAS soil is also not an ideal dataset for predictive mapping. Hence producing SHDC4EU faces two serious challanges: (1) LUCAS soil is a top-soil dataset, although there is now intention to sample also subsoils, (2) it covers only a number of physical and chemical soil variables (10–15), and soil types or similar are typically not recorded, making this largely a partial pedological survey.\n\n\n\n\n\nLUCAS soil samples (Orgiazzi et al. 2018) and connected existing and upcoming EO missions (bars indicate approximated temporal coverage). Note that the amount of EO data and missions is increasing exponentially.\n\n\n\n\nTo improve usability of predictions produced using LUCAS we have decided, in this project, to combine both LUCAS and the highest quality legacy national soil datasets. These synchronized and analysis ready fusions of soil laboratory points and observations will be generated by T4.6 “Integration and harmonization of in-situ and ancillary observations”. We anticipate that this will be a long process hence, in order to prevent serious delays, we will start producing SHDC4EU predictions, then in each iteration try to improve predictions as new countries join the campaign of contributing their data (under a standard Data Sharing Agreement) for the purpose of data mining i.e. producing open soil information for everyone.\n\n\n\nThe second important aspect that will determine the quality of the SHDC4EU outputs is the quality of covariate layers. Quality of the covariate layers is basically determined by three things: (1) spatial and temporal resolution, (2) spectral / thematic content, (3) general quality of data in terms of completeness, consistency, correctness and quality of metadata. For producing SHDC4EU we plan to use an extensive compilation of covariate (base) layers to produce predictions for SHDC4EU. Already available layers for continental Europe include (https://EcoDataCube.eu):\n\nLand mask (pan-EU: https://zenodo.org/doi/10.5281/zenodo.8171860) + World Settlement Footprint (WSF) i.e. world buildings 2000-2015, 2019 (https://geoservice.dlr.de/web/maps/eoc:wsfevolution and https://geoservice.dlr.de/web/maps/eoc:wsf2019);\nDigital Terrain Model variables at 6–scales 30-m, 60, 120, 240, 480 and 960-m:\n\nElevation, slope (%), min- max-curvature,\nHillshade, northness, easterness,\nPositive, negative openness,\nTopidx (TWI), geomorphon classes (10), catchment area, LS factor,\n\nLithological (surface geology) map of pan-EU at 250-m (https://zenodo.org/doi/10.5281/zenodo.4787631);\nBimonthly / quarterly GLAD Landsat composites (all bands + biophysical indices, 2000–2022) (explained in: https://doi.org/10.21203/rs.3.rs-4251113/v1):\n\nGreen, Red, NIR, SWIR1, SWIR2,\nNDVI, FAPAR,\nNDTI, NDWI, BSF,\nCumulative NDVI, NDTI and BSF;\n\nClimatic variables at 1-km (long-term or monthly series):\n\nCHELSA Climate Bioclimatic variables (https://chelsa-climate.org/bioclim/);\nCHELSA monthly precipitation time-series 2000–2022 (https://chelsa-climate.org/timeseries/);\nMODIS LST daytime and nighttime monthly time-series 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.1420114);\nMODIS monthly water vapor time-series 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.8193738);\nCumulative annual precipitation (2000–2022);\n\nMonthly / annual snow images 2000-2022 e.g. DLR Snow pack at 500-m (https://geoservice.dlr.de/web/maps/eoc:gsp:yearly):\n\nLong-term monthly snow probability time-series at 500-m (https://doi.org/10.5281/zenodo.5774953);\nCumulative annual snow probability 2000–2022;\n\nSurface water dynamics: occurrence probability of water long-term 1999–2021 (https://glad.umd.edu/dataset/global-surface-water-dynamics);\nOptional: Global Flood Database v1 (2000-2018) at 250-m annual flood event (http://global-flood-database.cloudtostreet.info/);\nLight at night time-series at 500-m resolution 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.7750174);\nCropland extent at 30-m for 2000 to 2022 based on (https://glad.umd.edu/dataset/croplands);\nBare soil percent and photosythetical vegetation percent annual for 2000 to 2022 based on the MODIS MCD43A4 product (500-m spatial resolution);\n\nIn addition to the existing layers above, we will also generate a number of novel layers tailored specifically for the purpose of representing land use practices and potential soil degradation factors. This include:\n\nBSI, Bare Soil Index (Mzid et al. 2021) and annual BEF, Bare Earth Fraction (e.g. proportion of pixels with NDVI &lt;0.35 based on 16–day and/or bimonthly data; there are other possible thresholds combination that can be used to optimize the results),\nNDTI, Normalized Differential Tillage Index (Ettehadi Osgouei et al. 2019),\nNOS, Number of Seasons / NOCC,** Number of Cropping Cycles**,\nLOS, Length of Seasons / CDR, Crop Duration Ratio (Estel et al. 2016),\nCrop-type based on EuroCrops dataset,\n\nNote that from all covariate layers listed above, 16-day / bimonthly / quarterly GLAD Landsat composites (all bands + all indices, 2000–2022) are the largest part of the data to be used taking almost 20TB of storage (in compressed format).\n\n\n\nBased on the availability of the training points and nature of the target variable in the SHDC4EU, we will either predict or derive soil variables from primary variables. The output soil health indicators can, thus, be considered of type either:\n\nPredicted dynamic pan-EU variables (2000–2022+) available at standard depth intervals (0–20, 20–50, 50–100, 100–200 cm).\nPredicted static pan-EU variables.\nPredicted and/or simulated pan-EU variables i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling.\nDerived pan-EU variables and indices.\nIn-situ only variables.\n\nMost of the soil chemical and biological variables are mapped as dynamic variables with annual or 5–year estimates (e.g. 5 maps for period 2000–2025) and at multiple depths (Witjes et al. 2023). This means that the amount of produced data can be significant if predictions are also provided per depth. For example, to map soil organic carbon content (weight %), we can predict 23 annual maps at 4–5 standard depths resulting in over 180 images (assuming that we also produce prediction errors per pixel). AI4SoilHealth tasks T5.5 “Development of AI4SoilHealth present and future soil degradation products”, T5.6 “Development of AI4SoilHealth forecasting services”, and T5.7 “Development of AI4SoilHealth soil functions evidence / model chains” will also generate large amounts of predicted and/or simulated pan-EU variables i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling.\nThe following predictive soil modeling methods are considered for use so far for generating SHDC4EU:\n\n2D and 3D predictive soil mapping (static predictions, time is ignored).\n2D+T predictive soil mapping spacetime.\n3D+T predictive soil mapping spacetime.\nPredictive soil mapping focused on long-term indices.\nDerivation from primary data.\nDerivation using Pedo-transfer functions.\nDerivation / simulations using Mechanistic model (iterative).\nDerivation of future projected predictions (scenario testing).\n\nFor each derivation method we use sets of algorithms, usually implemented in python or R programming languages. These are documented in detail and eventually allow for complete reproducibility of results; most importantly we do a series of benchmarking (listed in further sections) to compare predictive performance and select the algorithm that is most accurate + most robust at the same time. In the tasks T5.5 Development of AI4SoilHealth present and future soil degradation products, T5.6 Development of AI4SoilHealth forecasting services and T5.7 Development of AI4SoilHealth soil functions evidence / model chains, also process-based modeling can be used, especially to generate potential soil ecosystem services etc. Such modeling is at the order-of-magnitude more computationally demanding than predictive mapping hence we expect that these outputs will be of limited spatial detail (1-km) and or available only for experimental testing.\n\n\n\nThe general objective of the SHDC4EU is to serve best possible, most detailed, complete and consistent predictions of the the number of targeted soil health indicators in the EU Mission’s “Implementation Plan: A Soil Deal for Europe”:\n\npresence of pollutants, excess nutrients and salts,\nsoil organic carbon stock,\nsoil structure including soil bulk density and absence of soil sealing and erosion,\nsoil biodiversity,\nsoil nutrients and acidity (pH),\nvegetation cover,\nlandscape heterogeneity,\nforest cover.\n\nIf these are available for the complete land mask of pan-EU area, these can then be used to assess soil degradation state (e.g. salinization / sealing level and trends, concentration of excess nutrients and pollutants and trends, erosion state and trends, loss of SOC and trends etc), current soil properties and soil potential in terms of potential ecosystem services / potential SOC sequestration etc, potential productivity of soil, potential soil biodiversity etc. The working version of what we find as feasible to map at high spatial resolution is provided below.\nNote that some of the soil health indicators recommended by the European Commission / JRC, are not defacto soil variables (e.g. vegetation cover, landscape heterogeneity, forest cover) but will be generated in this project and integrated into SHDC4EU. Some potential options to represent the vegetation cover, landscape heterogeneity etc include:\n\nCanopy height, data already available for EU but could also be improved by outputs from the Open-Earth-Monitor project,\nGPP, Gross Primary Productivity (bimonthly and/or annual; based on the last 2–5yrs),\nLand cover and land use / cropping systems and applications (categories e.g. based on the EuroCrops), producing cropping system maps for the EU;\n\nHowever, producing cropping system maps for the EU, even only for the recent year is not trivial and producing such data should is at the moment optional.\nList of target soil variables that will be delivered in the SHDC4EU includes dynamic soil properties covering period 2000–2025+:\n\nSoil organic carbon density (kg/m3) ISO 10694. 1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year estimates;\nSoil carbon stock change (t/ha/yr) ISO 10694. 1996 for 0–20, 20–50, 50–100 cm depth intervals; long-term\nSoil pH in a suspension of soil in water (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 depth intervals, 5–year estimates;\nSoil pH measured in a CaCl2 solution (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil total nitrogen content (dg/kg) ISO 11261:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil bulk density (t/m3) Adapted ISO 11272:2017 for 0–20, 20–50, 50–100 cm, 5–year;\nSoil texture fractions (sand, silt, clay) (g/g) ISO:11277 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil WRB subgroup (factor / probs) based on the WRB2022 classificaton system for 0–200 cm, long-term;\nDepth to bedrock (cm) up to 200 cm, long-term;\nExtractable potassium content (mg/kg) USDA−NRCS, 2004 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nCarbonates content CaCO3 (g/g) ISO 10693:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nExtractable Phosphorus content (Olsen) (mg/kg) ISO 11263. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nMonthly Gross Primary Productivity (kg/ha/yr) FluxNET bi-monthly;\nBare Soil Fraction (m2/m2) Landsat-based, 5–year;\n\n\n\n\nFor each variable we will organize a soil variable registry system, so that each variable should have unique (short-as-possible) code. As simple coding system for SHDC4EU is suggested where the variable name is a combination of the three components:\n\ngeneric variable name,\nspecific laboratory / field / O&M method (ISO standard or similar),\nmeasurement unit,\n\nFor example, for SOC content in weight percent we recommend using oc_iso.10694.1995_wpct (dry combustion) or oc_iso.17184.2014_wpct; for SOC in permiles you can use e.g. oc_iso.10694.1995_wpml. Note that here we use ISO code, however since ISO is highly commercial and not-practical for open datasets, we recommend using the ISO code (allowed), but in fact pointing to a scientific reference i.e. a publication or PDF that is at the order of scale easier to obtain (instead of pointing to ISO website or similar). For example the variable oc_iso.10694.1995_wpct can be linked to the scientific reference Nelson et al. (1982) (also used by ISO).\nSoil variable names can be also provided in a shorter version (assuming that only 1 reference method exists) as a combination of the variable name and measurement unit code e.g. oc.wcpt. It is very important that all partners in the project consistently use the same codes, and if there are updates in the naming convention that they refer to the version of the code.\n\n\n\nThe general interest of this project is to produce a time-series of predictions of key soil/vegetation/land use variables with either annual or monthly support and covering the period 1997–2022+ and at highest spatial resolution e.g. 30-m. For variables that vary by depth, we will predict at 5 standard depths (0, 20, 50, 100 and 200 cm), then aggregate values to standard depth intervals:\n\n0–20 cm (topsoil) LUCAS standard;\n20–50 cm (subsoil1);\n50–100 cm (subsoil2);\n100–200 cm (subsoil3);\n\nTemporal support of predictions / simulations can be one of the following:\n\nLong-term primary soil properties and classes: e.g. soil types, soil water holding capacity;\nAnnual to 5–year values of primary soil and vegetation variables: e.g. annual bare soil fraction (index);\nBimonthly, monthly or seasonal values of soil and vegetation variables: e.g. bimonthly GPP.\nWeekly, 16–day values (original Landsat GLAD),\nDaily or hourly values (probably not of interest in this project).\n\nAggregated values of target soil and vegetation variables can also refer to some part of the distribution e.g. quantile e.g. P25, P50 (median / mean value) and P75.\n\n\n\nAs an ideal case, each predictive mapping model should produce / provide also uncertainty per pixel (prediction error) and summary results of robust cross-validation. Two options are possible for predictive error mapping:\n\nProvide lower and upper quantiles p=0.05 and p=0.95 so that a 90% probability prediction interval can be derived. For variables with skewed distribution / log-normally distributed it is recommended that the lower values are computed in the log-transformed space to avoid predicting negative values.\nProvide prediction error as 1 standard deviation: from this also prediction interval can be derived, assuming that the variable is normally distributed.\n\nAs a general recommendation we suggest using the Conformal Prediction method to produce prediction errors per pixel, best as implemented in the python libraries mappie and/or puncc.\n\n\n\nThe back-end of the SHDC4EU is based on using open source software such as PostGIS/PostGreSQL Rio-tiler, FastAPI and S3 in the back-end; Vue.js, OpenLayers and Geoserver in front end. We are aiming at using / building upon simple scalable solutions built on top of existing open source solutions. Note that the most important about this back-end design is that the system will be (A) cloud-native i.e. building upon cloud-optimized solutions, (B) easy to extend, (C) focused on usability of data i.e. serving seamless layers (complete, consistent, documented, version-controlled with a live support channels).\nAll pan-European layers (COGs) produced in this project, including majority of covariate layers, will be distributed through an existing infrastructure http://EcoDataCube.eu (maintained by OpenGeoHub foundation). This means that all layers will be made available:\n\nFor viewing i.e. as Web Mapping Service (WMS) so it can be displayed using OpenLayers or similar;\nFor direct data access via Simple Storage Service (S3) / files registered via the SpatioTemporal Asset Catalog (STAC) via https://stac.ecodatacube.eu;\nBack-up copy available via the NextCloud including via WebDAV.\n\nOpenGeoHub will be responsible that the data is available openly in-real-time (i.e. through S3 service) up to 5 years after the end of project, and that a copy of the data is archived on Zenodo or similar.\nWP5 will focus exclusively on pan-EU modeling, however, we might also use national / pilot data to test methods and develop experimental solutions that potentially could have much higher impact. For this purpose it is crucial that all project partners have access to the NextCloud and can freely collaborate on data without a need to have multiple copies and installations.\nAll tabular vector data will be stored in a single PostGIS DB and where possible made available to project participants (a GeoPKG copy of the point / polygon data will also be made available via the project Nextcloud). For land mask / geospatial standards we will rely on the Copernicus Land monitoring infrastructure so we will also include Turkey / Western Balkans and Ukraine i.e. also all candidate countries. For data exchange and sharing we use Nextcloud or similar. This data is however not publicly available.\n\n\n\nAll project partners agree to use standard formats to exchange data within the project and on the project websites. The recommended file formats for gridded spatial / spatiotemporal data include:\n\nCloud-Optimized GeoTIFF (*.tif).\nZarr (*.zarr) (also in combination with NetCDF (*.nc).\n\nFor vector geospatial data we recommend the following file formats:\n\nFor data subsets: GeoJSON and/or KML.\nFor cloud-data serving: Geoparquet (*.geoparquet) and/or lance.\nFor visualization: Mapbox Vector Tiles (*.mvt).\nFor GIS analysis: Geopackage (*.gpkg).\n\nFor tabular data, objects as Simple Features and similar, the following file formats are accepted for delivery:\n\nComma Separated Value (*.csv) and GeoCSV best compressed as *.csv.gz.\nR RDS (*.rds) and/or QS files that can be red and written in parallel.\nGeoJSON (*.json).\nFlatGeobuf (*.fgb).\n\nFor geospatial data to be FAIR, it should at least pass the following checks:\n\nIt is decision-ready, or at least analysis-ready (complete consistent optimized);\nIt is available in a professional catalog e.g. STAC catalog and/or Geonetwork;\nIt comes with technical documentation (ideally a peer-reviewed publication) / links to Github / Gitlab where users can find technical explanation of how was the data produced;\nIt has a version and each version has unique DOI;\nIt can be accessed directly i.e. file URL is available for HTTP requests (through S3 or similar).\n\nFor Cloud-Optimized GeoTIFFs (COG’s) it is highly recommended that all files are prepared using recommended settings i.e. a command line (&gt;GDALv3.2):\ngdal_translate in.vrt out.tif -co TILED=YES -co COPY_SRC_OVERVIEWS=YES -co COMPRESS=LZW\nEach COG will be quality controlled using (1) COG validator, (2) by visual inspection, (3) random sampling point overlay to certify that &gt;99% of pixels are available. After the quality control, all produced global mosaics will be registered and uploaded to S3 storage or similar.\n\n\n\nFor the sake of consistency and compatibility, project participants will use standard spatial resolutions to deliver and exchange data. Recommended standard pixel sizes / resolutions:\nTable: Standard bounding box and spatial resolutions.\n\n\n\nEurope COG\nBounding box\n\n\nContinental EU COG based on Copernicus\nSpatial resolutions and image size EPSG:3035\n\n\n\n\nXmin = 900,000\n\nYmin = 899,000\n\nXmax = 7,401,000\n\nYmax = 5,501,000\n\n\n10m | 650,100L x 460,200P\n\n25m | 260,040L x 184,080P\n\n30m | 216,700P x 153,400L\n\n100m | 65,010P x 46,020L\n\n250m | 26,004P x 18,408L\n\n1km | 6501P x 4602L\n\n\n\n\n\n\nSHDC will consistently use the standard OpenLandMap file-naming convention to submit new data-sets and similar (compare e.g. with the MODIS file naming convention). This is to ensure consistency and ease of use within the AI4SoilHealth project, but also by the end-users. This applies especially to WP4, WP5 and WP6.\nThe OpenLandMap file-naming convention works with 10 fields that basically define the most important properties of the data (this way users can search files, prepare data analysis etc, without even needing to access or open files. The 10 fields include:\n\nGeneric variable name (needs to be unique and approved by the AI4SoilHealth Coordination team): lclu;\nVariable procedure combination i.e. method standard (standard abbreviation): luisa;\nPosition in the probability distribution / variable type: c;\nSpatial support (usually horizontal block) in m or km: 30m;\nDepth reference or depth interval e.g. below (“b”), above (“a”) ground or at surface (“s”): s;\nTime reference begin time (YYYYMMDD): 20210101;\nTime reference end time: 20211231;\nBounding box (2 letters max): eu;\nEPSG code: epsg.3035;\nVersion code i.e. creation date: v20221015;\n\nAn example of a file-name based on the description above:\nlclu_luisa_c_30m_s_20210101_20211231_eu_epsg.3035_v20221015.tif\nNote that this file naming convention has the following properties:\n\nLarge quantities of files can be easily sorted and searched (one line queries in Bash).\nFile-naming patterns can be used to seamlessly build virtual mosaics and composites.\n\nKey spatiotemporal properties of the data are available in the file name e.g. variable type, O&M method, spatial resolution, bounding box, projection system, temporal references. Users can program analysis without opening or testing files.\nVersioning system is ubiquitous.\nAll file-names are unique.\n\nGeonetwork and STAC will be further used to link the unique file names to: (1) WPs, deliverables, themes / keywords, (2) DOI’s, (3) project homepages, (4) contact pages for support and feedback. For keywords we recommend using the INSPIRE keywords. To confirm that metadata is complete and consistent, we recommend using the INSPIRE metadata validator and/or https://data.europa.eu/en validator.\nSome simple additional rules for generating the file name include:\n\nCodes and abbreviations should be human-readable as much as possible (hence short, but not too short!);\nUse only English-US (en-us) language e.g. for months use jan, feb etc;\nConsistently use UNICODE standard: small letters only, no blank spaces, no non-ASCII characters;\nLimit the total file name size in characters to 256;\nFor time reference do not extend beyond hour minute and timezone;\nFor bounding boxes use as much as possible the 2–letter unique country code; for continents use the Equi7 Grid code i.e. eu,\nFor method codes use as much as possible unique IDs from ISO - ICS;\nFor MODIS products use consistently the MODIS products codes e.g. MOD11A2 v061; For long-term aggregates of seasonal, monthly, weekly values use the period name at the end of the method names (#3) for example the long-term estimate of MODIS LST daytime temperature for month August:\n\nlst.d_mod11a2v061.aug_m_1km_s_20200101_20211231_eu_epsg.3035_v20221015.tif\nA list of vocabularies to be used as abbreviated names of variables will be provided by OpenGeoHub. The same file-name convention described above can be also used for vector data (this would only have a different file extension) also.\n\n\n\nAll project participants are required to register project outputs and inform other project participants about the progress (via https://zenodo.org/communities/ai4soilhealth/). For internal documents / draft versions of outputs the following submission principles need to be followed:\n\nFor internal datasets and documents use project Gitlab and Mattermost to inform parties about the progress and receive feedback;\nRegularly inform your WP about the progress and planned dates (best via Mattermost channel).\nTo receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\nFor each new version of the output specify in short terms what has been improved, what has changed from last version.\nTag people in Mattermost that you would like to request to review the outputs.\n\nFor official releases please refer to the Consortium Agreement for the correct procedure. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\nFirst register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the minimum requirements regarding file formats and file naming, documentation and metadata.\nMake sure that no parts of the submission have any copyright issues.\nMake sure that all contributors are credited in detail.\nMake sure you provide appropriate disclaimer and terms of use; we recommend using “No warranty” as default.\nMake sure the license is compatible to the AI4SoilHealth project following our Consortium Agreement;\nMake sure acknowledgement (see below) is clearly provided.\nMake sure you provide contact information and instructions for users to provide feedback and request support.\nMake sure all project partners are informed about the date of release (through Gitlab and Mattermost).\n\nSuggested 3rd party platforms for registering outputs (we recommend using multiple of the listed options e.g. a, b, c etc):\n\nDatasets:\n\nhttps://zenodo.org/communities/ai4soilhealth/ and/or https://onedata.org/,\nMetadata entry via: Geonetwork and/or STAC,\nStructured summary via: https://ai4soilhealth.eu,\nSuggestion: registered on https://data.europa.eu/en,\n\nSoftware:\n\nGithub (https://github.com/ai4soilhealth) -&gt; suggestion: always generate DOI and put on Zenodo.org,\nhttps://archive.softwareheritage.org/,\nStructured summary via: https://ai4soilhealth.eu,\n\nTutorials / data catalogs:\n\nWebpages on github / gitlab (Rbookdown, python books) see e.g. https://opengeohub.github.io/SoilSamples/,\nMKdocs see e.g. https://gee-community-catalog.org/;\n\nData portals / web-GUI’s\n\nSub-domain under *.ai4soilhealth.eu,\nRecommended portal: https://gkhub.earthobservations.org/,\nVideo-tutorial published via: https://av.tib.eu/,\n\n\nWe are planning to have our own installation of Gitlab, STAC browser, Geonetwork and Pretalx. Final workflow for registering outputs will be specified in the final version of the implementation plan.\nDuring publishing of outputs, and especially if you register outputs via 3rd party repos, it of utmost importance that all partners use the correct attribution and links to project homepage:\n\nCorrect attribution: “The AI4SoilHealth project project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179.”;\nProject URL: https://cordis.europa.eu/project/id/101086179.\nProject homepage: https://ai4soilhealth.eu,\nCorrect default disclaimer is below.\n\n\n\n\nAll public releases of new data / software should point to the generic AI4SoilHealth disclaimer:\n\n“Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or European Commision. Neither the European Union nor the granting authority can be held responsible for them. The data is provided “as is”. AI4SoilHealth project consortium and its suppliers and licensors hereby disclaim all warranties of any kind, express or implied, including, without limitation, the warranties of merchantability, fitness for a particular purpose and non-infringement. Neither AI4SoilHealth Consortium nor its suppliers and licensors, makes any warranty that the Website will be error free or that access thereto will be continuous or uninterrupted. You understand that you download from, or otherwise obtain content or services through, the Website at your own discretion and risk.”\n\n\n\n\nConsidering the land mask for pan-EU (see figure), we will closely match the data coverage of Copernicus pan-european i.e. the official selection of countries listed here. Note the mask covers also all EU integration candidate countries, but can eventually also be subset to only European Union countries.\n\n\n\n\n\nLandmask for the SHDC for pan-EU.\n\n\n\n\nThere are a total of three landmask files available, each of which is aligned with the standard spatial/temporal resolution and sizes of SHDC4EU specifications. Additionally, these files include a corresponding look-up table that provides explanations for the values present in the raster data.\nTable: Technical description of the pan-EU land mask.\n\n\n\nLandmask\n\n\nPurpose/principle\n\n\nThe basic principle to create the land mask is to include as much as land as possible, to avoid missing any land pixels and ensure precise differentiation between land, ocean and inland water bodies.\n\nWhen generating the land mask, the two reference datasets in a way that:\n\n\nIf either of the two reference datasets identifies a pixel as land, it is considered a land pixel in our mask.\n\nRegarding ocean and inland water bodies, a pixel is classified as a water pixel only when both reference datasets confirm its identification as water.\n\n\n\n\n\n\nReference datasets\n\n\n\n\nWorldCover, 10 m resolution.\n\nEuroGlobalMap, with shapefiles of administrative boundaries, inland water bodies, ocean and landmask.\n\n\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution landmasks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “min” in GDAL. This “min” method allows taking the minimum values from the contributing pixels, to keep as much land as possible.\n\n\n\n\nResolution available\n\n\n10-m, 30-m, 100-m, 250-m, and 1-km resolution\n\n\n\n\nMask values\n\n\n\n\n10: not in the pan-EU area, i.e. out of mapping scope\n\n1: land\n\n2: inland water\n\n3: ocean\n\n\n\n\n\n\nISO-3166 country code mask\n\n\nPurpose/principle\n\n\nIn this mask, each country is assigned a unique value, which allows for the interpretation and analysis of data associated with a specific country. The values are assigned to each country according to iso-3166 country code, which can be found in the corresponding look-up table.\n\n\n\n\nReference datasets\n\n\nEuroGlobalMap country shapefile\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution masks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n\n\n\n\nResolution available\n\n\n10m, 30m and 100m\n\n\n\n\nMask values\n\n\nCan be found in the corresponding look-up table.\n\n\n\n\nNUTS-3 mask\n\n\nPurpose/principle\n\n\nIn this raster file, each unique NUT3 level area is assigned a unique value, which allows for the interpretation and analysis of data associated with specific NUTS-3 regions. Compared to ISO-3166 country code mask, NUTS-3 mask shows more details about regional administrative boundaries.\n\n\n\n\nReference datasets\n\n\nEuropean NUTS-3 shapefile\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution masks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n\n\n\n\nResolution available\n\n\n10 m, 30 m and 100 m\n\n\n\n\nMask values\n\n\nCan be found in the corresponding look-up table.\n\n\n\nAn important point to consider is that the ISO-code country mask provides a wider geographical coverage compared to the NUTS3 mask. This extended coverage includes countries such as Ukraine and others that lie beyond the NUTS3 administrative boundaries. Both the land mask and administrative code mask are in an Equal-area projection, allowing for accurate area estimation and facilitating aggregation per political/administrative unit.\nThese masks will be published and shared in a publicly available way on Zenodo. The working version (v0.1) is available from: https://doi.org/10.5281/zenodo.8171860. The layers can be opened directly in QGIS by copying links from Zenodo. The scripts used to generate these masks can be found in our project Gitlab or via the public Github. If any issue / problem is noticed, please report.\n\n\n\nPartners on consortium are invited to submit working versions of data and services / share preliminary outputs internally via NextCloud or similar. Folder structure and more detailed instructions will be provided by the WP lead OpenGeoHub. It is important however to distinguish between (1) internal releases, (2) public releases (partners responsible) and (3) public releases approved by the consortium.\nAll WP5 participants are required to register project outputs and inform other project participants about the progress. For internal documents / draft versions of outputs the following submission principles need to be followed:\n\nFor internal datasets and documents use project repositories, Gitlab, NextCloud and Mattermost to inform parties about the progress and receive feedback;\nRegularly inform your WP/Task group about the progress and planned dates (best via Mattermost channel).\nTo receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\nFor each new version of the output specify in short terms what has been improved, what has changed from last version.\nTag people in Mattermost that you would like to request to review the outputs.\n\nFor official releases please refer to these guidelines to avoid any delays. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\nFirst register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the minimum requirements regarding file formats and file naming, documentation and metadata.\nMake sure that no parts of the submission have any copyright issues.\nMake sure that all contributors are credited in detail.\nMake sure you provide appropriate disclaimer and terms of use; we recommend using “No warranty” as default.\nMake sure the license is compatible to the AI4SoilHealth project following our Consortium Agreement;\nMake sure acknowledgement (https://cordis.europa.eu/project/id/101086179) is clearly provided.\nMake sure you provide contact information and instructions for users to provide feedback and request support.\nMake sure all project partners are informed about the date of release (through Gitlab and Mattermost).\nIf there are any deviations from specifications you have put in the implementation plan, please indicate such deviations in the project management system.\n\nFor quality assurance (public releases) we recommend the following three checks for any new significant datasets:\n\nPreliminary data and code shared with all WP members via NextCloud / code on Gitlab / Github. Internal check in co-development sessions.\nPeer-review publications (as agreed in the proposal, all major new data products should go through peer-review).\nData and code exposed for public comments (https://github.com/ai4soilhealth) including the social media, especially Mastodon, X, Linkedin etc.\n\nIn principle, the publication process should be non-bureaucratic, agile and should not limit intonation and experimentation. However, for public releases approved by the consortium e.g. major deliverables as specified in the Grant Agreement, it is advised that these are approved by the Executive Board of the project so that the new dataset / service is then also officially promoted through media channels, landing page etc.\n\n\n\nIn order to ensure FAIR outputs, it is highly recommended that all production steps used to generate maps are documented in code, best as Rmarkdown / Python Jupyter computational notebooks. As agreed also in the project proposal, All empirical data obtained will follow strict and validated processes of monitoring and evaluation to avoid any potential error. The data produced in AI4SoilHealth will be made available as open data, following the FAIR principle. The project Data Management Plan (DMP) is available and has even more instructions.\nAI4SoilHealth has agreed to provide open access (OA) to research outputs (e.g., publications, data, software) through deposition in trusted repositories. Partners will provide OA for peer-reviewed scientific publications relating to their results. Authors of all peer-reviewed scientific publications will store them in an OA trusted repository, during and after the project’s life following Article 17 and Annex 5 of the General Assembly. The consortium members will be encouraged to publish in the Open Research Europe data platform, specifically via the Zenodo community for AI4SoilHealth (https://zenodo.org/communities/ai4soilhealth/).\nBesides OA publication, the project aims for early and open sharing of the soil health data, and the research and technological developments including open data, open standards, open source software, and open communication:\n\nOpen data: The AI4SoilHealth consortium will build solutions upon open datasets published using genuinely open data licenses.\n\nOpen standards (interoperability): The use of open standards prevents lock-in by, or dependency on any single data, software or service supplier. AI4SoilHealth will fully adopt the principles of FAIR data management and open standards to enable interoperability of methods and services, both within the project and beyond.\nOpen source software: AI4SoilHealth plans to release the mobile phone application on Github under the MIT license. All major outputs of the project should be made available via the project github (https://github.com/ai4soilhealth).\nOpen communication: The consortium has experience with organizing live, open, and free discussion forums and workshops.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#the-7-step-framework-for-soil-health-assessment",
    "href": "1-specifications.html#the-7-step-framework-for-soil-health-assessment",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "In a nutshell, Soil Health Data Cube is a building block and infrastructure for monitoring soil health across EU. The general idea of the SHDC4EU is that it serves some standard processes that help inform land owners / land managers about the soil health state of their land using open data. These processes are meant to help future users of the AI4SoilHealth app to quickly find out about the site, explore land potentials based on the state-of-the-art models and start taking concrete actions towards restoring land / improving soil health. This is closely connected with the main expected impacts of the AI4SoilHealth project (to make soil health data easier to access and provide tools and standards for soil health monitorng). In simple terms, the 7–step soil health assessment framework (Hengl 2024) includes:\n\nAssess the history of a site (last 30+ years) provide answers to questions e.g.:\n\nHow much SOC has been gained / lost over the last 30 years?\nWhich chemical and physical soil properties changed the most?\nIs there any land/soil degradation and how significant is it?\nHow has agronomic management impacted soil health?\nHow much did the land cover / vegetation cover change? What is the main change class?\nHow much did the landscape heterogeneity change over time?\n\nDetermine current state of soil (actual year):\n\nWhat is the current state of physical and chemical soil properties?\nWhat is the current (WRB) soil type / main soil forming processes and soil states?\nWhat are the current derived soil properties?\nWhat are the current macronutrient stocks in the soil?\n\nDetermine soil potential (next 10, 20, 30 yrs):\n\nWhat is the SOC sequestration potential of this soil?\nWhat is the agricultural potential of this soil?\nWhat are the other potential ecosystem functions provided some land use change or mitigations?\n\nCollect extra soil samples, send to lab, and add to training points:\n\nGenerate a sampling design that helps increase mapping accuracy. Optimize the number of samples while taking into account the costs of sampling.\nCollect new samples and send them to the lab.\n\nImport new local data then re-analyze and re-asses points 1, 2 and 3.\n\nImport new local samples / results and add to the training points. Re-assess local prediction accuracy.\nRepredict layers in SHDC4EU for steps #1, #2 and #3.\n\nSuggest concrete KPI’s (per farm / per land unit):\n\nProvided that soil potential assessment reveals a significant gap between potential and actual.\nEither directly provide concrete measures to help increase soil health and/or provide links to documents / organizations that can directly help increase soil health.\n\nTrack progress per farm and re-assess if required:\n\nEvery 1–2 years update the indicators (pan-EU).\nCompare with the planned transition in #6. If necessary recommend re-assessment.\n\n\nThis framework can be either apply to assess health of a specific site (either in the field using a mobile phone app, or in the office using a desktop app), or complete farms / administrative units. We, thus, aim to complete the data cube with primary and derived soil variables that can directly serve this framework, and especially so that can be worked alongside other Work Packages in the project in their testing of methodology.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#predictive-soil-mapping-based-on-machine-learning-and-hpc",
    "href": "1-specifications.html#predictive-soil-mapping-based-on-machine-learning-and-hpc",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "WP5 — with the tasks T5.2 Development of AI4SoilHealth computing engine and T5.3 Development of AI4SoilHealth Indicators data cube) — aims at implementing fully automated predictive soil mapping at high spatial resolution using large datasets (e.g. whole of Europe at 30-m spatial resolution). Prepared harmonized point datasets will are overlaid against time-series of EO images and/or static terrain-based or similar indices. These are then be used to predict values of target variables in 2D, 3D and 2D+T, 3D+T. The general workflow for predictive soil mapping based on Spatiotemporal Machine Learning and High Performance Computing (HPC) and assumes that all point data is analysis-ready, representative and correlates with the EO / covariate layers i.e. can be used to produce usable spatial predictions. Automated predictive soil mapping is currently implemented via the Python library scikit-map using OpenGeoHub’s High Performance Computing infrastructure.\nThe key interest of WP5 is to produce pan-EU predictions at the highest possible spatial resolution and that can directly serve soil health assessment at continental scale. Some variables are, however, not available for the whole of pan-EU, and some point datasets will be produced experimentally and used for testing purposes only. Hence, within the WP5, all soil variables are modeled / mapped under one the following 3 tiers:\n\nTier 1: Pan-EU, production ready variables: usually based on LUCAS + national soil datasets;\nTier 2 (internal, under construction): National / in-situ data: Tested locally, then upscaled to whole of EU; usually based on collaboration of WP3,4,5,6;\nTier 3 (internal, under construction): Pan-EU, new experimental variables (currently not in the LUCAS soil and most likely can not be modeled/mapped across EU, but can only be used to assess soil health at site); based on the collaboration of WP3,4,5;\n\nWe expect that project partners + pilots will feed data for Tier 2 and Tier 3. However, not everything can be mapped across the EU, hence some variables will eventually stay available only locally and a small selection of variables will be available in-situ only i.e. these will not be mapped at all.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#training-points",
    "href": "1-specifications.html#training-points",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "Quality of the outputs would most likely be dictated by the following four key aspects of modeling:\n\nThe general modeling design i.e. how appropriate the statistical / machine learning methods are. For example: are all casualties considered, are all distributions represented, are all statistical assumptions met?\nPredictive performance of the algorithms used i.e. is the best / fastest algorithm used for modeling? Is the algorithm robust, noise-proof, artifacts-proof etc?\nQuality and diversity of the covariate layers used.\nQuality and spatial and feature space representation of training points.\n\nAssuming that #1 and #2 are optimized, then the only remaining factors that control success of the modeling / mapping processes are aspects #3 and #4 i.e. the quality of covariate layers and quality of training points. In the past (prior to 2010), it was relatively difficult to produce pan-EU maps of soil health indicators as there was only limited training point data. Thanks to the European Commission’s LUCAS soil project, we now have 3–4 repetitions of ground measurements of soil properties of highest quality (about 22,000 sites are revisited per period) (see figure below). This is a unique opportunity to test building high resolution predictions, including dynamic soil property predictions. Ideally, such testing should be a joint effort of the AI4SoilHealth consortium, in an open collaboration with JRC and members of other sister projects funded by the same Soil mission.\nNevertheless, LUCAS soil is also not an ideal dataset for predictive mapping. Hence producing SHDC4EU faces two serious challanges: (1) LUCAS soil is a top-soil dataset, although there is now intention to sample also subsoils, (2) it covers only a number of physical and chemical soil variables (10–15), and soil types or similar are typically not recorded, making this largely a partial pedological survey.\n\n\n\n\n\nLUCAS soil samples (Orgiazzi et al. 2018) and connected existing and upcoming EO missions (bars indicate approximated temporal coverage). Note that the amount of EO data and missions is increasing exponentially.\n\n\n\n\nTo improve usability of predictions produced using LUCAS we have decided, in this project, to combine both LUCAS and the highest quality legacy national soil datasets. These synchronized and analysis ready fusions of soil laboratory points and observations will be generated by T4.6 “Integration and harmonization of in-situ and ancillary observations”. We anticipate that this will be a long process hence, in order to prevent serious delays, we will start producing SHDC4EU predictions, then in each iteration try to improve predictions as new countries join the campaign of contributing their data (under a standard Data Sharing Agreement) for the purpose of data mining i.e. producing open soil information for everyone.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#covariate-base-layers",
    "href": "1-specifications.html#covariate-base-layers",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "The second important aspect that will determine the quality of the SHDC4EU outputs is the quality of covariate layers. Quality of the covariate layers is basically determined by three things: (1) spatial and temporal resolution, (2) spectral / thematic content, (3) general quality of data in terms of completeness, consistency, correctness and quality of metadata. For producing SHDC4EU we plan to use an extensive compilation of covariate (base) layers to produce predictions for SHDC4EU. Already available layers for continental Europe include (https://EcoDataCube.eu):\n\nLand mask (pan-EU: https://zenodo.org/doi/10.5281/zenodo.8171860) + World Settlement Footprint (WSF) i.e. world buildings 2000-2015, 2019 (https://geoservice.dlr.de/web/maps/eoc:wsfevolution and https://geoservice.dlr.de/web/maps/eoc:wsf2019);\nDigital Terrain Model variables at 6–scales 30-m, 60, 120, 240, 480 and 960-m:\n\nElevation, slope (%), min- max-curvature,\nHillshade, northness, easterness,\nPositive, negative openness,\nTopidx (TWI), geomorphon classes (10), catchment area, LS factor,\n\nLithological (surface geology) map of pan-EU at 250-m (https://zenodo.org/doi/10.5281/zenodo.4787631);\nBimonthly / quarterly GLAD Landsat composites (all bands + biophysical indices, 2000–2022) (explained in: https://doi.org/10.21203/rs.3.rs-4251113/v1):\n\nGreen, Red, NIR, SWIR1, SWIR2,\nNDVI, FAPAR,\nNDTI, NDWI, BSF,\nCumulative NDVI, NDTI and BSF;\n\nClimatic variables at 1-km (long-term or monthly series):\n\nCHELSA Climate Bioclimatic variables (https://chelsa-climate.org/bioclim/);\nCHELSA monthly precipitation time-series 2000–2022 (https://chelsa-climate.org/timeseries/);\nMODIS LST daytime and nighttime monthly time-series 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.1420114);\nMODIS monthly water vapor time-series 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.8193738);\nCumulative annual precipitation (2000–2022);\n\nMonthly / annual snow images 2000-2022 e.g. DLR Snow pack at 500-m (https://geoservice.dlr.de/web/maps/eoc:gsp:yearly):\n\nLong-term monthly snow probability time-series at 500-m (https://doi.org/10.5281/zenodo.5774953);\nCumulative annual snow probability 2000–2022;\n\nSurface water dynamics: occurrence probability of water long-term 1999–2021 (https://glad.umd.edu/dataset/global-surface-water-dynamics);\nOptional: Global Flood Database v1 (2000-2018) at 250-m annual flood event (http://global-flood-database.cloudtostreet.info/);\nLight at night time-series at 500-m resolution 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.7750174);\nCropland extent at 30-m for 2000 to 2022 based on (https://glad.umd.edu/dataset/croplands);\nBare soil percent and photosythetical vegetation percent annual for 2000 to 2022 based on the MODIS MCD43A4 product (500-m spatial resolution);\n\nIn addition to the existing layers above, we will also generate a number of novel layers tailored specifically for the purpose of representing land use practices and potential soil degradation factors. This include:\n\nBSI, Bare Soil Index (Mzid et al. 2021) and annual BEF, Bare Earth Fraction (e.g. proportion of pixels with NDVI &lt;0.35 based on 16–day and/or bimonthly data; there are other possible thresholds combination that can be used to optimize the results),\nNDTI, Normalized Differential Tillage Index (Ettehadi Osgouei et al. 2019),\nNOS, Number of Seasons / NOCC,** Number of Cropping Cycles**,\nLOS, Length of Seasons / CDR, Crop Duration Ratio (Estel et al. 2016),\nCrop-type based on EuroCrops dataset,\n\nNote that from all covariate layers listed above, 16-day / bimonthly / quarterly GLAD Landsat composites (all bands + all indices, 2000–2022) are the largest part of the data to be used taking almost 20TB of storage (in compressed format).",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#predictive-mapping-and-derivation-methods",
    "href": "1-specifications.html#predictive-mapping-and-derivation-methods",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "Based on the availability of the training points and nature of the target variable in the SHDC4EU, we will either predict or derive soil variables from primary variables. The output soil health indicators can, thus, be considered of type either:\n\nPredicted dynamic pan-EU variables (2000–2022+) available at standard depth intervals (0–20, 20–50, 50–100, 100–200 cm).\nPredicted static pan-EU variables.\nPredicted and/or simulated pan-EU variables i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling.\nDerived pan-EU variables and indices.\nIn-situ only variables.\n\nMost of the soil chemical and biological variables are mapped as dynamic variables with annual or 5–year estimates (e.g. 5 maps for period 2000–2025) and at multiple depths (Witjes et al. 2023). This means that the amount of produced data can be significant if predictions are also provided per depth. For example, to map soil organic carbon content (weight %), we can predict 23 annual maps at 4–5 standard depths resulting in over 180 images (assuming that we also produce prediction errors per pixel). AI4SoilHealth tasks T5.5 “Development of AI4SoilHealth present and future soil degradation products”, T5.6 “Development of AI4SoilHealth forecasting services”, and T5.7 “Development of AI4SoilHealth soil functions evidence / model chains” will also generate large amounts of predicted and/or simulated pan-EU variables i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling.\nThe following predictive soil modeling methods are considered for use so far for generating SHDC4EU:\n\n2D and 3D predictive soil mapping (static predictions, time is ignored).\n2D+T predictive soil mapping spacetime.\n3D+T predictive soil mapping spacetime.\nPredictive soil mapping focused on long-term indices.\nDerivation from primary data.\nDerivation using Pedo-transfer functions.\nDerivation / simulations using Mechanistic model (iterative).\nDerivation of future projected predictions (scenario testing).\n\nFor each derivation method we use sets of algorithms, usually implemented in python or R programming languages. These are documented in detail and eventually allow for complete reproducibility of results; most importantly we do a series of benchmarking (listed in further sections) to compare predictive performance and select the algorithm that is most accurate + most robust at the same time. In the tasks T5.5 Development of AI4SoilHealth present and future soil degradation products, T5.6 Development of AI4SoilHealth forecasting services and T5.7 Development of AI4SoilHealth soil functions evidence / model chains, also process-based modeling can be used, especially to generate potential soil ecosystem services etc. Such modeling is at the order-of-magnitude more computationally demanding than predictive mapping hence we expect that these outputs will be of limited spatial detail (1-km) and or available only for experimental testing.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#targeted-soil-and-soil-health-variables",
    "href": "1-specifications.html#targeted-soil-and-soil-health-variables",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "The general objective of the SHDC4EU is to serve best possible, most detailed, complete and consistent predictions of the the number of targeted soil health indicators in the EU Mission’s “Implementation Plan: A Soil Deal for Europe”:\n\npresence of pollutants, excess nutrients and salts,\nsoil organic carbon stock,\nsoil structure including soil bulk density and absence of soil sealing and erosion,\nsoil biodiversity,\nsoil nutrients and acidity (pH),\nvegetation cover,\nlandscape heterogeneity,\nforest cover.\n\nIf these are available for the complete land mask of pan-EU area, these can then be used to assess soil degradation state (e.g. salinization / sealing level and trends, concentration of excess nutrients and pollutants and trends, erosion state and trends, loss of SOC and trends etc), current soil properties and soil potential in terms of potential ecosystem services / potential SOC sequestration etc, potential productivity of soil, potential soil biodiversity etc. The working version of what we find as feasible to map at high spatial resolution is provided below.\nNote that some of the soil health indicators recommended by the European Commission / JRC, are not defacto soil variables (e.g. vegetation cover, landscape heterogeneity, forest cover) but will be generated in this project and integrated into SHDC4EU. Some potential options to represent the vegetation cover, landscape heterogeneity etc include:\n\nCanopy height, data already available for EU but could also be improved by outputs from the Open-Earth-Monitor project,\nGPP, Gross Primary Productivity (bimonthly and/or annual; based on the last 2–5yrs),\nLand cover and land use / cropping systems and applications (categories e.g. based on the EuroCrops), producing cropping system maps for the EU;\n\nHowever, producing cropping system maps for the EU, even only for the recent year is not trivial and producing such data should is at the moment optional.\nList of target soil variables that will be delivered in the SHDC4EU includes dynamic soil properties covering period 2000–2025+:\n\nSoil organic carbon density (kg/m3) ISO 10694. 1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year estimates;\nSoil carbon stock change (t/ha/yr) ISO 10694. 1996 for 0–20, 20–50, 50–100 cm depth intervals; long-term\nSoil pH in a suspension of soil in water (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 depth intervals, 5–year estimates;\nSoil pH measured in a CaCl2 solution (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil total nitrogen content (dg/kg) ISO 11261:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil bulk density (t/m3) Adapted ISO 11272:2017 for 0–20, 20–50, 50–100 cm, 5–year;\nSoil texture fractions (sand, silt, clay) (g/g) ISO:11277 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil WRB subgroup (factor / probs) based on the WRB2022 classificaton system for 0–200 cm, long-term;\nDepth to bedrock (cm) up to 200 cm, long-term;\nExtractable potassium content (mg/kg) USDA−NRCS, 2004 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nCarbonates content CaCO3 (g/g) ISO 10693:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nExtractable Phosphorus content (Olsen) (mg/kg) ISO 11263. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nMonthly Gross Primary Productivity (kg/ha/yr) FluxNET bi-monthly;\nBare Soil Fraction (m2/m2) Landsat-based, 5–year;",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#variable-registry",
    "href": "1-specifications.html#variable-registry",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "For each variable we will organize a soil variable registry system, so that each variable should have unique (short-as-possible) code. As simple coding system for SHDC4EU is suggested where the variable name is a combination of the three components:\n\ngeneric variable name,\nspecific laboratory / field / O&M method (ISO standard or similar),\nmeasurement unit,\n\nFor example, for SOC content in weight percent we recommend using oc_iso.10694.1995_wpct (dry combustion) or oc_iso.17184.2014_wpct; for SOC in permiles you can use e.g. oc_iso.10694.1995_wpml. Note that here we use ISO code, however since ISO is highly commercial and not-practical for open datasets, we recommend using the ISO code (allowed), but in fact pointing to a scientific reference i.e. a publication or PDF that is at the order of scale easier to obtain (instead of pointing to ISO website or similar). For example the variable oc_iso.10694.1995_wpct can be linked to the scientific reference Nelson et al. (1982) (also used by ISO).\nSoil variable names can be also provided in a shorter version (assuming that only 1 reference method exists) as a combination of the variable name and measurement unit code e.g. oc.wcpt. It is very important that all partners in the project consistently use the same codes, and if there are updates in the naming convention that they refer to the version of the code.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#spatio-temporal-reference",
    "href": "1-specifications.html#spatio-temporal-reference",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "The general interest of this project is to produce a time-series of predictions of key soil/vegetation/land use variables with either annual or monthly support and covering the period 1997–2022+ and at highest spatial resolution e.g. 30-m. For variables that vary by depth, we will predict at 5 standard depths (0, 20, 50, 100 and 200 cm), then aggregate values to standard depth intervals:\n\n0–20 cm (topsoil) LUCAS standard;\n20–50 cm (subsoil1);\n50–100 cm (subsoil2);\n100–200 cm (subsoil3);\n\nTemporal support of predictions / simulations can be one of the following:\n\nLong-term primary soil properties and classes: e.g. soil types, soil water holding capacity;\nAnnual to 5–year values of primary soil and vegetation variables: e.g. annual bare soil fraction (index);\nBimonthly, monthly or seasonal values of soil and vegetation variables: e.g. bimonthly GPP.\nWeekly, 16–day values (original Landsat GLAD),\nDaily or hourly values (probably not of interest in this project).\n\nAggregated values of target soil and vegetation variables can also refer to some part of the distribution e.g. quantile e.g. P25, P50 (median / mean value) and P75.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#uncertainty-of-predictions",
    "href": "1-specifications.html#uncertainty-of-predictions",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "As an ideal case, each predictive mapping model should produce / provide also uncertainty per pixel (prediction error) and summary results of robust cross-validation. Two options are possible for predictive error mapping:\n\nProvide lower and upper quantiles p=0.05 and p=0.95 so that a 90% probability prediction interval can be derived. For variables with skewed distribution / log-normally distributed it is recommended that the lower values are computed in the log-transformed space to avoid predicting negative values.\nProvide prediction error as 1 standard deviation: from this also prediction interval can be derived, assuming that the variable is normally distributed.\n\nAs a general recommendation we suggest using the Conformal Prediction method to produce prediction errors per pixel, best as implemented in the python libraries mappie and/or puncc.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#back-end-front-end-components",
    "href": "1-specifications.html#back-end-front-end-components",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "The back-end of the SHDC4EU is based on using open source software such as PostGIS/PostGreSQL Rio-tiler, FastAPI and S3 in the back-end; Vue.js, OpenLayers and Geoserver in front end. We are aiming at using / building upon simple scalable solutions built on top of existing open source solutions. Note that the most important about this back-end design is that the system will be (A) cloud-native i.e. building upon cloud-optimized solutions, (B) easy to extend, (C) focused on usability of data i.e. serving seamless layers (complete, consistent, documented, version-controlled with a live support channels).\nAll pan-European layers (COGs) produced in this project, including majority of covariate layers, will be distributed through an existing infrastructure http://EcoDataCube.eu (maintained by OpenGeoHub foundation). This means that all layers will be made available:\n\nFor viewing i.e. as Web Mapping Service (WMS) so it can be displayed using OpenLayers or similar;\nFor direct data access via Simple Storage Service (S3) / files registered via the SpatioTemporal Asset Catalog (STAC) via https://stac.ecodatacube.eu;\nBack-up copy available via the NextCloud including via WebDAV.\n\nOpenGeoHub will be responsible that the data is available openly in-real-time (i.e. through S3 service) up to 5 years after the end of project, and that a copy of the data is archived on Zenodo or similar.\nWP5 will focus exclusively on pan-EU modeling, however, we might also use national / pilot data to test methods and develop experimental solutions that potentially could have much higher impact. For this purpose it is crucial that all project partners have access to the NextCloud and can freely collaborate on data without a need to have multiple copies and installations.\nAll tabular vector data will be stored in a single PostGIS DB and where possible made available to project participants (a GeoPKG copy of the point / polygon data will also be made available via the project Nextcloud). For land mask / geospatial standards we will rely on the Copernicus Land monitoring infrastructure so we will also include Turkey / Western Balkans and Ukraine i.e. also all candidate countries. For data exchange and sharing we use Nextcloud or similar. This data is however not publicly available.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#data-exchange-formats",
    "href": "1-specifications.html#data-exchange-formats",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "All project partners agree to use standard formats to exchange data within the project and on the project websites. The recommended file formats for gridded spatial / spatiotemporal data include:\n\nCloud-Optimized GeoTIFF (*.tif).\nZarr (*.zarr) (also in combination with NetCDF (*.nc).\n\nFor vector geospatial data we recommend the following file formats:\n\nFor data subsets: GeoJSON and/or KML.\nFor cloud-data serving: Geoparquet (*.geoparquet) and/or lance.\nFor visualization: Mapbox Vector Tiles (*.mvt).\nFor GIS analysis: Geopackage (*.gpkg).\n\nFor tabular data, objects as Simple Features and similar, the following file formats are accepted for delivery:\n\nComma Separated Value (*.csv) and GeoCSV best compressed as *.csv.gz.\nR RDS (*.rds) and/or QS files that can be red and written in parallel.\nGeoJSON (*.json).\nFlatGeobuf (*.fgb).\n\nFor geospatial data to be FAIR, it should at least pass the following checks:\n\nIt is decision-ready, or at least analysis-ready (complete consistent optimized);\nIt is available in a professional catalog e.g. STAC catalog and/or Geonetwork;\nIt comes with technical documentation (ideally a peer-reviewed publication) / links to Github / Gitlab where users can find technical explanation of how was the data produced;\nIt has a version and each version has unique DOI;\nIt can be accessed directly i.e. file URL is available for HTTP requests (through S3 or similar).\n\nFor Cloud-Optimized GeoTIFFs (COG’s) it is highly recommended that all files are prepared using recommended settings i.e. a command line (&gt;GDALv3.2):\ngdal_translate in.vrt out.tif -co TILED=YES -co COPY_SRC_OVERVIEWS=YES -co COMPRESS=LZW\nEach COG will be quality controlled using (1) COG validator, (2) by visual inspection, (3) random sampling point overlay to certify that &gt;99% of pixels are available. After the quality control, all produced global mosaics will be registered and uploaded to S3 storage or similar.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#standard-spatialtemporal-resolutions-and-support-sizes",
    "href": "1-specifications.html#standard-spatialtemporal-resolutions-and-support-sizes",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "For the sake of consistency and compatibility, project participants will use standard spatial resolutions to deliver and exchange data. Recommended standard pixel sizes / resolutions:\nTable: Standard bounding box and spatial resolutions.\n\n\n\nEurope COG\nBounding box\n\n\nContinental EU COG based on Copernicus\nSpatial resolutions and image size EPSG:3035\n\n\n\n\nXmin = 900,000\n\nYmin = 899,000\n\nXmax = 7,401,000\n\nYmax = 5,501,000\n\n\n10m | 650,100L x 460,200P\n\n25m | 260,040L x 184,080P\n\n30m | 216,700P x 153,400L\n\n100m | 65,010P x 46,020L\n\n250m | 26,004P x 18,408L\n\n1km | 6501P x 4602L",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#file-naming-convention",
    "href": "1-specifications.html#file-naming-convention",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "SHDC will consistently use the standard OpenLandMap file-naming convention to submit new data-sets and similar (compare e.g. with the MODIS file naming convention). This is to ensure consistency and ease of use within the AI4SoilHealth project, but also by the end-users. This applies especially to WP4, WP5 and WP6.\nThe OpenLandMap file-naming convention works with 10 fields that basically define the most important properties of the data (this way users can search files, prepare data analysis etc, without even needing to access or open files. The 10 fields include:\n\nGeneric variable name (needs to be unique and approved by the AI4SoilHealth Coordination team): lclu;\nVariable procedure combination i.e. method standard (standard abbreviation): luisa;\nPosition in the probability distribution / variable type: c;\nSpatial support (usually horizontal block) in m or km: 30m;\nDepth reference or depth interval e.g. below (“b”), above (“a”) ground or at surface (“s”): s;\nTime reference begin time (YYYYMMDD): 20210101;\nTime reference end time: 20211231;\nBounding box (2 letters max): eu;\nEPSG code: epsg.3035;\nVersion code i.e. creation date: v20221015;\n\nAn example of a file-name based on the description above:\nlclu_luisa_c_30m_s_20210101_20211231_eu_epsg.3035_v20221015.tif\nNote that this file naming convention has the following properties:\n\nLarge quantities of files can be easily sorted and searched (one line queries in Bash).\nFile-naming patterns can be used to seamlessly build virtual mosaics and composites.\n\nKey spatiotemporal properties of the data are available in the file name e.g. variable type, O&M method, spatial resolution, bounding box, projection system, temporal references. Users can program analysis without opening or testing files.\nVersioning system is ubiquitous.\nAll file-names are unique.\n\nGeonetwork and STAC will be further used to link the unique file names to: (1) WPs, deliverables, themes / keywords, (2) DOI’s, (3) project homepages, (4) contact pages for support and feedback. For keywords we recommend using the INSPIRE keywords. To confirm that metadata is complete and consistent, we recommend using the INSPIRE metadata validator and/or https://data.europa.eu/en validator.\nSome simple additional rules for generating the file name include:\n\nCodes and abbreviations should be human-readable as much as possible (hence short, but not too short!);\nUse only English-US (en-us) language e.g. for months use jan, feb etc;\nConsistently use UNICODE standard: small letters only, no blank spaces, no non-ASCII characters;\nLimit the total file name size in characters to 256;\nFor time reference do not extend beyond hour minute and timezone;\nFor bounding boxes use as much as possible the 2–letter unique country code; for continents use the Equi7 Grid code i.e. eu,\nFor method codes use as much as possible unique IDs from ISO - ICS;\nFor MODIS products use consistently the MODIS products codes e.g. MOD11A2 v061; For long-term aggregates of seasonal, monthly, weekly values use the period name at the end of the method names (#3) for example the long-term estimate of MODIS LST daytime temperature for month August:\n\nlst.d_mod11a2v061.aug_m_1km_s_20200101_20211231_eu_epsg.3035_v20221015.tif\nA list of vocabularies to be used as abbreviated names of variables will be provided by OpenGeoHub. The same file-name convention described above can be also used for vector data (this would only have a different file extension) also.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#registering-project-outputs",
    "href": "1-specifications.html#registering-project-outputs",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "All project participants are required to register project outputs and inform other project participants about the progress (via https://zenodo.org/communities/ai4soilhealth/). For internal documents / draft versions of outputs the following submission principles need to be followed:\n\nFor internal datasets and documents use project Gitlab and Mattermost to inform parties about the progress and receive feedback;\nRegularly inform your WP about the progress and planned dates (best via Mattermost channel).\nTo receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\nFor each new version of the output specify in short terms what has been improved, what has changed from last version.\nTag people in Mattermost that you would like to request to review the outputs.\n\nFor official releases please refer to the Consortium Agreement for the correct procedure. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\nFirst register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the minimum requirements regarding file formats and file naming, documentation and metadata.\nMake sure that no parts of the submission have any copyright issues.\nMake sure that all contributors are credited in detail.\nMake sure you provide appropriate disclaimer and terms of use; we recommend using “No warranty” as default.\nMake sure the license is compatible to the AI4SoilHealth project following our Consortium Agreement;\nMake sure acknowledgement (see below) is clearly provided.\nMake sure you provide contact information and instructions for users to provide feedback and request support.\nMake sure all project partners are informed about the date of release (through Gitlab and Mattermost).\n\nSuggested 3rd party platforms for registering outputs (we recommend using multiple of the listed options e.g. a, b, c etc):\n\nDatasets:\n\nhttps://zenodo.org/communities/ai4soilhealth/ and/or https://onedata.org/,\nMetadata entry via: Geonetwork and/or STAC,\nStructured summary via: https://ai4soilhealth.eu,\nSuggestion: registered on https://data.europa.eu/en,\n\nSoftware:\n\nGithub (https://github.com/ai4soilhealth) -&gt; suggestion: always generate DOI and put on Zenodo.org,\nhttps://archive.softwareheritage.org/,\nStructured summary via: https://ai4soilhealth.eu,\n\nTutorials / data catalogs:\n\nWebpages on github / gitlab (Rbookdown, python books) see e.g. https://opengeohub.github.io/SoilSamples/,\nMKdocs see e.g. https://gee-community-catalog.org/;\n\nData portals / web-GUI’s\n\nSub-domain under *.ai4soilhealth.eu,\nRecommended portal: https://gkhub.earthobservations.org/,\nVideo-tutorial published via: https://av.tib.eu/,\n\n\nWe are planning to have our own installation of Gitlab, STAC browser, Geonetwork and Pretalx. Final workflow for registering outputs will be specified in the final version of the implementation plan.\nDuring publishing of outputs, and especially if you register outputs via 3rd party repos, it of utmost importance that all partners use the correct attribution and links to project homepage:\n\nCorrect attribution: “The AI4SoilHealth project project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179.”;\nProject URL: https://cordis.europa.eu/project/id/101086179.\nProject homepage: https://ai4soilhealth.eu,\nCorrect default disclaimer is below.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#disclaimer-for-data-software-products",
    "href": "1-specifications.html#disclaimer-for-data-software-products",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "All public releases of new data / software should point to the generic AI4SoilHealth disclaimer:\n\n“Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or European Commision. Neither the European Union nor the granting authority can be held responsible for them. The data is provided “as is”. AI4SoilHealth project consortium and its suppliers and licensors hereby disclaim all warranties of any kind, express or implied, including, without limitation, the warranties of merchantability, fitness for a particular purpose and non-infringement. Neither AI4SoilHealth Consortium nor its suppliers and licensors, makes any warranty that the Website will be error free or that access thereto will be continuous or uninterrupted. You understand that you download from, or otherwise obtain content or services through, the Website at your own discretion and risk.”",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#land-mask",
    "href": "1-specifications.html#land-mask",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "Considering the land mask for pan-EU (see figure), we will closely match the data coverage of Copernicus pan-european i.e. the official selection of countries listed here. Note the mask covers also all EU integration candidate countries, but can eventually also be subset to only European Union countries.\n\n\n\n\n\nLandmask for the SHDC for pan-EU.\n\n\n\n\nThere are a total of three landmask files available, each of which is aligned with the standard spatial/temporal resolution and sizes of SHDC4EU specifications. Additionally, these files include a corresponding look-up table that provides explanations for the values present in the raster data.\nTable: Technical description of the pan-EU land mask.\n\n\n\nLandmask\n\n\nPurpose/principle\n\n\nThe basic principle to create the land mask is to include as much as land as possible, to avoid missing any land pixels and ensure precise differentiation between land, ocean and inland water bodies.\n\nWhen generating the land mask, the two reference datasets in a way that:\n\n\nIf either of the two reference datasets identifies a pixel as land, it is considered a land pixel in our mask.\n\nRegarding ocean and inland water bodies, a pixel is classified as a water pixel only when both reference datasets confirm its identification as water.\n\n\n\n\n\n\nReference datasets\n\n\n\n\nWorldCover, 10 m resolution.\n\nEuroGlobalMap, with shapefiles of administrative boundaries, inland water bodies, ocean and landmask.\n\n\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution landmasks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “min” in GDAL. This “min” method allows taking the minimum values from the contributing pixels, to keep as much land as possible.\n\n\n\n\nResolution available\n\n\n10-m, 30-m, 100-m, 250-m, and 1-km resolution\n\n\n\n\nMask values\n\n\n\n\n10: not in the pan-EU area, i.e. out of mapping scope\n\n1: land\n\n2: inland water\n\n3: ocean\n\n\n\n\n\n\nISO-3166 country code mask\n\n\nPurpose/principle\n\n\nIn this mask, each country is assigned a unique value, which allows for the interpretation and analysis of data associated with a specific country. The values are assigned to each country according to iso-3166 country code, which can be found in the corresponding look-up table.\n\n\n\n\nReference datasets\n\n\nEuroGlobalMap country shapefile\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution masks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n\n\n\n\nResolution available\n\n\n10m, 30m and 100m\n\n\n\n\nMask values\n\n\nCan be found in the corresponding look-up table.\n\n\n\n\nNUTS-3 mask\n\n\nPurpose/principle\n\n\nIn this raster file, each unique NUT3 level area is assigned a unique value, which allows for the interpretation and analysis of data associated with specific NUTS-3 regions. Compared to ISO-3166 country code mask, NUTS-3 mask shows more details about regional administrative boundaries.\n\n\n\n\nReference datasets\n\n\nEuropean NUTS-3 shapefile\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution masks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n\n\n\n\nResolution available\n\n\n10 m, 30 m and 100 m\n\n\n\n\nMask values\n\n\nCan be found in the corresponding look-up table.\n\n\n\nAn important point to consider is that the ISO-code country mask provides a wider geographical coverage compared to the NUTS3 mask. This extended coverage includes countries such as Ukraine and others that lie beyond the NUTS3 administrative boundaries. Both the land mask and administrative code mask are in an Equal-area projection, allowing for accurate area estimation and facilitating aggregation per political/administrative unit.\nThese masks will be published and shared in a publicly available way on Zenodo. The working version (v0.1) is available from: https://doi.org/10.5281/zenodo.8171860. The layers can be opened directly in QGIS by copying links from Zenodo. The scripts used to generate these masks can be found in our project Gitlab or via the public Github. If any issue / problem is noticed, please report.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#layer-submission-process-and-quality-control",
    "href": "1-specifications.html#layer-submission-process-and-quality-control",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "Partners on consortium are invited to submit working versions of data and services / share preliminary outputs internally via NextCloud or similar. Folder structure and more detailed instructions will be provided by the WP lead OpenGeoHub. It is important however to distinguish between (1) internal releases, (2) public releases (partners responsible) and (3) public releases approved by the consortium.\nAll WP5 participants are required to register project outputs and inform other project participants about the progress. For internal documents / draft versions of outputs the following submission principles need to be followed:\n\nFor internal datasets and documents use project repositories, Gitlab, NextCloud and Mattermost to inform parties about the progress and receive feedback;\nRegularly inform your WP/Task group about the progress and planned dates (best via Mattermost channel).\nTo receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\nFor each new version of the output specify in short terms what has been improved, what has changed from last version.\nTag people in Mattermost that you would like to request to review the outputs.\n\nFor official releases please refer to these guidelines to avoid any delays. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\nFirst register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the minimum requirements regarding file formats and file naming, documentation and metadata.\nMake sure that no parts of the submission have any copyright issues.\nMake sure that all contributors are credited in detail.\nMake sure you provide appropriate disclaimer and terms of use; we recommend using “No warranty” as default.\nMake sure the license is compatible to the AI4SoilHealth project following our Consortium Agreement;\nMake sure acknowledgement (https://cordis.europa.eu/project/id/101086179) is clearly provided.\nMake sure you provide contact information and instructions for users to provide feedback and request support.\nMake sure all project partners are informed about the date of release (through Gitlab and Mattermost).\nIf there are any deviations from specifications you have put in the implementation plan, please indicate such deviations in the project management system.\n\nFor quality assurance (public releases) we recommend the following three checks for any new significant datasets:\n\nPreliminary data and code shared with all WP members via NextCloud / code on Gitlab / Github. Internal check in co-development sessions.\nPeer-review publications (as agreed in the proposal, all major new data products should go through peer-review).\nData and code exposed for public comments (https://github.com/ai4soilhealth) including the social media, especially Mastodon, X, Linkedin etc.\n\nIn principle, the publication process should be non-bureaucratic, agile and should not limit intonation and experimentation. However, for public releases approved by the consortium e.g. major deliverables as specified in the Grant Agreement, it is advised that these are approved by the Executive Board of the project so that the new dataset / service is then also officially promoted through media channels, landing page etc.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#reproducible-research",
    "href": "1-specifications.html#reproducible-research",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "In order to ensure FAIR outputs, it is highly recommended that all production steps used to generate maps are documented in code, best as Rmarkdown / Python Jupyter computational notebooks. As agreed also in the project proposal, All empirical data obtained will follow strict and validated processes of monitoring and evaluation to avoid any potential error. The data produced in AI4SoilHealth will be made available as open data, following the FAIR principle. The project Data Management Plan (DMP) is available and has even more instructions.\nAI4SoilHealth has agreed to provide open access (OA) to research outputs (e.g., publications, data, software) through deposition in trusted repositories. Partners will provide OA for peer-reviewed scientific publications relating to their results. Authors of all peer-reviewed scientific publications will store them in an OA trusted repository, during and after the project’s life following Article 17 and Annex 5 of the General Assembly. The consortium members will be encouraged to publish in the Open Research Europe data platform, specifically via the Zenodo community for AI4SoilHealth (https://zenodo.org/communities/ai4soilhealth/).\nBesides OA publication, the project aims for early and open sharing of the soil health data, and the research and technological developments including open data, open standards, open source software, and open communication:\n\nOpen data: The AI4SoilHealth consortium will build solutions upon open datasets published using genuinely open data licenses.\n\nOpen standards (interoperability): The use of open standards prevents lock-in by, or dependency on any single data, software or service supplier. AI4SoilHealth will fully adopt the principles of FAIR data management and open standards to enable interoperability of methods and services, both within the project and beyond.\nOpen source software: AI4SoilHealth plans to release the mobile phone application on Github under the MIT license. All major outputs of the project should be made available via the project github (https://github.com/ai4soilhealth).\nOpen communication: The consortium has experience with organizing live, open, and free discussion forums and workshops.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "2-points.html",
    "href": "2-points.html",
    "title": "Point data import and quality control",
    "section": "",
    "text": "This section provides detailed steps implemented to import and bind all soil laboratory / soil site data.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Point data import and quality control"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Soil Health Data Cube for pan-EU (SHDC4EU) is imagined as a compilation of pan-EU layers organized into a cloud-optimized data cube and served through an API / as analysis or decision ready data (in later phases of the project this data will be accessible also via a mobile phone app and various web-GIS applications). This is one of the main deliverables of AI4SoilHealth WP5 (“Harmonised EU-wide soil monitoring tools and services”) and is intended to cover most of the pan-EU mapping and cloud data services efforts. This document describes general development principles and specifications of the SHDC4EU and envisages the proposed optimal usage of the data cube. To read and understand what a data cube is and which technology is used to optimize access and usability, please refer to the medium article.\n\n\n\nWe provide code and examples of how to generate so-called Analysis-Ready training data sets. Some minimum conditions for a data set to be analysis ready include:\n\nIt requires no special pre-processing to remove artifacts, harmonize values within columns, bind or subset data;\nIt comes with extensive metadata so that there is no mistake about how was the data collected, prepared and distributed and by whom;\nIt is ready in some standard format e.g. Cloud Optimized GeoTIFF; for vector data sets we recommend using (open) cloud-native data formats to distribute data either Geopackage, Geoparquet, Flatgeobuf or similar;",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#who-this-book-is-for",
    "href": "preface.html#who-this-book-is-for",
    "title": "Preface",
    "section": "",
    "text": "Soil Health Data Cube for pan-EU (SHDC4EU) is imagined as a compilation of pan-EU layers organized into a cloud-optimized data cube and served through an API / as analysis or decision ready data (in later phases of the project this data will be accessible also via a mobile phone app and various web-GIS applications). This is one of the main deliverables of AI4SoilHealth WP5 (“Harmonised EU-wide soil monitoring tools and services”) and is intended to cover most of the pan-EU mapping and cloud data services efforts. This document describes general development principles and specifications of the SHDC4EU and envisages the proposed optimal usage of the data cube. To read and understand what a data cube is and which technology is used to optimize access and usability, please refer to the medium article.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#data-and-services",
    "href": "preface.html#data-and-services",
    "title": "Preface",
    "section": "",
    "text": "We provide code and examples of how to generate so-called Analysis-Ready training data sets. Some minimum conditions for a data set to be analysis ready include:\n\nIt requires no special pre-processing to remove artifacts, harmonize values within columns, bind or subset data;\nIt comes with extensive metadata so that there is no mistake about how was the data collected, prepared and distributed and by whom;\nIt is ready in some standard format e.g. Cloud Optimized GeoTIFF; for vector data sets we recommend using (open) cloud-native data formats to distribute data either Geopackage, Geoparquet, Flatgeobuf or similar;",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Preface"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This document provides information about the Soil Health Data Cube for pan-EU (SHDC4EU). The main purpose of the SHDC is to be as a platform for more detailed computing i.e. to estimate trends in important soil health indicators (e.g. Bare Soil Fraction, vegetation cover, chemical soil properties and similar). The SHDC is available via S3 (Simple Storage Service) and STAC as open data, which means that any researcher across EU can access data directly using rstac or similar, and fetch values / aggregate per polygon or farm. list will be continuously updated and extended.\n\nSoil Health Data Cube for pan-EU (SHDC4EU) is compilation of pan-EU layers organized into a data cube and served through an API as analysis or decision ready data. It will be published via the EcoDataCube.eu infrastructure and served with unrestricted access as a genuine open data project comparable to Copernicus Land Monitoring services, Zenodo.org, Amazon AWS Open Data and similar. It aims at serving the EU Soil Observatory hosted by the European Commission JRC. We are preparing a complete, consistent Data Cube which will include EU-wide seasonal crop-type maps, primary soil properties, land degradation indices, terrain parameters, and similar EO products, all at 30-m resolution and encompassing the period 2000–2024+, climatic time-series images (rainfall, soil moisture, surface temperature) and projected crop models (typically only at 1-km spatial resolution). These will then be combined into a unified complete consistent data cube that can be used to run machine learning models to detect and examine relationships between the field-estimated and EO-based SHI. The general workflow of using the SHDC4EU, in a nutshell, is: (1) assess the management and eventual degradation history of a piece of land, (2) assess current properties and states, and (3) assess potential of soil + land under different (climate change) scenarios. This is the basic framework which other WPs can use as a computing engine on top of which to build their particular applications. For processing: (1) data is used to assess the history of each pixel in terms of positive and negative trends in GPP, vegetation cover, SOC and other soil properties, (2) the most recent (actual) soil properties and classes will be provided to individual pixels / farms, and (3) the land potential will be estimated by extrapolating process-based and/or ML models to future climate land-use scenarios.\n\n\nCompiled data (imported, standardized, quality-controlled) is available through a diversity of standard formats:\n\nCOG files (compressed Cloud Optimized GeoTIFFs);\nGPKG files (Geopackage file ready to be opened in QGIS);\n\nAll files can be downloaded using the STAC browser.\n\n\n\nThe minimum requirements to submit a dataset for inclusion to Soil Health Data Cube are:\n\nPan-EU coverage (or at least aiming at the global coverage) AND,\n\nLicense and terms of use clearly specified AND,\n\nComplete and consistent metadata that can ensure correct standardization and harmonization steps AND,\n\nAt least 50 unique spatial locations AND,\n\nNo broken or invalid URLs,\n\nData sets that do NOT satisfy the above listed minimum requirements might be removed. If you discover an issue with license, data description or version number of a dataset, please open a Github issue.\nRecommended settings for all datasets are:\n\nPeer-reviewed versions of the datasets (i.e. a dataset accompanied with a peer-reviewed publication) should have the priority,\n\nRegister your dataset (use e.g. https://zenodo.org/communities/ai4soilhealth/) and assign a DOI to each version,\n\nProvide enough metadata so that it can be imported and bind with other data without errors,\n\nIf your dataset is a compilation of previously published datasets, please indicate in the description,\n\nInformation outdated or missing? Please open an issue or best do a correction and then a pull request.\n\n\n\nThis work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.\n\n\n\nUse liability: OpenGeoHub foundations cannot provide any warranty as to the accuracy, reliability, or completeness of furnished data. Users assume responsibility to determine the usability of these data. The user is responsible for the results of any application of this data for other than its intended purpose.\nDistribution liability: OpenGeoHub foundations make no warranty, expressed or implied, regarding these data, nor does the fact of distribution constitute such a warranty. OpenGeoHub foundations cannot assume liability for any damages caused by any errors or omissions in these data. If appropriate, OpenGeoHub foundations can only certify that the data it distributes are an authentic copy of the records that were accepted for inclusion in the OpenGeoHub foundations archives.\n\n\n\nTo cite this document please use:\n@book{shdc4eu_2024,\n  author       = {Hengl, T., Minarik, R., Tian, X., Parente, L., Ho, Y.-F., Consoli, D., Simoes, R., and contributors},\n  title        = {{Soil Health Data Cube for pan-EU: specifications and data tutorials}},\n  year         = {2024},\n  publisher    = {OpenGeoHub foundation},\n  address      = {Doorwerth},\n  version      = {v0.1},\n  doi          = {10.5281/zenodo.??},\n  url          = {https://shdc.ai4soilhealth.eu/}\n}\n\n\n\n\nOpenGeoHub foundation is a not-for-profit research foundation located in Wageningen, the Netherlands. We specifically promote publishing and sharing of Open Geographical and Geoscientific Data, using and developing Open Source Software and encouraging and empowering under-represented researchers e.g. those from ODA recipient countries and female researchers. We believe that the key measure of quality of research in all sciences (and especially in geographical information sciences) is in transparency and reproducibility of the computer code used to generate results (read more in: “Everyone has a right to know what is happening with the planet”).\n\n\n\nAI4SoilHealth.eu project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#download-compiled-data",
    "href": "index.html#download-compiled-data",
    "title": "Welcome",
    "section": "",
    "text": "Compiled data (imported, standardized, quality-controlled) is available through a diversity of standard formats:\n\nCOG files (compressed Cloud Optimized GeoTIFFs);\nGPKG files (Geopackage file ready to be opened in QGIS);\n\nAll files can be downloaded using the STAC browser.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#add-your-own-data",
    "href": "index.html#add-your-own-data",
    "title": "Welcome",
    "section": "",
    "text": "The minimum requirements to submit a dataset for inclusion to Soil Health Data Cube are:\n\nPan-EU coverage (or at least aiming at the global coverage) AND,\n\nLicense and terms of use clearly specified AND,\n\nComplete and consistent metadata that can ensure correct standardization and harmonization steps AND,\n\nAt least 50 unique spatial locations AND,\n\nNo broken or invalid URLs,\n\nData sets that do NOT satisfy the above listed minimum requirements might be removed. If you discover an issue with license, data description or version number of a dataset, please open a Github issue.\nRecommended settings for all datasets are:\n\nPeer-reviewed versions of the datasets (i.e. a dataset accompanied with a peer-reviewed publication) should have the priority,\n\nRegister your dataset (use e.g. https://zenodo.org/communities/ai4soilhealth/) and assign a DOI to each version,\n\nProvide enough metadata so that it can be imported and bind with other data without errors,\n\nIf your dataset is a compilation of previously published datasets, please indicate in the description,\n\nInformation outdated or missing? Please open an issue or best do a correction and then a pull request.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Welcome",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Welcome",
    "section": "",
    "text": "Use liability: OpenGeoHub foundations cannot provide any warranty as to the accuracy, reliability, or completeness of furnished data. Users assume responsibility to determine the usability of these data. The user is responsible for the results of any application of this data for other than its intended purpose.\nDistribution liability: OpenGeoHub foundations make no warranty, expressed or implied, regarding these data, nor does the fact of distribution constitute such a warranty. OpenGeoHub foundations cannot assume liability for any damages caused by any errors or omissions in these data. If appropriate, OpenGeoHub foundations can only certify that the data it distributes are an authentic copy of the records that were accepted for inclusion in the OpenGeoHub foundations archives.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "Welcome",
    "section": "",
    "text": "To cite this document please use:\n@book{shdc4eu_2024,\n  author       = {Hengl, T., Minarik, R., Tian, X., Parente, L., Ho, Y.-F., Consoli, D., Simoes, R., and contributors},\n  title        = {{Soil Health Data Cube for pan-EU: specifications and data tutorials}},\n  year         = {2024},\n  publisher    = {OpenGeoHub foundation},\n  address      = {Doorwerth},\n  version      = {v0.1},\n  doi          = {10.5281/zenodo.??},\n  url          = {https://shdc.ai4soilhealth.eu/}\n}",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about-opengeohub",
    "href": "index.html#about-opengeohub",
    "title": "Welcome",
    "section": "",
    "text": "OpenGeoHub foundation is a not-for-profit research foundation located in Wageningen, the Netherlands. We specifically promote publishing and sharing of Open Geographical and Geoscientific Data, using and developing Open Source Software and encouraging and empowering under-represented researchers e.g. those from ODA recipient countries and female researchers. We believe that the key measure of quality of research in all sciences (and especially in geographical information sciences) is in transparency and reproducibility of the computer code used to generate results (read more in: “Everyone has a right to know what is happening with the planet”).",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Welcome",
    "section": "",
    "text": "AI4SoilHealth.eu project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  }
]