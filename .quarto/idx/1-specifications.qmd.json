{"title":"Soil Health Data Cube specifications","markdown":{"headingText":"Soil Health Data Cube specifications","containsRefs":false,"markdown":"\n## The 7-step framework for soil health assessment\n\nIn a nutshell, Soil Health Data Cube is a building block and infrastructure \nfor monitoring soil health across EU. The general idea of the SHDC4EU is that \nit serves some standard processes that help inform land owners / land managers \nabout the soil health state of their land using open data. These processes are meant to help \nfuture users of the AI4SoilHealth app to quickly find out about the site, \nexplore land potentials based on the state-of-the-art models and start taking \nconcrete actions towards restoring land / improving soil health. This is closely\nconnected with the main expected impacts of the AI4SoilHealth project (to make \nsoil health data easier to access and provide tools and standards for soil health monitorng). In simple terms, the 7–step soil health assessment framework [@hengl2024osg] includes:\n\n1. **Assess the history of a site** (last 30+ years) provide answers to questions e.g.:\n    - How much SOC has been gained / lost over the last 30 years? \n    - Which chemical and physical soil properties changed the most?\n    - Is there any land/soil degradation and how significant is it?\n    - How has agronomic management impacted soil health?\n    - How much did the land cover / vegetation cover change? What is the main [change class](https://docs.google.com/spreadsheets/d/1SX51OilNt-cUYpAa7t0LAvZRTzq3Sd4WJLnX6mWKfQk/edit#gid=1828976853)?\n    - How much did the landscape heterogeneity change over time?\n2. **Determine current state of soil** (actual year):\n    - What is the current state of physical and chemical soil properties?\n    - What is the current (WRB) soil type / main soil forming processes and soil states?\n    - What are the current derived soil properties?\n    - What are the current macronutrient stocks in the soil?\n3. **Determine soil potential** (next 10, 20, 30 yrs):\n    - What is the SOC sequestration potential of this soil?\n    - What is the agricultural potential of this soil?\n    - What are the other potential ecosystem functions provided some land use change or mitigations?\n4. **Collect extra soil samples, send to lab, and add to training points**:\n    - Generate a sampling design that helps increase mapping accuracy. Optimize the number of samples while taking into account the costs of sampling.\n    - Collect new samples and send them to the lab.\n5. **Import new local data then re-analyze and re-asses points 1, 2 and 3**.\n    - Import new local samples / results and add to the training points. Re-assess local prediction accuracy.\n    - Repredict layers in SHDC4EU for steps #1, #2 and #3. \n6. **Suggest concrete KPI’s** (per farm / per land unit):\n    - Provided that soil potential assessment reveals a significant gap between potential and actual.\n    - Either directly provide concrete measures to help increase soil health and/or provide links to documents / organizations that can directly help increase soil health.\n7. **Track progress per farm and re-assess if required**:\n    - Every 1–2 years update the indicators (pan-EU).\n    - Compare with the planned transition in #6. If necessary recommend re-assessment.\n\nThis framework can be either apply to assess health of a specific site (either in the field using a mobile phone app, or in the office using a desktop app), or complete farms / administrative units. We, thus, aim to complete the data cube with primary and derived soil variables that can directly serve this framework, and especially so that can be worked alongside other Work Packages in the project in their testing of methodology.\n\n## Predictive soil mapping based on Machine Learning and HPC\n\nWP5 — with the tasks T5.2 Development of AI4SoilHealth computing engine and T5.3 Development of AI4SoilHealth Indicators data cube) — aims at implementing fully automated predictive soil mapping at high spatial resolution using large datasets (e.g. whole of Europe at 30-m spatial resolution). Prepared harmonized point datasets will are overlaid against time-series of EO images and/or static terrain-based or similar indices. These are then be used to predict values of target variables in 2D, 3D and 2D+T, 3D+T. The general workflow for predictive soil mapping based on **Spatiotemporal Machine Learning** and **High Performance Computing** (HPC) and assumes that all point data is analysis-ready, representative and correlates with the EO / covariate layers i.e. can be used to produce usable spatial predictions. Automated predictive soil mapping is currently implemented via the Python library [scikit-map](https://github.com/scikit-map/scikit-map) using OpenGeoHub’s High Performance Computing infrastructure. \n\nThe key interest of WP5 is to produce pan-EU predictions at the highest possible spatial resolution and that can directly serve soil health assessment at continental scale. Some variables are, however, not available for the whole of pan-EU, and some point datasets will be produced experimentally and used for testing purposes only. Hence, within the WP5, all soil variables are modeled / mapped under one the following 3 tiers:\n\n* **Tier 1: Pan-EU, production ready variables**: usually based on [LUCAS + national soil datasets](https://esdac.jrc.ec.europa.eu/projects/lucas);\n* **Tier 2 (internal, under construction): National / in-situ data**: Tested locally, then upscaled to whole of EU; usually based on collaboration of WP3,4,5,6;\n* **Tier 3 (internal, under construction): Pan-EU, new experimental variables** (currently not in the LUCAS soil and most likely can not be modeled/mapped across EU, but can only be used to assess soil health _at site_); based on the collaboration of WP3,4,5;\n\nWe expect that project partners + pilots will feed data for Tier 2 and Tier 3. However, not everything can be mapped across the EU, hence some variables will eventually stay available only locally and a small selection of variables will be available _in-situ_ only i.e. these will not be mapped at all.\n\n## Training points\n\nQuality of the outputs would most likely be dictated by the following four key aspects of modeling:\n\n1. **The general modeling design** i.e. how appropriate the statistical / machine learning methods are. For example: are all casualties considered, are all distributions represented, are all statistical assumptions met?\n2. **Predictive performance of the algorithms used** i.e. is the best / fastest algorithm used for modeling? Is the algorithm robust, noise-proof, artifacts-proof etc?\n3. **Quality and diversity of the covariate layers used**.\n4. **Quality and spatial and feature space representation of training points**. \n\nAssuming that #1 and #2 are optimized, then the only remaining factors that control success of the modeling / mapping processes are aspects #3 and #4 i.e. the quality of covariate layers and quality of training points. In the past (prior to 2010), it was relatively difficult to produce pan-EU maps of soil health indicators as there was only limited training point data. Thanks to the European Commission’s [LUCAS soil project](https://esdac.jrc.ec.europa.eu/projects/lucas), we now have 3–4 repetitions of ground measurements of soil properties of highest quality (about 22,000 sites are revisited per period) (see figure below). This is a unique opportunity to test building high resolution predictions, including dynamic soil property predictions. Ideally, such testing should be a joint effort of the AI4SoilHealth consortium, in an open collaboration with JRC and members of other sister projects funded by the same Soil mission.\n\nNevertheless, LUCAS soil is also not an ideal dataset for predictive mapping. \nHence producing SHDC4EU faces two serious challanges: (1) LUCAS soil is a top-soil dataset, \nalthough there is now intention to sample also subsoils, (2) it covers only a number \nof physical and chemical soil variables (10–15), and soil types or similar are \ntypically not recorded, making this largely a partial pedological survey. \n\n```{r lucas-plot, echo=FALSE, fig.cap=\"LUCAS soil samples [@orgiazzi2018lucas] and connected existing and upcoming EO missions (bars indicate approximated temporal coverage). Note that the amount of EO data and missions is increasing exponentially.\", out.width=\"100%\"}\nknitr::include_graphics(\"images/fig_lucas_points_scheme.jpg\")\n```\n\nTo improve usability of predictions produced using LUCAS we have decided, in this project, to combine both LUCAS and the highest quality legacy national soil datasets. These synchronized and analysis ready fusions of soil laboratory points and observations will be generated by T4.6 “Integration and harmonization of in-situ and ancillary observations”. We anticipate that this will be a long process hence, in order to prevent serious delays, we will start producing SHDC4EU predictions, then in each iteration try to improve predictions as new countries join the campaign of contributing their data (under a standard Data Sharing Agreement) for the purpose of data mining i.e. producing open soil information for everyone.\n\n## Covariate (base) layers\n\nThe second important aspect that will determine the quality of the SHDC4EU outputs is the quality of covariate layers. Quality of the covariate layers is basically determined by three things: (1) spatial and temporal resolution, (2) spectral / thematic content, (3) general quality of data in terms of completeness, consistency, correctness and quality of metadata. For producing SHDC4EU we plan to use an extensive compilation of covariate (base) layers to produce predictions for SHDC4EU. Already available layers for continental Europe include ([https://EcoDataCube.eu](https://EcoDataCube.eu)):\n\n* Land mask (pan-EU: [https://zenodo.org/doi/10.5281/zenodo.8171860](https://zenodo.org/doi/10.5281/zenodo.8171860)) + World Settlement Footprint (WSF) i.e. world buildings 2000-2015, 2019 ([https://geoservice.dlr.de/web/maps/eoc:wsfevolution](https://geoservice.dlr.de/web/maps/eoc:wsfevolution) and [https://geoservice.dlr.de/web/maps/eoc:wsf2019](https://geoservice.dlr.de/web/maps/eoc:wsf2019));\n* Digital Terrain Model variables at 6–scales 30-m, 60, 120, 240, 480 and 960-m:\n    * Elevation, slope (%), min- max-curvature,\n    * Hillshade, northness, easterness,\n    * Positive, negative openness,\n    * Topidx (TWI), geomorphon classes (10), catchment area, LS factor,\n* Lithological (surface geology) map of pan-EU at 250-m ([https://zenodo.org/doi/10.5281/zenodo.4787631](https://zenodo.org/doi/10.5281/zenodo.4787631)); \n* Bimonthly / quarterly GLAD Landsat composites (all bands + biophysical indices, 2000–2022) (explained in: [https://doi.org/10.21203/rs.3.rs-4251113/v1](https://doi.org/10.21203/rs.3.rs-4251113/v1)):\n    * Green, Red, NIR, SWIR1, SWIR2,\n    * NDVI, FAPAR,\n    * NDTI, NDWI, BSF,\n    * Cumulative NDVI, NDTI and BSF;\n* Climatic variables at 1-km (long-term or monthly series):\n    * CHELSA Climate Bioclimatic variables ([https://chelsa-climate.org/bioclim/](https://chelsa-climate.org/bioclim/));\n    * CHELSA monthly precipitation time-series 2000–2022 ([https://chelsa-climate.org/timeseries/](https://chelsa-climate.org/timeseries/)); \n    * MODIS LST daytime and nighttime monthly time-series 2000–2022 ([https://zenodo.org/doi/10.5281/zenodo.1420114](https://zenodo.org/doi/10.5281/zenodo.1420114));\n    * MODIS monthly water vapor time-series 2000–2022 ([https://zenodo.org/doi/10.5281/zenodo.8193738](https://zenodo.org/doi/10.5281/zenodo.8193738));\n    * Cumulative annual precipitation (2000–2022); \n* Monthly / annual snow images 2000-2022 e.g. DLR Snow pack at 500-m ([https://geoservice.dlr.de/web/maps/eoc:gsp:yearly](https://geoservice.dlr.de/web/maps/eoc:gsp:yearly)):\n    * Long-term monthly snow probability time-series at 500-m ([https://doi.org/10.5281/zenodo.5774953](https://doi.org/10.5281/zenodo.5774953)); \n    * Cumulative annual snow probability 2000–2022;\n* Surface water dynamics: occurrence probability of water long-term 1999–2021 ([https://glad.umd.edu/dataset/global-surface-water-dynamics](https://glad.umd.edu/dataset/global-surface-water-dynamics)); \n* Optional: Global Flood Database v1 (2000-2018) at 250-m annual flood event ([http://global-flood-database.cloudtostreet.info/](http://global-flood-database.cloudtostreet.info/)); \n* Light at night time-series at 500-m resolution 2000–2022 ([https://zenodo.org/doi/10.5281/zenodo.7750174](https://zenodo.org/doi/10.5281/zenodo.7750174));\n*  [Cropland extent](https://zenodo.org/doi/10.5281/zenodo.12527545) at 30-m for 2000 to 2022 based on ([https://glad.umd.edu/dataset/croplands](https://glad.umd.edu/dataset/croplands));\n*  [Bare soil percent](https://zenodo.org/doi/10.5281/zenodo.11961219) and photosythetical vegetation percent annual for 2000 to 2022 based on the MODIS MCD43A4 product (500-m spatial resolution);\n\nIn addition to the existing layers above, we will also generate a number of novel layers tailored specifically for the purpose of representing land use practices and potential soil degradation factors. This include:\n\n* BSI, **Bare Soil Index** [@mzid2021analysis] and annual BEF, **Bare Earth Fraction** (e.g. proportion of pixels with NDVI <0.35 based on 16–day and/or bimonthly data; there are other possible thresholds combination that can be used to optimize the results),\n* NDTI, **Normalized Differential Tillage Index** [@ettehadi2019separating],\n* NOS, **Number of Seasons** / NOCC,** Number of Cropping Cycles**,\n* LOS, **Length of Seasons** / CDR, **Crop Duration Ratio** [@estel2016mapping],\n* Crop-type based on [EuroCrops dataset](https://www.eurocrops.tum.de/),\n\nNote that from all covariate layers listed above, 16-day / bimonthly / quarterly GLAD Landsat composites (all bands + all indices, 2000–2022) are the largest part of the data to be used taking almost 20TB of storage (in compressed format).\n\n## Predictive mapping and derivation methods\n\nBased on the availability of the training points and nature of the target variable in the SHDC4EU, we will either predict or derive soil variables from primary variables. The output soil health indicators can, thus, be considered of type either:\n\n1. **Predicted dynamic pan-EU variables** (2000–2022+) available at standard depth intervals (0–20, 20–50, 50–100, 100–200 cm).\n2. **Predicted static pan-EU variables**.\n3. **Predicted and/or simulated pan-EU variables** i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling.\n4. **Derived pan-EU variables and indices**.\n5. **In-situ only variables**.\n\nMost of the soil chemical and biological variables are mapped as dynamic variables with annual or 5–year estimates (e.g. 5 maps for period 2000–2025) and at multiple depths [@witjes2023ecodatacube]. This means that the amount of produced data can be significant if predictions are also provided per depth. For example, to map soil organic carbon content (weight %), we can predict 23 annual maps at 4–5 standard depths resulting in over 180 images (assuming that we also produce prediction errors per pixel). AI4SoilHealth tasks T5.5 \"Development of AI4SoilHealth present and future soil degradation products\", T5.6 \"Development of AI4SoilHealth forecasting services\", and T5.7 \"Development of AI4SoilHealth soil functions evidence / model chains\" will also generate large amounts of predicted and/or simulated pan-EU variables i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling. \n\nThe following predictive soil modeling methods are considered for use so far for generating SHDC4EU:\n\n1. 2D and 3D predictive soil mapping (static predictions, time is ignored).\n2. 2D+T predictive soil mapping spacetime.\n3. 3D+T predictive soil mapping spacetime.\n4. Predictive soil mapping focused on long-term indices.\n5. Derivation from primary data.\n6. Derivation using Pedo-transfer functions.\n7. Derivation / simulations using Mechanistic model (iterative).\n8. Derivation of future projected predictions (scenario testing).\n\nFor each derivation method we use sets of algorithms, usually implemented in python or R programming languages. These are documented in detail and eventually allow for complete reproducibility of results; most importantly we do a series of benchmarking (listed in further sections) to compare predictive performance and select the algorithm that is most accurate + most robust at the same time. In the tasks T5.5 Development of AI4SoilHealth present and future soil degradation products, T5.6 Development of AI4SoilHealth forecasting services and T5.7 Development of AI4SoilHealth soil functions evidence / model chains, also **process-based modeling** can be used, especially to generate potential soil ecosystem services etc. Such modeling is at the order-of-magnitude more computationally demanding than predictive mapping hence we expect that these outputs will be of limited spatial detail (1-km) and or available only for experimental testing.\n\n## Targeted soil and soil-health variables\n\nThe general objective of the SHDC4EU is to serve best possible, most detailed, complete and consistent predictions of the the number of targeted soil health indicators in the EU Mission’s [“Implementation Plan: A Soil Deal for Europe”](https://research-and-innovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-calls/horizon-europe/eu-missions-horizon-europe/soil-deal-europe_en): \n\n1. presence of pollutants, excess nutrients and salts, \n2. soil organic carbon stock, \n3. soil structure including soil bulk density and absence of soil sealing and erosion, \n4. soil biodiversity, \n5. soil nutrients and acidity (pH), \n6. vegetation cover, \n7. landscape heterogeneity, \n8. forest cover. \n\nIf these are available for the complete land mask of pan-EU area, these can then be used to assess soil degradation state (e.g. salinization / sealing level and trends, concentration of excess nutrients and pollutants and trends, erosion state and trends, loss of SOC and trends etc), current soil properties and soil potential in terms of potential ecosystem services / potential SOC sequestration etc, potential productivity of soil, potential soil biodiversity etc. The working version of what we find as feasible to map at high spatial resolution is provided below.\n\nNote that some of the soil health indicators recommended by the European Commission / JRC, are not _defacto_ soil variables (e.g. vegetation cover, landscape heterogeneity, forest cover) but will be generated in this project and integrated into SHDC4EU. Some potential options to represent the vegetation cover, landscape heterogeneity etc include:\n\n* **[Canopy height](https://glad.earthengine.app/view/europe-tree-dynamics)**, data already available for EU but could also be improved by outputs from the Open-Earth-Monitor project,\n* GPP, **Gross Primary Productivity** (bimonthly and/or annual; based on the last 2–5yrs),\n* Land cover and land use / **cropping systems and applications** (categories e.g. based on the EuroCrops), producing cropping system maps for the EU; \n\nHowever, producing [cropping system maps](https://joint-research-centre.ec.europa.eu/jrc-news-and-updates/eu-crop-map-2021-10-18_en) for the EU, even only for the recent year is not trivial and producing such data should is at the moment optional.\n\nList of target soil variables that will be delivered in the SHDC4EU includes dynamic soil properties covering period 2000–2025+:\n\n*  **Soil organic carbon density** (kg/m3) ISO 10694. 1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year estimates;\n*  **Soil carbon stock change** (t/ha/yr) ISO 10694. 1996 for 0–20, 20–50, 50–100 cm depth intervals; long-term\n*  **Soil pH in a suspension of soil in water** (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 depth intervals, 5–year estimates;\n*  **Soil pH measured in a CaCl2 solution** (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\n*  **Soil total nitrogen content** (dg/kg) ISO 11261:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\n*  **Soil bulk density** (t/m3) Adapted ISO 11272:2017 for 0–20, 20–50, 50–100 cm, 5–year;\n*  **Soil texture fractions** (sand, silt, clay) (g/g) ISO:11277 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\n*  **Soil WRB subgroup** (factor / probs) based on the WRB2022 classificaton system for 0–200 cm, long-term;\n*  **Depth to bedrock** (cm) up to 200 cm, long-term;\n*  **Extractable potassium content** (mg/kg) USDA−NRCS, 2004 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\n*  **Carbonates content CaCO3** (g/g) ISO 10693:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\n*  **Extractable Phosphorus content (Olsen)** (mg/kg) ISO 11263. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\n*  **Monthly Gross Primary Productivity** (kg/ha/yr) FluxNET bi-monthly;\n*  **Bare Soil Fraction** (m2/m2) Landsat-based, 5–year;\n\n## Variable registry\n\nFor each variable we will organize a soil variable registry system, so that each variable should have unique (short-as-possible) code. As simple coding system for SHDC4EU is suggested where the variable name is a combination of the three components:\n\n* generic variable name,\n* specific laboratory / field / O&M method (ISO standard or similar),\n* measurement unit,\n\nFor example, for SOC content in weight percent we recommend using `oc_iso.10694.1995_wpct` (dry combustion) or `oc_iso.17184.2014_wpct`; for SOC in permiles you can use e.g. `oc_iso.10694.1995_wpml`. Note that here we use ISO code, however since ISO is highly commercial and not-practical for open datasets, we recommend using the ISO code (allowed), but in fact pointing to a scientific reference i.e. a publication or PDF that is at the order of scale easier to obtain (instead of pointing to ISO website or similar). For example the variable `oc_iso.10694.1995_wpct` can be linked to the scientific reference [Nelson et al. (1982)](https://acsess.onlinelibrary.wiley.com/doi/abs/10.2134/agronmonogr9.2.2ed.c29) (also used by ISO).\n\nSoil variable names can be also provided in a shorter version (assuming that only 1 reference method exists) as a combination of the variable name and measurement unit code e.g. `oc.wcpt`. It is very important that all partners in the project consistently use the same codes, and if there are updates in the naming convention that they refer to the version of the code.\n\n## Spatio-temporal reference\n\nThe general interest of this project is to produce a time-series of predictions of key soil/vegetation/land use variables with either annual or monthly support and covering the period 1997–2022+ and at highest spatial resolution e.g. 30-m. For variables that vary by depth, we will predict at 5 standard depths (0, 20, 50, 100 and 200 cm), then aggregate values to standard depth intervals:\n\n* 0–20 cm (**topsoil**) LUCAS standard;\n* 20–50 cm (**subsoil1**);\n* 50–100 cm (**subsoil2**);\n* 100–200 cm (**subsoil3**);\n\nTemporal support of predictions / simulations can be one of the following:\n\n* Long-term primary soil properties and classes: e.g. soil types, soil water holding capacity;\n* Annual to 5–year values of primary soil and vegetation variables: e.g. annual bare soil fraction (index);\n* Bimonthly, monthly or seasonal values of soil and vegetation variables: e.g. bimonthly GPP. \n* Weekly, 16–day values (original Landsat GLAD),\n* Daily or hourly values (probably not of interest in this project).\n\nAggregated values of target soil and vegetation variables can also refer to some part of the distribution e.g. quantile e.g. P25, P50 (median / mean value) and P75.\n\n## Uncertainty of predictions\n\nAs an ideal case, each predictive mapping model should produce / provide also \nuncertainty per pixel (prediction error) and summary results of robust cross-validation. \nTwo options are possible for predictive error mapping:\n\n- **Provide lower and upper quantiles** `p=0.05` and `p=0.95` so that a 90% probability prediction interval can be derived. For variables with skewed distribution / log-normally distributed it is recommended that the lower values are computed in the log-transformed space to avoid predicting negative values.\n- **Provide prediction error as 1 standard deviation**: from this also prediction interval can be derived, assuming that the variable is normally distributed. \n\nAs a general recommendation we suggest using the **Conformal Prediction method** \nto produce prediction errors per pixel, best as implemented in the python libraries [mappie](https://mapie.readthedocs.io/en/latest/)\nand/or [puncc](https://github.com/deel-ai/puncc).\n\n## Back-end / front-end components\n\nThe back-end of the SHDC4EU is based on using open source software such as [PostGIS/PostGreSQL](https://postgis.net/) [Rio-tiler](https://cogeotiff.github.io/rio-tiler/), [FastAPI](https://fastapi.tiangolo.com/) and S3 in the back-end; [Vue.js](https://vuejs.org/), OpenLayers and Geoserver in front end. We are aiming at using / building upon simple scalable solutions built on top of existing open source solutions. Note that the most important about this back-end design is that the system will be (A) **cloud-native** i.e. building upon cloud-optimized solutions, (B) easy to extend, (C) focused on usability of data i.e. serving seamless layers (complete, consistent, documented, version-controlled with a live support channels).\n\nAll pan-European layers (COGs) produced in this project, including majority of covariate layers, will be distributed through an existing infrastructure [http://EcoDataCube.eu](http://EcoDataCube.eu) (maintained by OpenGeoHub foundation). This means that all layers will be made available:\n\n* For viewing i.e. as **Web Mapping Service** (WMS) so it can be displayed using OpenLayers or similar;\n* For direct data access via **Simple Storage Service** (S3) / files registered via the **SpatioTemporal Asset Catalog** (STAC) via [https://stac.ecodatacube.eu](https://stac.ecodatacube.eu);\n* Back-up copy available via the **NextCloud** including via [WebDAV](https://kwb-r.github.io/kwb.nextcloud/).\n\nOpenGeoHub will be responsible that the data is available openly in-real-time (i.e. through S3 service) up to 5 years after the end of project, and that a copy of the data is archived on Zenodo or similar.\n\nWP5 will focus exclusively on pan-EU modeling, however, we might also use national / pilot data to test methods and develop experimental solutions that potentially could have much higher impact. For this purpose it is crucial that all project partners have access to the NextCloud and can freely collaborate on data without a need to have multiple copies and installations.\n\nAll tabular vector data will be stored in a **single PostGIS DB** and where possible made available to project participants (a GeoPKG copy of the point / polygon data will also be made available via the project Nextcloud). For land mask / geospatial standards we will rely on the [Copernicus Land monitoring infrastructure](https://land.copernicus.eu/pan-european) so we will also include Turkey / Western Balkans and Ukraine i.e. also all candidate countries. For data exchange and sharing we use Nextcloud or similar. This data is however not publicly available.\n\n## Data exchange formats\n\nAll project partners agree to use standard formats to exchange data within the project and on the project websites. The recommended file formats for gridded spatial / spatiotemporal data include:\n\n* [Cloud-Optimized GeoTIFF](https://www.cogeo.org/) (`*.tif`).\n* [Zarr](https://gdal.org/drivers/raster/zarr.html) (`*.zarr`) (also in combination with [NetCDF](https://gdal.org/drivers/raster/netcdf.html) (`*.nc`).\n\nFor vector geospatial data we recommend the following file formats:\n\n* For data subsets: [GeoJSON](https://geojson.org/) and/or KML.\n* For cloud-data serving: [Geoparquet](https://github.com/opengeospatial/geoparquet) (`*.geoparquet`) and/or [lance](https://github.com/eto-ai/lance).\n* For visualization: [Mapbox Vector Tiles](https://gdal.org/drivers/vector/mvt.html) (`*.mvt`).\n* For GIS analysis: [Geopackage](https://www.geopackage.org/) (`*.gpkg`).\n\nFor tabular data, objects as [Simple Features](https://en.wikipedia.org/wiki/Simple_Features) and similar, the following file formats are accepted for delivery:\n\n* Comma Separated Value (`*.csv`) and [GeoCSV](https://giswiki.hsr.ch/GeoCSV) best compressed as `*.csv.gz`. \n* R RDS (`*.rds`) and/or [QS](https://github.com/traversc/qs) files that can be [red and written in parallel](http://mgimond.github.io/ES218/Week02b.html).\n* [GeoJSON](https://gdal.org/drivers/vector/geojson.html#vector-geojson) (`*.json`).\n* [FlatGeobuf](https://github.com/flatgeobuf/flatgeobuf) (`*.fgb`).\n\nFor geospatial data to be FAIR, it should at least pass the following checks:\n\n* It is decision-ready, or at least analysis-ready (complete consistent optimized);\n* It is available in a professional catalog e.g. [STAC catalog](https://stacspec.org/) and/or [Geonetwork](https://geonetwork-opensource.org/);\n* It comes with technical documentation (ideally a peer-reviewed publication) / links to Github / Gitlab where users can find technical explanation of how was the data produced;\n* It has a version and each version has unique [DOI](https://www.doi.org/);\n* It can be accessed directly i.e. file URL is available for HTTP requests (through S3 or similar).\n\nFor Cloud-Optimized GeoTIFFs (COG’s) it is highly recommended that all files are prepared using recommended settings i.e. a [command line](https://github.com/cogeotiff/cog-spec/blob/master/spec.md) (>GDALv3.2):\n\n```\ngdal_translate in.vrt out.tif -co TILED=YES -co COPY_SRC_OVERVIEWS=YES -co COMPRESS=LZW\n```\n\nEach COG will be quality controlled using (1) [COG validator](https://github.com/rouault/cog_validator), (2) by visual inspection, (3) random sampling point overlay to certify that >99% of pixels are available. After the quality control, all produced global mosaics will be registered and uploaded to S3 storage or similar.\n\n## Standard spatial/temporal resolutions and support sizes\n\nFor the sake of consistency and compatibility, project participants will use standard spatial resolutions to deliver and exchange data. Recommended standard pixel sizes / resolutions:\n\n_Table: Standard bounding box and spatial resolutions._\n\n<table>\n  <tr>\n   <td>Europe COG \\\nBounding box\n   </td>\n   <td>Continental EU COG based on <a href=\"https://land.copernicus.eu/portal_vocabularies/geotags/eea39\">Copernicus</a>  \\\nSpatial resolutions and image size <a href=\"https://epsg.io/3035\">EPSG:3035</a>\n   </td>\n  </tr>\n  <tr>\n   <td>Xmin = 900,000\n<p>\nYmin = 899,000\n<p>\nXmax = 7,401,000\n<p>\nYmax = 5,501,000\n   </td>\n   <td>10m | 650,100L x 460,200P\n<p>\n25m | 260,040L x 184,080P\n<p>\n30m | 216,700P x 153,400L\n<p>\n100m | 65,010P x 46,020L\n<p>\n250m | 26,004P x 18,408L\n<p>\n1km | 6501P x 4602L\n   </td>\n  </tr>\n</table>\n\n\n## File Naming convention\n\nSHDC will consistently use the standard OpenLandMap file-naming convention to submit new data-sets and similar (compare e.g. with the MODIS file naming convention). This is to ensure consistency and ease of use within the AI4SoilHealth project, but also by the end-users. This applies especially to WP4, WP5 and WP6. \n\nThe OpenLandMap file-naming convention works with 10 fields that basically define the most important properties of the data (this way users can search files, prepare data analysis etc, without even needing to access or open files. The 10 fields include:\n\n1. Generic variable name (needs to be unique and approved by the AI4SoilHealth Coordination team): `lclu`;\n2. Variable procedure combination i.e. method standard (standard abbreviation): `luisa`;\n3. Position in the probability distribution / variable type: `c`;\n4. Spatial support (usually horizontal block) in m or km: `30m`;\n5. Depth reference or depth interval e.g. below (\"b\"), above (\"a\") ground or at surface (\"s\"): `s`;\n6. Time reference begin time (YYYYMMDD): `20210101`;\n7. Time reference end time: `20211231`;\n8. Bounding box (2 letters max): `eu`; \n9. EPSG code: `epsg.3035`;\n10. Version code i.e. creation date: `v20221015`;\n\nAn example of a file-name based on the description above:\n\n```\nlclu_luisa_c_30m_s_20210101_20211231_eu_epsg.3035_v20221015.tif\n```\n\nNote that this file naming convention has the following properties:\n\n* Large quantities of files can be easily sorted and searched (one line queries in [Bash](https://www.gnu.org/software/bash/)).\n* File-naming patterns can be used to seamlessly build virtual mosaics and composites.  \n* Key spatiotemporal properties of the data are available in the file name e.g. variable type, O&M method, spatial resolution, bounding box, projection system, temporal references. Users can program analysis without opening or testing files.\n* Versioning system is ubiquitous.\n* All file-names are unique.\n\nGeonetwork and STAC will be further used to link the unique file names to: (1) WPs, deliverables, themes / keywords, (2) DOI’s, (3) project homepages, (4) contact pages for support and feedback. For keywords we recommend using the [INSPIRE keywords](https://inspire.ec.europa.eu/glossary). To confirm that metadata is complete and consistent, we recommend using the [INSPIRE metadata validator](https://inspire.ec.europa.eu/validator/home/index.html) and/or [https://data.europa.eu/en](https://data.europa.eu/en) validator.\n\nSome simple additional rules for generating the file name include:\n\n1. Codes and abbreviations should be human-readable as much as possible (hence short, but not too short!);\n2. Use only English-US (en-us) language e.g. for months use `jan`, `feb` etc;\n3. Consistently use [UNICODE standard](https://unicode.org/standard/standard.html): small letters only, no blank spaces, no non-ASCII characters;\n4. Limit the total file name size in characters to `256`;\n5. For time reference do not extend beyond hour minute and timezone;\n6. For bounding boxes use as much as possible the 2–letter unique country code; for continents use the Equi7 Grid code i.e. `eu`,\n7. For method codes use as much as possible unique IDs from [ISO - ICS](https://www.iso.org/standards-catalogue/browse-by-ics.html); \n8. For MODIS products use consistently the MODIS products codes e.g. [MOD11A2 v061](https://lpdaac.usgs.gov/products/mod11a2v061/); \nFor long-term aggregates of seasonal, monthly, weekly values use the period name at the end of the method names (#3) for example the long-term estimate of MODIS LST daytime temperature for month August:\n\n```\nlst.d_mod11a2v061.aug_m_1km_s_20200101_20211231_eu_epsg.3035_v20221015.tif\n```\n\nA list of vocabularies to be used as abbreviated names of variables will be provided by OpenGeoHub. The same file-name convention described above can be also used for vector data (this would only have a different file extension) also. \n\n## Registering project outputs\n\nAll project participants are required to register project outputs and inform other project participants about the progress (via [https://zenodo.org/communities/ai4soilhealth/](https://zenodo.org/communities/ai4soilhealth/)). For internal documents / draft versions of outputs the following submission principles need to be followed:\n\n* For internal datasets and documents use project Gitlab and Mattermost to inform parties about the progress and receive feedback;\n* Regularly inform your WP about the progress and planned dates (best via Mattermost channel).\n* To receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\n* For each new version of the output specify in short terms what has been improved, what has changed from last version.\n* Tag people in Mattermost that you would like to request to review the outputs.   \n\nFor official releases please refer to the Consortium Agreement for the correct procedure. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\n* First register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the **minimum requirements** regarding file formats and file naming, documentation and metadata.\n* Make sure that no parts of the submission have any **copyright issues**. \n* Make sure that **all contributors are credited in detail**.\n* Make sure you provide appropriate **disclaimer** and **terms of use**; we recommend using “No warranty” as default.\n* Make sure the **license** is compatible to the AI4SoilHealth project following our Consortium Agreement;\n* Make sure **acknowledgement** (see below) is clearly provided.\n* Make sure you provide **contact information** and instructions for users to provide feedback and request support.\n* Make sure all project partners are informed about the date of release (through Gitlab and Mattermost).\n\nSuggested 3rd party platforms for registering outputs (we recommend using multiple of the listed options e.g. a, b, c etc):\n\n1. Datasets: \n    1. [https://zenodo.org/communities/ai4soilhealth/](https://zenodo.org/communities/ai4soilhealth/) and/or [https://onedata.org/](https://onedata.org/),\n    2. Metadata entry via: Geonetwork and/or STAC,\n    3. Structured summary via: [https://ai4soilhealth.eu](https://ai4soilhealth.eu),\n    4. Suggestion: registered on [https://data.europa.eu/en](https://data.europa.eu/en),\n2. Software: \n    5. Github ([https://github.com/ai4soilhealth](https://github.com/ai4soilhealth)) -> suggestion: always generate DOI and put on Zenodo.org,\n    6. [https://archive.softwareheritage.org/](https://archive.softwareheritage.org/), \n    7. Structured summary via: [https://ai4soilhealth.eu](https://ai4soilhealth.eu),\n3. Tutorials / data catalogs:\n    8. Webpages on github / gitlab (Rbookdown, python books) see e.g. [https://opengeohub.github.io/SoilSamples/](https://opengeohub.github.io/SoilSamples/),\n    9. MKdocs see e.g. [https://gee-community-catalog.org/](https://gee-community-catalog.org/); \n4. Data portals / web-GUI’s\n    10. Sub-domain under *.ai4soilhealth.eu,\n    11. Recommended portal: [https://gkhub.earthobservations.org/](https://gkhub.earthobservations.org/),\n    12. Video-tutorial published via: [https://av.tib.eu/](https://av.tib.eu/), \n\nWe are planning to have our own installation of Gitlab, [STAC browser](https://github.com/radiantearth/stac-browser), [Geonetwork](https://geonetwork-opensource.org/) and [Pretalx](https://pretalx.com/). Final workflow for registering outputs will be specified in the final version of the implementation plan.\n\nDuring publishing of outputs, and especially if you register outputs via 3rd party repos, it of utmost importance that all partners use the correct attribution and links to project homepage:\n\n* Correct attribution: “The AI4SoilHealth project project has received funding from the European Union's Horizon Europe research an innovation programme under grant agreement [No. 101086179](https://cordis.europa.eu/project/id/101086179).”; \n* Project URL: [https://cordis.europa.eu/project/id/101086179](https://cordis.europa.eu/project/id/101086179).\n* Project homepage: [https://ai4soilhealth.eu](https://ai4soilhealth.eu),\n* Correct default disclaimer is below.\n\n## Disclaimer for data / software products\n\nAll public releases of new data / software should point to the generic AI4SoilHealth disclaimer:\n\n> “Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or European Commision. Neither the European Union nor the granting authority can be held responsible for them. The data is provided “as is”. AI4SoilHealth project consortium and its suppliers and licensors hereby disclaim all warranties of any kind, express or implied, including, without limitation, the warranties of merchantability, fitness for a particular purpose and non-infringement. Neither AI4SoilHealth Consortium nor its suppliers and licensors, makes any warranty that the Website will be error free or that access thereto will be continuous or uninterrupted. You understand that you download from, or otherwise obtain content or services through, the Website at your own discretion and risk.”\n\n\n## Land mask\n\nConsidering the land mask for pan-EU (see figure), we will closely match the data coverage of [Copernicus pan-european](https://land.copernicus.eu/pan-european) i.e. the official selection of countries listed [here](https://lanEEA39d.copernicus.eu/portal_vocabularies/geotags/eea39). Note the mask covers also all EU integration candidate countries, but can eventually also be subset to only European Union countries.\n\n```{r landmask, echo=FALSE, fig.cap=\"Landmask for the SHDC for pan-EU.\", out.width=\"70%\"}\nknitr::include_graphics(\"images/land_mask_paneu.png\")\n```\n\nThere are a total of three landmask files available, each of which is aligned with the standard spatial/temporal resolution and sizes of SHDC4EU specifications. Additionally, these files include a corresponding look-up table that provides explanations for the values present in the raster data.\n\n_Table: Technical description of the pan-EU land mask._\n\n\n<table>\n  <tr>\n   <td rowspan=\"5\" >Landmask\n   </td>\n   <td>Purpose/principle\n   </td>\n   <td>The basic principle to create the land mask is to include as much as land as possible, to avoid missing any land pixels and ensure precise differentiation between land, ocean and inland water bodies.\n<p>\nWhen generating the land mask, the two reference datasets in a way that:\n<ul>\n\n<li>If either of the two reference datasets identifies a pixel as land, it is considered a land pixel in our mask. \n\n<li>Regarding ocean and inland water bodies, a pixel is classified as a water pixel only when both reference datasets confirm its identification as water.\n</li>\n</ul>\n   </td>\n  </tr>\n  <tr>\n   <td>Reference datasets\n   </td>\n   <td>\n<ol>\n\n<li><a href=\"https://esa-worldcover.org/en\">WorldCover</a>, 10 m resolution.\n\n<li><a href=\"https://www.mapsforeurope.org/datasets/euro-global-map\">EuroGlobalMap</a>, with shapefiles of administrative boundaries, inland water bodies, ocean and landmask.\n</li>\n</ol>\n   </td>\n  </tr>\n  <tr>\n   <td>Mosaic/resampling method\n   </td>\n   <td>The coarse resolution landmasks (>10 m) are generated by resampling from the 10m resolution base map using resampling method “min” in GDAL. This “min” method allows taking the minimum values from the contributing pixels, to keep as much land as possible.\n   </td>\n  </tr>\n  <tr>\n   <td>Resolution available\n   </td>\n   <td>10-m, 30-m, 100-m, 250-m, and 1-km resolution\n   </td>\n  </tr>\n  <tr>\n   <td>Mask values\n   </td>\n   <td>\n<ul>\n\n<li>10: not in the pan-EU area, i.e. out of mapping scope\n\n<li>1: land\n\n<li>2: inland water\n\n<li>3: ocean\n</li>\n</ul>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"5\" >ISO-3166 country code mask\n   </td>\n   <td>Purpose/principle\n   </td>\n   <td>In this mask, each country is assigned a unique value, which allows for the interpretation and analysis of data associated with a specific country. The values are assigned to each country according to iso-3166 country code, which can be found in the corresponding look-up table.\n   </td>\n  </tr>\n  <tr>\n   <td>Reference datasets\n   </td>\n   <td><a href=\"https://www.mapsforeurope.org/datasets/euro-global-map\">EuroGlobalMap</a> country shapefile\n   </td>\n  </tr>\n  <tr>\n   <td>Mosaic/resampling method\n   </td>\n   <td>The coarse resolution masks (>10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n   </td>\n  </tr>\n  <tr>\n   <td>Resolution available\n   </td>\n   <td> 10m, 30m and 100m \n   </td>\n  </tr>\n  <tr>\n   <td>Mask values\n   </td>\n   <td>Can be found in the corresponding look-up table.\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"5\" >NUTS-3 mask\n   </td>\n   <td>Purpose/principle\n   </td>\n   <td>In this raster file, each unique NUT3 level area is assigned a unique value, which allows for the interpretation and analysis of data associated with specific NUTS-3 regions. Compared to ISO-3166 country code mask, NUTS-3 mask shows more details about regional administrative boundaries.\n   </td>\n  </tr>\n  <tr>\n   <td>Reference datasets\n   </td>\n   <td>European NUTS-3 shapefile\n   </td>\n  </tr>\n  <tr>\n   <td>Mosaic/resampling method\n   </td>\n   <td>The coarse resolution masks (>10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n   </td>\n  </tr>\n  <tr>\n   <td>Resolution available\n   </td>\n   <td> 10 m, 30 m and 100 m \n   </td>\n  </tr>\n  <tr>\n   <td>Mask values\n   </td>\n   <td>Can be found in the corresponding look-up table.\n   </td>\n  </tr>\n</table>\n\n\nAn important point to consider is that the ISO-code country mask provides a wider geographical coverage compared to the NUTS3 mask. This extended coverage includes countries such as Ukraine and others that lie beyond the NUTS3 administrative boundaries. Both the land mask and administrative code mask are in an Equal-area projection, allowing for accurate area estimation and facilitating aggregation per political/administrative unit.\n\nThese masks will be published and shared in a publicly available way on Zenodo. The working version (v0.1) is available from: [ https://doi.org/10.5281/zenodo.8171860](https://doi.org/10.5281/zenodo.8171860). The layers can be opened directly in QGIS by copying links from Zenodo. The scripts used to generate these masks can be found in our [project Gitlab](https://gitlab.opengeohub.org/ai4soilhealth/internal-implementation/-/issues) or via the [public Github](https://github.com/AI4SoilHealth/SoilHealthDataCube/). If any issue / problem is noticed, please report.\n\n## Layer submission process and quality control\n\nPartners on consortium are invited to submit working versions of data and services / share preliminary outputs internally via NextCloud or similar. Folder structure and more detailed instructions will be provided by the WP lead OpenGeoHub. It is important however to distinguish between (1) internal releases, (2) public releases (partners responsible) and (3) public releases approved by the consortium. \n\nAll WP5 participants are required to register project outputs and inform other project participants about the progress. For internal documents / draft versions of outputs the following submission principles need to be followed:\n\n* For internal datasets and documents use project repositories, Gitlab, NextCloud and Mattermost to inform parties about the progress and receive feedback;\n* Regularly inform your WP/Task group about the progress and planned dates (best via Mattermost channel).\n* To receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\n* For each new version of the output specify in short terms what has been improved, what has changed from last version.\n* Tag people in Mattermost that you would like to request to review the outputs.   \n\nFor official releases please refer to these guidelines to avoid any delays. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\n* First register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the **minimum requirements** regarding file formats and file naming, documentation and metadata.\n* Make sure that no parts of the submission have any **copyright issues**. \n* Make sure that **all contributors are credited in detail**.\n* Make sure you provide appropriate **disclaimer** and **terms of use**; we recommend using “No warranty” as default.\n* Make sure the **license** is compatible to the AI4SoilHealth project following our Consortium Agreement;\n* Make sure **acknowledgement** ([https://cordis.europa.eu/project/id/101086179](https://cordis.europa.eu/project/id/101086179)) is clearly provided.\n* Make sure you provide **contact information** and instructions for users to provide feedback and request support.\n* Make sure all project partners are informed about the date of release (through Gitlab and Mattermost).\n* If there are any deviations from specifications you have put in the implementation plan, please indicate such deviations in the project management system.\n\nFor quality assurance (public releases) we recommend the following three checks for any new significant datasets:\n\n1. Preliminary data and code shared with all WP members via NextCloud / code on Gitlab / Github. Internal check in co-development sessions. \n2. Peer-review publications (as agreed in the proposal, all major new data products should go through peer-review).\n3. Data and code exposed for public comments ([https://github.com/ai4soilhealth](https://github.com/ai4soilhealth)) including the social media, especially Mastodon, X, Linkedin etc.\n\nIn principle, the publication process should be non-bureaucratic, agile and should not limit intonation and experimentation. However, for public releases approved by the consortium e.g. major deliverables as specified in the Grant Agreement, it is advised that these are approved by the Executive Board of the project so that the new dataset / service is then also officially promoted through media channels, landing page etc.\n\n## Reproducible research\n\nIn order to ensure FAIR outputs, it is highly recommended that all production steps used to generate maps are documented in code, best as **Rmarkdown / Python Jupyter computational notebooks**. As agreed also in the project proposal, All empirical data obtained will follow strict and validated processes of monitoring and evaluation to avoid any potential error. The data produced in AI4SoilHealth will be made available as open data, following the **FAIR **principle. The project **Data Management Plan (DMP)** is available and has even more instructions.\n\nAI4SoilHealth has agreed to provide **open access (OA)** to research outputs (_e.g._, publications, data, software) through deposition in trusted repositories. Partners will provide OA for peer-reviewed scientific publications relating to their results. Authors of all peer-reviewed scientific publications will store them in an OA trusted repository, during and after the project’s life following Article 17 and Annex 5 of the General Assembly. The consortium members will be encouraged to publish in the Open Research Europe data platform, specifically via the Zenodo community for AI4SoilHealth ([https://zenodo.org/communities/ai4soilhealth/](https://zenodo.org/communities/ai4soilhealth/)). \n\nBesides OA publication, the project aims for **early and open sharing of the soil health data, and the research and technological developments **including open data, open standards, open source software, and open communication:\n\n* **Open data**: The AI4SoilHealth consortium will build solutions upon open datasets published using genuinely [open data licenses](https://opendefinition.org/licenses/).   \n* **Open standards (interoperability)**: The use of open standards prevents lock-in by, or dependency on any single data, software or service supplier. AI4SoilHealth will fully adopt the principles of FAIR data management and open standards to enable interoperability of methods and services, both within the project and beyond.\n* **Open source software**: AI4SoilHealth plans to release the mobile phone application on Github under the MIT license. All major outputs of the project should be made available via the project github ([https://github.com/ai4soilhealth](https://github.com/ai4soilhealth)).\n* **Open communication:** The consortium has experience with organizing live, open, and free discussion forums and workshops. \n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"1-specifications.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.47","bibliography":["references.bib"],"theme":{"light":"flatly","dark":"solar"},"mermaid":{"theme":"dark"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}