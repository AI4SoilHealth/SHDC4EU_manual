[
  {
    "objectID": "1-specifications.html",
    "href": "1-specifications.html",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "In a nutshell, AI4SoilHealth Soil Health Data Cube is a building block and infrastructure for monitoring soil health across EU. The general idea of the SHDC4EU is that it serves some standard processes that help inform land owners / land managers about the soil health state of their land using open data. These processes are meant to help future users of the AI4SoilHealth app to quickly find out about the site, explore land potentials based on the state-of-the-art models and start taking concrete actions towards restoring land / improving soil health. This is closely connected with the main expected impacts of the AI4SoilHealth project (to make soil health data easier to access and provide tools and standards for soil health monitorng). In simple terms, the 7–step soil health assessment framework (Hengl 2024) includes:\n\nAssess the history of a site (last 30+ years) provide answers to questions e.g.:\n\n\nHow much SOC has been gained / lost over the last 30 years?\nWhich chemical and physical soil properties changed the most?\nIs there any land/soil degradation and how significant is it?\nHow has agronomic management impacted soil health?\nHow much did the land cover / vegetation cover change? What is the main change class?\nHow much did the landscape heterogeneity change over time?\n\n\nDetermine current state of soil (actual year):\n\n\nWhat is the current state of physical and chemical soil properties?\nWhat is the current (WRB) soil type / main soil forming processes and soil states?\nWhat are the current derived soil properties?\nWhat are the current macronutrient stocks in the soil?\n\n\nDetermine soil potential (next 10, 20, 30 yrs):\n\n\nWhat is the SOC sequestration potential of this soil?\nWhat is the agricultural potential of this soil?\nWhat are the other potential ecosystem functions provided some land use change or mitigations?\n\n\nCollect extra soil samples, send to lab, and add to training points:\n\n\nGenerate a sampling design that helps increase mapping accuracy. Optimize the number of samples while taking into account the costs of sampling.\nCollect new samples and send them to the lab.\n\n\nImport new local data then re-analyze and re-asses points 1, 2 and 3.\n\n\nImport new local samples / results and add to the training points. Re-assess local prediction accuracy.\nRe-predict layers in SHDC4EU for steps #1, #2 and #3.\n\n\nSuggest concrete KPI’s (per farm / per land unit):\n\n\nProvided that soil potential assessment reveals a significant gap between potential and actual.\nEither directly provide concrete measures to help increase soil health and/or provide links to documents / organizations that can directly help increase soil health.\n\n\nTrack progress per farm and re-assess if required:\n\n\nEvery 1–2 years update the indicators (pan-EU).\nCompare with the planned transition in #6. If necessary recommend re-assessment.\n\nThis framework can be either apply to assess health of a specific site (either in the field using a mobile phone app, or in the office using a desktop app), or complete farms / administrative units. We, thus, aim to complete the data cube with primary and derived soil variables that can directly serve this framework, and especially so that can be worked alongside other Work Packages in the project in their testing of methodology.\n\n\n\n\n\nGeneral design and functionality of the AI4SoilHealth App: working with Tier 1 (exhaustive data from the data cube), Tier 2 (direct in-situ measurements) and Tier 3 (data sent to laboratory) observation and predictions.\n\n\n\n\n\n\n\nWP5 — with the tasks T5.2 Development of AI4SoilHealth computing engine and T5.3 Development of AI4SoilHealth Indicators data cube) — aims at implementing fully automated predictive soil mapping at high spatial resolution using large datasets (e.g. whole of Europe at 30-m spatial resolution). Prepared harmonized point datasets are overlaid against time-series of EO images and/or static terrain-based or similar indices. These are then be used to predict values of target variables in 2D, 3D and 2D+T, 3D+T. The general workflow for predictive soil mapping based on Spatiotemporal Machine Learning and High Performance Computing (HPC) and assumes that all point data is analysis-ready, representative and correlates with the EO / covariate layers i.e. can be used to produce usable spatial predictions. Automated predictive soil mapping is currently implemented via the Python library scikit-map using OpenGeoHub’s High Performance Computing infrastructure, and other workflows implemented in C++. The overall soil mapping framework is referred to as the EO-soilmapper (see figure below).\n\n\n\n\n\nEO-soilmapper is the OpenGeoHub’s predictive soil mapping flagship product. Image source: Tian et al. (2024).\n\n\n\n\nThe key interest of WP5 is to produce pan-EU predictions at the highest possible spatial resolution and that can directly serve soil health assessment at continental scale. Some variables are, however, not available for the whole of pan-EU, and some point datasets might be produced experimentally and used for testing purposes only. Hence, within the WP5, all soil variables are modeled / mapped under one the following 3 tiers:\n\nTier 1: Pan-EU, production ready variables: usually based on LUCAS + national soil datasets;\nTier 2 (internal, under construction): National / in-situ data: Tested locally, then upscaled to whole of EU; usually based on collaboration of WP3,4,5,6;\nTier 3 (internal, under construction): Pan-EU, new experimental variables (currently not in the LUCAS soil and most likely can not be modeled/mapped across EU, but can only be used to assess soil health at site); based on the collaboration of WP3,4,5;\n\nWe expect that project partners + pilots feed data for Tier 2 and Tier 3. However, not everything can be mapped across the EU, hence some variables will eventually stay available only locally and a small selection of variables will be available in-situ only i.e. these will not be mapped at all.\n\n\n\nQuality of the outputs would most likely be dictated by the following four key aspects of modeling:\n\nThe general modeling design i.e. how appropriate the statistical / machine learning methods are. For example: are all casualties considered, are all distributions represented, are all statistical assumptions met?\nPredictive performance of the algorithms used i.e. is the best / fastest algorithm used for modeling? Is the algorithm robust, noise-proof, artifacts-proof etc?\nQuality and diversity of the covariate layers used.\nQuality and spatial and feature space representation of training points.\n\nAssuming that #1 and #2 are optimized, then the only remaining factors that control success of the modeling / mapping processes are aspects #3 and #4 i.e. the quality of covariate layers and quality of training points. In the past (prior to 2010), it was relatively difficult to produce pan-EU maps of soil health indicators as there was only limited training point data. Thanks to the European Commission’s LUCAS soil project, we now have 3–4 repetitions of ground measurements of soil properties of highest quality (about 22,000 sites are revisited per period) (see figure below). This is a unique opportunity to test building high resolution predictions, including dynamic soil property predictions. Ideally, such testing should be a joint effort of the AI4SoilHealth consortium, in an open collaboration with JRC and members of other sister projects funded by the same Soil mission.\nNevertheless, LUCAS soil is also not an ideal data set for predictive mapping. Hence producing SHDC4EU faces two serious challenges: (1) LUCAS soil is a top-soil data set, although there is now intention to sample also sub-soils, (2) it covers only a number of physical and chemical soil variables (10–15), and soil types or similar are typically not recorded, making this largely a partial pedological survey.\n\n\n\n\n\nLUCAS soil samples (Orgiazzi et al. 2018) and connected existing and upcoming EO missions (bars indicate approximated temporal coverage). Note that the amount of EO data and missions is increasing exponentially.\n\n\n\n\nTo improve usability of predictions produced using LUCAS we have decided, in this project, to combine both LUCAS and the highest quality legacy national soil data sets. These synchronized and analysis ready fusions of soil laboratory points and observations are generated by T4.6 “Integration and harmonization of in-situ and ancillary observations”. We anticipate that this will be a long process hence, in order to prevent serious delays, we will start producing SHDC4EU predictions, then in each iteration try to improve predictions as new countries join the campaign of contributing their data (under a standard Data Sharing Agreement) for the purpose of data mining i.e. producing open soil information for everyone (Tian et al. 2024?).\n\n\n\nThe second important aspect that determines the quality of the SHDC4EU outputs is the quality of covariate layers. Quality of the covariate layers is basically determined by three things: (1) spatial and temporal resolution, (2) spectral / thematic content, (3) general quality of data in terms of completeness, consistency, correctness and quality of metadata. For producing SHDC4EU we use an extensive compilation of covariate (base) layers to produce predictions for SHDC4EU. Already available layers for continental Europe include (https://EcoDataCube.eu):\n\nLand mask (pan-EU: https://zenodo.org/doi/10.5281/zenodo.8171860) + World Settlement Footprint (WSF) i.e. world buildings 2000-2015, 2019 (https://geoservice.dlr.de/web/maps/eoc:wsfevolution and https://geoservice.dlr.de/web/maps/eoc:wsf2019);\nDigital Terrain Model variables at 6–scales 30-m, 60, 120, 240, 480 and 960-m:\n\nElevation, slope (%), min- max-curvature,\nHillshade, northness, easterness,\nPositive, negative openness,\nTopidx (TWI), geomorphon classes (10), catchment area, LS factor,\n\nLithological (surface geology) map of pan-EU at 250-m (https://zenodo.org/doi/10.5281/zenodo.4787631);\nBimonthly / quarterly GLAD Landsat composites (all bands + biophysical indices, 2000–2022) (explained in: https://doi.org/10.21203/rs.3.rs-4251113/v1):\n\nGreen, Red, NIR, SWIR1, SWIR2,\nNDVI, FAPAR,\nNDTI, NDWI, BSF,\nCumulative NDVI, NDTI and BSF;\n\nClimatic variables at 1-km (long-term or monthly series):\n\nCHELSA Climate Bioclimatic variables (https://chelsa-climate.org/bioclim/);\nCHELSA monthly precipitation time-series 2000–2022 (https://chelsa-climate.org/timeseries/);\nMODIS LST daytime and nighttime monthly time-series 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.1420114);\nMODIS monthly water vapor time-series 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.8193738);\nCumulative annual precipitation (2000–2022);\nMonthly / annual snow images 2000-2022 e.g. DLR Snow pack at 500-m (https://geoservice.dlr.de/web/maps/eoc:gsp:yearly):\nLong-term monthly snow probability time-series at 500-m (https://doi.org/10.5281/zenodo.5774953);\nCumulative annual snow probability 2000–2022;\nSurface water dynamics: occurrence probability of water long-term 1999–2021 (https://glad.umd.edu/dataset/global-surface-water-dynamics);\n\nOptional: Global Flood Database v1 (2000-2018) at 250-m annual flood event (http://global-flood-database.cloudtostreet.info/);\nLight at night time-series at 500-m resolution 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.7750174);\nCropland extent at 30-m for 2000 to 2022 based on (https://glad.umd.edu/dataset/croplands);\nBare soil percent and photosythetical vegetation percent annual for 2000 to 2022 based on the MODIS MCD43A4 product (500-m spatial resolution);\n\nIn addition to the existing layers above, we are also generating a number of novel layers tailored specifically for the purpose of representing land use practices and potential soil degradation factors. These include:\n\nBSI, Bare Soil Index (Mzid et al. 2021) and annual BEF, Bare Earth Fraction (e.g. proportion of pixels with NDVI &lt;0.35 based on 16–day and/or bimonthly data; there are other possible thresholds combination that can be used to optimize the results),\nNDTI, Normalized Differential Tillage Index (Ettehadi Osgouei et al. 2019),\nNOS, Number of Seasons / NOCC,** Number of Cropping Cycles**,\nLOS, Length of Seasons / CDR, Crop Duration Ratio (Estel et al. 2016),\nCrop-type based on EuroCrops dataset,\n\nNote that from all covariate layers listed above, 16-day / bimonthly / quarterly GLAD Landsat composites (all bands + all indices, 2000–2022) are the largest part of the data to be used taking almost 20TB of storage in compressed format (Tian et al. 2024).\n\n\n\nBased on the availability of the training points and nature of the target variable in the SHDC4EU, we either directly predict soil properties or derive soil variables from primary variables. The output soil health indicators can, thus, be considered of type either:\n\nPredicted dynamic pan-EU variables (2000–2022+) available at standard depth intervals (0–20, 20–50, 50–100, 100–200 cm).\nPredicted static pan-EU variables.\nPredicted and/or simulated pan-EU variables i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling.\nDerived pan-EU variables and indices.\nIn-situ only variables.\n\nMost of the soil chemical and biological variables are mapped as dynamic variables with annual or 4/5–year estimates (e.g. 5 maps for period 2000–2025) and at multiple depths (Witjes et al. 2023). This means that the amount of produced data can be significant if predictions are also provided per depth. For example, to map soil organic carbon content (weight %), we can predict 23 annual maps at 4–5 standard depths resulting in over 180 images (assuming that we also produce prediction errors per pixel). AI4SoilHealth tasks T5.5 “Development of AI4SoilHealth present and future soil degradation products”, T5.6 “Development of AI4SoilHealth forecasting services”, and T5.7 “Development of AI4SoilHealth soil functions evidence / model chains” will also generate large amounts of predicted and/or simulated pan-EU variables i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling.\nThe following predictive soil modeling methods are considered for use so far for generating SHDC4EU:\n\n2D and 3D predictive soil mapping (static predictions, time is ignored).\n2D+T predictive soil mapping spacetime.\n3D+T predictive soil mapping spacetime.\nPredictive soil mapping focused on long-term indices.\nDerivation from primary data.\nDerivation using Pedo-transfer functions.\nDerivation / simulations using Mechanistic model (iterative).\nDerivation of future projected predictions (scenario testing).\n\nFor each derivation method we use sets of algorithms, usually implemented in python or R programming languages. These are documented in detail and eventually allow for complete reproducibility of results; most importantly we do a series of benchmarking (listed in further sections) to compare predictive performance and select the algorithm that is most accurate + most robust at the same time. In the tasks T5.5 Development of AI4SoilHealth present and future soil degradation products, T5.6 Development of AI4SoilHealth forecasting services and T5.7 Development of AI4SoilHealth soil functions evidence / model chains, also process-based modeling can be used, especially to generate potential soil ecosystem services etc. Such modeling is at the order-of-magnitude more computationally demanding than predictive mapping hence we expect that these outputs will be of limited spatial detail (1-km) and or available only for experimental testing.\n\n\n\nThe general objective of the SHDC4EU is to serve best possible, most detailed, complete and consistent predictions of the the number of targeted soil health indicators in the EU Mission’s “Implementation Plan: A Soil Deal for Europe”:\n\npresence of pollutants, excess nutrients and salts,\nsoil organic carbon stock,\nsoil structure including soil bulk density and absence of soil sealing and erosion,\nsoil biodiversity,\nsoil nutrients and acidity (pH),\nvegetation cover,\nlandscape heterogeneity,\nforest cover.\n\nIf these are available for the complete land mask of pan-EU area, these can then be used to assess soil degradation state (e.g. salinization / sealing level and trends, concentration of excess nutrients and pollutants and trends, erosion state and trends, loss of SOC and trends etc), current soil properties and soil potential in terms of potential ecosystem services / potential SOC sequestration etc, potential productivity of soil, potential soil biodiversity etc. The working version of what we find as feasible to map at high spatial resolution is provided below.\nNote that some of the soil health indicators recommended by the European Commission / JRC, are not defacto soil variables (e.g. vegetation cover, landscape heterogeneity, forest cover) but will be generated in this project and integrated into SHDC4EU. Some potential options to represent the vegetation cover, landscape heterogeneity etc include:\n\nCanopy height, data already available for EU but could also be improved by outputs from the Open-Earth-Monitor project;\nGPP, Gross Primary Productivity (bimonthly and/or annual; based on the last 2–5yrs);\nLand cover and land use / cropping systems and applications (categories e.g. based on the EuroCrops and similar), producing cropping system maps for the EU;\n\nHowever, producing cropping system maps for the EU, even only for the recent year is not trivial and producing such data is at the moment optional.\nList of target soil variables that will be delivered in the SHDC4EU includes dynamic soil properties covering period 2000–2025+:\n\nSoil organic carbon density (kg/m3) ISO 10694. 1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year estimates;\nSoil carbon stock change (t/ha/yr) ISO 10694. 1996 for 0–20, 20–50, 50–100 cm depth intervals; long-term\nSoil pH in a suspension of soil in water (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 depth intervals, 5–year estimates;\nSoil pH measured in a CaCl2 solution (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil total nitrogen content (dg/kg) ISO 11261:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil bulk density (t/m3) Adapted ISO 11272:2017 for 0–20, 20–50, 50–100 cm, 5–year;\nSoil texture fractions (sand, silt, clay) (g/g) ISO:11277 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil WRB subgroup (factor / probs) based on the WRB2022 classification system for 0–200 cm, long-term;\nDepth to bedrock (cm) up to 200 cm, long-term;\nExtractable potassium content (mg/kg) USDA−NRCS, 2004 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nCarbonates content CaCO3 (g/g) ISO 10693:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nExtractable Phosphorus content (Olsen) (mg/kg) ISO 11263. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nMonthly Gross Primary Productivity (kg/ha/yr) FluxNET bi-monthly;\nBare Soil Fraction (m2/m2) Landsat-based, 5–year;\n\n\n\n\nFor each variable we provide a soil variable registry system, so that each variable should have unique (short-as-possible) code. As simple coding system for SHDC4EU is suggested where the variable name is a combination of the three components:\n\ngeneric variable name,\nspecific laboratory / field / O&M method (ISO standard or similar),\nmeasurement unit,\n\nFor example, for SOC content in weight percent we recommend using oc_iso.10694.1995_wpct (dry combustion) or oc_iso.17184.2014_wpct; for SOC in permiles you can use e.g. oc_iso.10694.1995_wpml. Note that here we use ISO code, however since ISO is commercial (proprietary) and probably not suited for open datasets, we recommend using the ISO code (allowed), but in fact pointing to a scientific reference i.e. a publication or PDF that is at the order of scale easier to obtain (instead of pointing to ISO website or similar). For example the variable oc_iso.10694.1995_wpct can be linked to the scientific reference Nelson et al. (1982) (also used by ISO).\nSoil variable names can be also provided in a shorter version (assuming that only 1 reference method exists) as a combination of the variable name and measurement unit code e.g. oc.wcpt. It is very important that all partners in the project consistently use the same codes, and if there are updates in the naming convention that they refer to the version of the code.\n\n\n\nThe general interest of this project is to produce a time-series of predictions of key soil/vegetation/land use variables with either annual or monthly support and covering the period 1997–2022+ and at highest spatial resolution e.g. 30-m. For variables that vary by depth, we predict at 5 standard depths (0, 20, 50, 100 and 200 cm), then aggregate values to standard depth intervals:\n\n0–20 cm (topsoil) LUCAS standard;\n20–50 cm (subsoil1);\n50–100 cm (subsoil2);\n100–200 cm (subsoil3);\n\nTemporal support of predictions / simulations can be one of the following:\n\nLong-term primary soil properties and classes: e.g. soil types, soil water holding capacity;\nAnnual to 4/5–year values of primary soil and vegetation variables: e.g. annual bare soil fraction (index);\nBimonthly, monthly or seasonal values of soil and vegetation variables: e.g. bimonthly GPP.\nWeekly, 16–day values (original Landsat GLAD),\nDaily or hourly values (probably not of interest in this project).\n\nAggregated values of target soil and vegetation variables can also refer to some part of the distribution e.g. quantile e.g. P25, P50 (median / mean value) and P75; or P025 and P975 or the 95% probability range (corresponds to 2 standard deviations).\n\n\n\nAs an ideal case, each predictive mapping model should produce / provide also uncertainty per pixel (prediction error) and summary results of robust cross-validation. Two options are possible for predictive error mapping:\n\nProvide lower and upper quantiles p=0.025 and p=0.975 so that a 95% probability prediction interval (± 2 standard deviations) can be derived. For variables with skewed distribution / log-normally distributed it is recommended that the lower values are computed in the log-transformed space to avoid predicting negative values.\nProvide prediction error as 1 standard deviation: from this also prediction interval can be derived, assuming that the variable is normally distributed.\n\nAs a general recommendation we suggest using the Conformal Prediction method to produce prediction errors per pixel, best as implemented in the python libraries mappie and/or puncc.\n\n\n\nThe back-end of the SHDC4EU is based on using open source software such as PostGIS/PostGreSQL Rio-tiler, FastAPI and S3 in the back-end; Vue.js, OpenLayers and Geoserver in front end. We are aiming at prioritizing / building upon simple scalable solutions built on top of existing open source solutions. Note that the most important about this back-end design is that the system is: (A) cloud-native i.e. building upon cloud-optimized solutions, (B) easy to extend, (C) focused on usability of data i.e. serving seamless layers (complete, consistent, documented, version-controlled with a live support channels).\nAll pan-European layers (COGs) produced in this project, including majority of covariate layers, are distributed through an existing infrastructure http://EcoDataCube.eu (maintained by OpenGeoHub foundation). This means that all layers are made available:\n\nFor viewing i.e. as Web Mapping Service (WMS) so it can be displayed using OpenLayers or similar;\nFor direct data access via Simple Storage Service (S3) / files registered via the SpatioTemporal Asset Catalog (STAC) via https://stac.ecodatacube.eu;\nBack-up copy available via Zenodo and the project NextCloud including via WebDAV.\n\nOpenGeoHub is responsible that the data is available openly in-real-time (i.e. through S3 service) up to 5 years after the end of project, and that a copy of the data is archived on Zenodo or similar.\nWP5 focuses exclusively on pan-EU modeling, however, we might also use national / pilot data to test methods and develop experimental solutions that potentially could have much higher impact. For this purpose it is crucial that all project partners have access to the NextCloud and can freely collaborate on data without a need to have multiple copies and installations.\nAll tabular vector data will be stored in a single PostGIS DB and where possible made available to project participants (a GeoPKG copy of the point / polygon data will also be made available via the project Nextcloud). For land mask / geospatial standards we rely on the Copernicus Land monitoring infrastructure so we also include Turkey / Western Balkans and Ukraine i.e. also all candidate countries. For data exchange and sharing we use Nextcloud or similar. This data is however not publicly available.\n\n\n\nAll project partners agree to use standard formats to exchange data within the project and on the project websites. The recommended file formats for gridded spatial / spatiotemporal data include:\n\nCloud-Optimized GeoTIFF (*.tif).\nZarr (*.zarr) (also in combination with NetCDF (*.nc).\n\nFor vector geospatial data we recommend the following file formats:\n\nFor data subsets: GeoJSON and/or KML.\nFor cloud-data serving: Geoparquet (*.geoparquet) and/or lance.\nFor visualization: Mapbox Vector Tiles (*.mvt).\nFor GIS analysis: Geopackage (*.gpkg).\n\nFor tabular data, objects as Simple Features and similar, the following file formats are accepted for delivery:\n\nComma Separated Value (*.csv) and GeoCSV best compressed as *.csv.gz.\nR RDS (*.rds) and/or QS files that can be red and written in parallel.\nGeoJSON (*.json).\n\nFor geospatial data to be FAIR, it should at least pass the following checks:\n\nIt is decision-ready, or at least analysis-ready (complete consistent optimized);\nIt is available in a professional catalog e.g. STAC catalog and/or Geonetwork;\nIt comes with technical documentation (ideally a peer-reviewed publication) / links to Github / Gitlab where users can find technical explanation of how was the data produced;\nIt has a version and each version has unique DOI;\nIt can be accessed directly i.e. file URL is available for HTTP requests (through S3 or similar).\n\ngdal_translate in.vrt out.tif -of COG -co TILED=YES -co COPY_SRC_OVERVIEWS=YES -co COMPRESS=LZW\nEach COG are quality controlled using (1) COG validator, (2) by visual inspection, (3) random sampling point overlay to certify that &gt;99% of pixels are available. After the quality control, all produced global mosaics can be registered and uploaded to S3 storage or similar.\n\n\n\nFor the sake of consistency and compatibility, project participants should use standard spatial resolutions to deliver and exchange data. Recommended standard pixel sizes / resolutions:\nTable: Standard bounding box and spatial resolutions.\n\n\n\nEurope COG\nBounding box\n\n\nContinental EU COG based on Copernicus\nSpatial resolutions and image size EPSG:3035\n\n\n\n\nXmin = 900,000\n\nYmin = 899,000\n\nXmax = 7,401,000\n\nYmax = 5,501,000\n\n\n10m | 650,100L x 460,200P\n\n25m | 260,040L x 184,080P\n\n30m | 216,700P x 153,400L\n\n100m | 65,010P x 46,020L\n\n250m | 26,004P x 18,408L\n\n1km | 6501P x 4602L\n\n\n\n\n\n\n\n\nTiling system (100-km blocks) used in the EO-soilmapper to run processing\n\n\n\n\n\n\n\nSHDC consistently uses the standard OpenLandMap file-naming convention to submit new data-sets and similar (compare e.g. with the MODIS file naming convention). This is to ensure consistency and ease of use within the AI4SoilHealth project, but also by the end-users. This applies especially to WP4, WP5 and WP6.\nThe OpenLandMap file-naming convention works with 10 fields that basically define the most important properties of the data (this way users can search files, prepare data analysis etc, without even needing to access or open files. The 10 fields include:\n\nGeneric variable name (needs to be unique and approved by the AI4SoilHealth Coordination team): lclu;\nVariable procedure combination i.e. method standard (standard abbreviation): luisa;\nPosition in the probability distribution / variable type: c;\nSpatial support (usually horizontal block) in m or km: 30m;\nDepth reference or depth interval e.g. below (“b”), above (“a”) ground or at surface (“s”): s;\nTime reference begin time (YYYYMMDD): 20210101;\nTime reference end time: 20211231;\nBounding box (2 letters max): eu;\nEPSG code: epsg.3035;\nVersion code i.e. creation date: v20221015;\n\nAn example of a file-name based on the description above:\nlclu_luisa_c_30m_s_20210101_20211231_eu_epsg.3035_v20221015.tif\nNote that this file naming convention has the following properties:\n\nLarge quantities of files can be easily sorted and searched (one line queries in Bash).\nFile-naming patterns can be used to seamlessly build virtual mosaics and composites.\n\nKey spatiotemporal properties of the data are available in the file name e.g. variable type, O&M method, spatial resolution, bounding box, projection system, temporal references. Users can program analysis without opening or testing files.\nVersioning system is ubiquitous.\nAll file-names are unique.\n\nGeonetwork and STAC will be further used to link the unique file names to: (1) WPs, deliverables, themes / keywords, (2) DOI’s, (3) project homepages, (4) contact pages for support and feedback. For keywords we recommend using the INSPIRE keywords. To confirm that metadata is complete and consistent, we recommend using the INSPIRE metadata validator and/or https://data.europa.eu/en validator.\nSome simple additional rules for generating the file name include:\n\nCodes and abbreviations should be human-readable as much as possible (hence short, but not too short!);\nUse only English-US (en-us) language e.g. for months use jan, feb etc;\nConsistently use UNICODE standard: small letters only, no blank spaces, no non-ASCII characters;\nLimit the total file name size in characters to 256;\nFor time reference do not extend beyond hour minute and timezone;\nFor bounding boxes use as much as possible the 2–letter unique country code; for continents use the Equi7 Grid code i.e. eu,\nFor method codes use as much as possible unique IDs from ISO - ICS;\nFor MODIS products use consistently the MODIS products codes e.g. MOD11A2 v061; For long-term aggregates of seasonal, monthly, weekly values use the period name at the end of the method names (#3) for example the long-term estimate of MODIS LST daytime temperature for month August:\n\nlst.d_mod11a2v061.aug_m_1km_s_20200101_20211231_eu_epsg.3035_v20221015.tif\nA list of vocabularies to be used as abbreviated names of variables is provided by OpenGeoHub. The same file-name convention described above can be also used for vector data (this would only have a different file extension) also.\n\n\n\nAll project participants are required to register project outputs and inform other project participants about the progress (via https://zenodo.org/communities/ai4soilhealth/). For internal documents / draft versions of outputs the following submission principles need to be followed:\n\nFor internal datasets and documents use project Gitlab and Mattermost to inform parties about the progress and receive feedback;\nRegularly inform your WP about the progress and planned dates (best via Mattermost channel).\nTo receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\nFor each new version of the output specify in short terms what has been improved, what has changed from last version.\nTag people in Mattermost that you would like to request to review the outputs.\n\nFor official releases please refer to the Consortium Agreement for the correct procedure. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\nFirst register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the minimum requirements regarding file formats and file naming, documentation and metadata.\nMake sure that no parts of the submission have any copyright issues.\nMake sure that all contributors are credited in detail.\nMake sure you provide appropriate disclaimer and terms of use; we recommend using “No warranty” as default.\nMake sure the license is compatible to the AI4SoilHealth project following our Consortium Agreement;\nMake sure acknowledgement (see below) is clearly provided.\nMake sure you provide contact information and instructions for users to provide feedback and request support.\nMake sure all project partners are informed about the date of release (through Gitlab and Mattermost).\n\nSuggested 3rd party platforms for registering outputs (we recommend using multiple of the listed options e.g. a, b, c etc):\n\nDatasets: https://zenodo.org/communities/ai4soilhealth/ and/or https://onedata.org/; metadata entry via: Geonetwork and/or STAC; structured summary via: https://ai4soilhealth.eu, suggestion: registered on https://data.europa.eu/en,\nSoftware: Github (https://github.com/ai4soilhealth) -&gt; suggestion: always generate DOI and put on Zenodo.org; https://archive.softwareheritage.org/, structured summary via: https://ai4soilhealth.eu,\nTutorials / data catalogs: webpages on github / gitlab (Rbookdown, python books) see e.g. https://opengeohub.github.io/SoilSamples/, MKdocs see e.g. https://gee-community-catalog.org/;\nData portals / web-GUI’s: sub-domain under *.ai4soilhealth.eu; recommended portal: https://gkhub.earthobservations.org/; video-tutorial published via: https://av.tib.eu/,\n\nWe are planning to have our own installation of Gitlab, STAC browser, Geonetwork and Pretalx. Final workflow for registering outputs will be specified in the final version of the implementation plan.\nDuring publishing of outputs, and especially if you register outputs via 3rd party repos, it of utmost importance that all partners use the correct attribution and links to the project homepage:\n\nCorrect attribution: “The AI4SoilHealth project project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179.”;\nProject URL: https://cordis.europa.eu/project/id/101086179.\nProject homepage: https://ai4soilhealth.eu,\nCorrect default disclaimer is below.\n\n\n\n\nAll public releases of new data / software should point to the generic AI4SoilHealth disclaimer:\n\n“Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or European Commision. Neither the European Union nor the granting authority can be held responsible for them. The data is provided “as is”. AI4SoilHealth project consortium and its suppliers and licensors hereby disclaim all warranties of any kind, express or implied, including, without limitation, the warranties of merchantability, fitness for a particular purpose and non-infringement. Neither AI4SoilHealth Consortium nor its suppliers and licensors, makes any warranty that the Website will be error free or that access thereto will be continuous or uninterrupted. You understand that you download from, or otherwise obtain content or services through, the Website at your own discretion and risk.”\n\n\n\n\nConsidering the land mask for pan-EU (see figure), we try to closely match the data coverage of Copernicus pan-european i.e.  the official selection of countries listed here. Note the mask covers also all EU integration candidate countries, but can eventually also be subset to only European Union countries.\n\n\n\n\n\nLandmask for the SHDC for pan-EU.\n\n\n\n\nThere are a total of three landmask files available, each of which is aligned with the standard spatial/temporal resolution and sizes of SHDC4EU specifications. Additionally, these files include a corresponding look-up table that provides explanations for the values present in the raster data.\nTable: Technical description of the pan-EU land mask.\n\n\n\nLandmask\n\n\nPurpose/principle\n\n\nThe basic principle to create the land mask is to include as much as land as possible, to avoid missing any land pixels and ensure precise differentiation between land, ocean and inland water bodies.\n\nWhen generating the land mask, the two reference datasets in a way that:\n\n\nIf either of the two reference datasets identifies a pixel as land, it is considered a land pixel in our mask.\n\nRegarding ocean and inland water bodies, a pixel is classified as a water pixel only when both reference datasets confirm its identification as water.\n\n\n\n\n\n\nReference datasets\n\n\n\n\nWorldCover, 10 m resolution.\n\nEuroGlobalMap, with shapefiles of administrative boundaries, inland water bodies, ocean and landmask.\n\n\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution landmasks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “min” in GDAL. This “min” method allows taking the minimum values from the contributing pixels, to keep as much land as possible.\n\n\n\n\nResolution available\n\n\n10-m, 30-m, 100-m, 250-m, and 1-km resolution\n\n\n\n\nMask values\n\n\n\n\n10: not in the pan-EU area, i.e. out of mapping scope\n\n1: land\n\n2: inland water\n\n3: ocean\n\n\n\n\n\n\nISO-3166 country code mask\n\n\nPurpose/principle\n\n\nIn this mask, each country is assigned a unique value, which allows for the interpretation and analysis of data associated with a specific country. The values are assigned to each country according to iso-3166 country code, which can be found in the corresponding look-up table.\n\n\n\n\nReference datasets\n\n\nEuroGlobalMap country shapefile\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution masks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n\n\n\n\nResolution available\n\n\n10m, 30m and 100m\n\n\n\n\nMask values\n\n\nCan be found in the corresponding look-up table.\n\n\n\n\nNUTS-3 mask\n\n\nPurpose/principle\n\n\nIn this raster file, each unique NUT3 level area is assigned a unique value, which allows for the interpretation and analysis of data associated with specific NUTS-3 regions. Compared to ISO-3166 country code mask, NUTS-3 mask shows more details about regional administrative boundaries.\n\n\n\n\nReference datasets\n\n\nEuropean NUTS-3 shapefile\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution masks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n\n\n\n\nResolution available\n\n\n10 m, 30 m and 100 m\n\n\n\n\nMask values\n\n\nCan be found in the corresponding look-up table.\n\n\n\nAn important point to consider is that the ISO-code country mask provides a wider geographical coverage compared to the NUTS3 mask. This extended coverage includes countries such as Ukraine and others that lie beyond the NUTS3 administrative boundaries. Both the land mask and administrative code mask are in an Equal-area projection, allowing for accurate area estimation and facilitating aggregation per political/administrative unit.\nThese masks are published and shared in a publicly available way on Zenodo. The working version (v0.1) is available from: https://doi.org/10.5281/zenodo.8171860. The layers can be opened directly in QGIS by copying links from Zenodo. The scripts used to generate these masks can be found in our project Gitlab or via the public Github. If any issue / problem is noticed, please report.\n\n\n\nPartners on consortium are invited to submit working versions of data and services / share preliminary outputs internally via NextCloud or similar. Folder structure and more detailed instructions are provided by the WP lead OpenGeoHub. It is important however to distinguish between (1) internal releases, (2) public releases (partners responsible) and (3) public releases approved by the consortium.\nAll WP5 participants are required to register project outputs and inform other project participants about the progress. For internal documents / draft versions of outputs the following submission principles need to be followed:\n\nFor internal datasets and documents use project repositories, Gitlab, NextCloud and Mattermost to inform parties about the progress and receive feedback;\nRegularly inform your WP/Task group about the progress and planned dates (best via Mattermost channel).\nTo receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\nFor each new version of the output specify in short terms what has been improved, what has changed from last version.\nTag people in Mattermost that you would like to request to review the outputs.\n\nFor official releases please refer to these guidelines to avoid any delays. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\nFirst register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the minimum requirements regarding file formats and file naming, documentation and metadata.\nMake sure that no parts of the submission have any copyright issues.\nMake sure that all contributors are credited in detail.\nMake sure you provide appropriate disclaimer and terms of use; we recommend using “No warranty” as default.\nMake sure the license is compatible to the AI4SoilHealth project following our Consortium Agreement;\nMake sure acknowledgement (https://cordis.europa.eu/project/id/101086179) is clearly provided.\nMake sure you provide contact information and instructions for users to provide feedback and request support.\nMake sure all project partners are informed about the date of release (through Gitlab and Mattermost).\nIf there are any deviations from specifications you have put in the implementation plan, please indicate such deviations in the project management system.\n\nFor quality assurance (public releases) we recommend the following three checks for any new significant datasets:\n\nPreliminary data and code shared with all WP members via NextCloud / code on Gitlab / Github. Internal check in co-development sessions.\nPeer-review publications (as agreed in the proposal, all major new data products should go through peer-review).\nData and code exposed for public comments (https://github.com/ai4soilhealth) including the social media, especially Mastodon, X, Linkedin etc.\n\nIn principle, the publication process should be non-bureaucratic, agile and should not limit intonation and experimentation. However, for public releases approved by the consortium e.g. major deliverables as specified in the Grant Agreement, it is advised that these are approved by the Executive Board of the project so that the new dataset / service is then also officially promoted through media channels, landing page etc.\n\n\n\nIn order to ensure FAIR outputs, it is highly recommended that all production steps used to generate maps are documented in code, best as Rmarkdown / Python Jupyter computational notebooks. As agreed also in the project proposal, All empirical data obtained follows strict and validated processes of monitoring and evaluation to avoid any potential error. The data produced in AI4SoilHealth will be made available as open data, following the FAIR principle. The project Data Management Plan (DMP) is available and has even more instructions.\nAI4SoilHealth has agreed to provide open access (OA) to research outputs (e.g., publications, data, software) through deposition in trusted repositories. Partners provide OA for peer-reviewed scientific publications relating to their results. Authors of all peer-reviewed scientific publications stores them in an OA trusted repository, during and after the project’s life following Article 17 and Annex 5 of the General Assembly. The consortium members are encouraged to publish in the Open Research Europe data platform, specifically via the Zenodo community for AI4SoilHealth (https://zenodo.org/communities/ai4soilhealth/).\nBesides OA publication, the project aims for early and open sharing of the soil health data, and the research and technological developments  including open data, open standards, open source software, and open communication:\n\nOpen data: The AI4SoilHealth consortium builds solutions upon open datasets published using genuinely open data licenses.\n\nOpen standards (interoperability): The use of open standards prevents lock-in by, or dependency on any single data, software or service supplier. AI4SoilHealth fully adopts the principles of FAIR data management and open standards to enable interoperability of methods and services, both within the project and beyond.\nOpen source software: AI4SoilHealth plans to release the mobile phone application on Github under the MIT license. All major outputs of the project should be made available via the project github (https://github.com/ai4soilhealth).\nOpen communication: The consortium has experience with organizing live, open, and free discussion forums and workshops.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#the-7-step-framework-for-soil-health-assessment",
    "href": "1-specifications.html#the-7-step-framework-for-soil-health-assessment",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "In a nutshell, AI4SoilHealth Soil Health Data Cube is a building block and infrastructure for monitoring soil health across EU. The general idea of the SHDC4EU is that it serves some standard processes that help inform land owners / land managers about the soil health state of their land using open data. These processes are meant to help future users of the AI4SoilHealth app to quickly find out about the site, explore land potentials based on the state-of-the-art models and start taking concrete actions towards restoring land / improving soil health. This is closely connected with the main expected impacts of the AI4SoilHealth project (to make soil health data easier to access and provide tools and standards for soil health monitorng). In simple terms, the 7–step soil health assessment framework (Hengl 2024) includes:\n\nAssess the history of a site (last 30+ years) provide answers to questions e.g.:\n\n\nHow much SOC has been gained / lost over the last 30 years?\nWhich chemical and physical soil properties changed the most?\nIs there any land/soil degradation and how significant is it?\nHow has agronomic management impacted soil health?\nHow much did the land cover / vegetation cover change? What is the main change class?\nHow much did the landscape heterogeneity change over time?\n\n\nDetermine current state of soil (actual year):\n\n\nWhat is the current state of physical and chemical soil properties?\nWhat is the current (WRB) soil type / main soil forming processes and soil states?\nWhat are the current derived soil properties?\nWhat are the current macronutrient stocks in the soil?\n\n\nDetermine soil potential (next 10, 20, 30 yrs):\n\n\nWhat is the SOC sequestration potential of this soil?\nWhat is the agricultural potential of this soil?\nWhat are the other potential ecosystem functions provided some land use change or mitigations?\n\n\nCollect extra soil samples, send to lab, and add to training points:\n\n\nGenerate a sampling design that helps increase mapping accuracy. Optimize the number of samples while taking into account the costs of sampling.\nCollect new samples and send them to the lab.\n\n\nImport new local data then re-analyze and re-asses points 1, 2 and 3.\n\n\nImport new local samples / results and add to the training points. Re-assess local prediction accuracy.\nRe-predict layers in SHDC4EU for steps #1, #2 and #3.\n\n\nSuggest concrete KPI’s (per farm / per land unit):\n\n\nProvided that soil potential assessment reveals a significant gap between potential and actual.\nEither directly provide concrete measures to help increase soil health and/or provide links to documents / organizations that can directly help increase soil health.\n\n\nTrack progress per farm and re-assess if required:\n\n\nEvery 1–2 years update the indicators (pan-EU).\nCompare with the planned transition in #6. If necessary recommend re-assessment.\n\nThis framework can be either apply to assess health of a specific site (either in the field using a mobile phone app, or in the office using a desktop app), or complete farms / administrative units. We, thus, aim to complete the data cube with primary and derived soil variables that can directly serve this framework, and especially so that can be worked alongside other Work Packages in the project in their testing of methodology.\n\n\n\n\n\nGeneral design and functionality of the AI4SoilHealth App: working with Tier 1 (exhaustive data from the data cube), Tier 2 (direct in-situ measurements) and Tier 3 (data sent to laboratory) observation and predictions.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#training-points",
    "href": "1-specifications.html#training-points",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "Quality of the outputs would most likely be dictated by the following four key aspects of modeling:\n\nThe general modeling design i.e. how appropriate the statistical / machine learning methods are. For example: are all casualties considered, are all distributions represented, are all statistical assumptions met?\nPredictive performance of the algorithms used i.e. is the best / fastest algorithm used for modeling? Is the algorithm robust, noise-proof, artifacts-proof etc?\nQuality and diversity of the covariate layers used.\nQuality and spatial and feature space representation of training points.\n\nAssuming that #1 and #2 are optimized, then the only remaining factors that control success of the modeling / mapping processes are aspects #3 and #4 i.e. the quality of covariate layers and quality of training points. In the past (prior to 2010), it was relatively difficult to produce pan-EU maps of soil health indicators as there was only limited training point data. Thanks to the European Commission’s LUCAS soil project, we now have 3–4 repetitions of ground measurements of soil properties of highest quality (about 22,000 sites are revisited per period) (see figure below). This is a unique opportunity to test building high resolution predictions, including dynamic soil property predictions. Ideally, such testing should be a joint effort of the AI4SoilHealth consortium, in an open collaboration with JRC and members of other sister projects funded by the same Soil mission.\nNevertheless, LUCAS soil is also not an ideal data set for predictive mapping. Hence producing SHDC4EU faces two serious challenges: (1) LUCAS soil is a top-soil data set, although there is now intention to sample also sub-soils, (2) it covers only a number of physical and chemical soil variables (10–15), and soil types or similar are typically not recorded, making this largely a partial pedological survey.\n\n\n\n\n\nLUCAS soil samples (Orgiazzi et al. 2018) and connected existing and upcoming EO missions (bars indicate approximated temporal coverage). Note that the amount of EO data and missions is increasing exponentially.\n\n\n\n\nTo improve usability of predictions produced using LUCAS we have decided, in this project, to combine both LUCAS and the highest quality legacy national soil data sets. These synchronized and analysis ready fusions of soil laboratory points and observations are generated by T4.6 “Integration and harmonization of in-situ and ancillary observations”. We anticipate that this will be a long process hence, in order to prevent serious delays, we will start producing SHDC4EU predictions, then in each iteration try to improve predictions as new countries join the campaign of contributing their data (under a standard Data Sharing Agreement) for the purpose of data mining i.e. producing open soil information for everyone (Tian et al. 2024?).",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#covariate-base-layers",
    "href": "1-specifications.html#covariate-base-layers",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "The second important aspect that determines the quality of the SHDC4EU outputs is the quality of covariate layers. Quality of the covariate layers is basically determined by three things: (1) spatial and temporal resolution, (2) spectral / thematic content, (3) general quality of data in terms of completeness, consistency, correctness and quality of metadata. For producing SHDC4EU we use an extensive compilation of covariate (base) layers to produce predictions for SHDC4EU. Already available layers for continental Europe include (https://EcoDataCube.eu):\n\nLand mask (pan-EU: https://zenodo.org/doi/10.5281/zenodo.8171860) + World Settlement Footprint (WSF) i.e. world buildings 2000-2015, 2019 (https://geoservice.dlr.de/web/maps/eoc:wsfevolution and https://geoservice.dlr.de/web/maps/eoc:wsf2019);\nDigital Terrain Model variables at 6–scales 30-m, 60, 120, 240, 480 and 960-m:\n\nElevation, slope (%), min- max-curvature,\nHillshade, northness, easterness,\nPositive, negative openness,\nTopidx (TWI), geomorphon classes (10), catchment area, LS factor,\n\nLithological (surface geology) map of pan-EU at 250-m (https://zenodo.org/doi/10.5281/zenodo.4787631);\nBimonthly / quarterly GLAD Landsat composites (all bands + biophysical indices, 2000–2022) (explained in: https://doi.org/10.21203/rs.3.rs-4251113/v1):\n\nGreen, Red, NIR, SWIR1, SWIR2,\nNDVI, FAPAR,\nNDTI, NDWI, BSF,\nCumulative NDVI, NDTI and BSF;\n\nClimatic variables at 1-km (long-term or monthly series):\n\nCHELSA Climate Bioclimatic variables (https://chelsa-climate.org/bioclim/);\nCHELSA monthly precipitation time-series 2000–2022 (https://chelsa-climate.org/timeseries/);\nMODIS LST daytime and nighttime monthly time-series 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.1420114);\nMODIS monthly water vapor time-series 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.8193738);\nCumulative annual precipitation (2000–2022);\nMonthly / annual snow images 2000-2022 e.g. DLR Snow pack at 500-m (https://geoservice.dlr.de/web/maps/eoc:gsp:yearly):\nLong-term monthly snow probability time-series at 500-m (https://doi.org/10.5281/zenodo.5774953);\nCumulative annual snow probability 2000–2022;\nSurface water dynamics: occurrence probability of water long-term 1999–2021 (https://glad.umd.edu/dataset/global-surface-water-dynamics);\n\nOptional: Global Flood Database v1 (2000-2018) at 250-m annual flood event (http://global-flood-database.cloudtostreet.info/);\nLight at night time-series at 500-m resolution 2000–2022 (https://zenodo.org/doi/10.5281/zenodo.7750174);\nCropland extent at 30-m for 2000 to 2022 based on (https://glad.umd.edu/dataset/croplands);\nBare soil percent and photosythetical vegetation percent annual for 2000 to 2022 based on the MODIS MCD43A4 product (500-m spatial resolution);\n\nIn addition to the existing layers above, we are also generating a number of novel layers tailored specifically for the purpose of representing land use practices and potential soil degradation factors. These include:\n\nBSI, Bare Soil Index (Mzid et al. 2021) and annual BEF, Bare Earth Fraction (e.g. proportion of pixels with NDVI &lt;0.35 based on 16–day and/or bimonthly data; there are other possible thresholds combination that can be used to optimize the results),\nNDTI, Normalized Differential Tillage Index (Ettehadi Osgouei et al. 2019),\nNOS, Number of Seasons / NOCC,** Number of Cropping Cycles**,\nLOS, Length of Seasons / CDR, Crop Duration Ratio (Estel et al. 2016),\nCrop-type based on EuroCrops dataset,\n\nNote that from all covariate layers listed above, 16-day / bimonthly / quarterly GLAD Landsat composites (all bands + all indices, 2000–2022) are the largest part of the data to be used taking almost 20TB of storage in compressed format (Tian et al. 2024).",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#predictive-mapping-and-derivation-methods",
    "href": "1-specifications.html#predictive-mapping-and-derivation-methods",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "Based on the availability of the training points and nature of the target variable in the SHDC4EU, we either directly predict soil properties or derive soil variables from primary variables. The output soil health indicators can, thus, be considered of type either:\n\nPredicted dynamic pan-EU variables (2000–2022+) available at standard depth intervals (0–20, 20–50, 50–100, 100–200 cm).\nPredicted static pan-EU variables.\nPredicted and/or simulated pan-EU variables i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling.\nDerived pan-EU variables and indices.\nIn-situ only variables.\n\nMost of the soil chemical and biological variables are mapped as dynamic variables with annual or 4/5–year estimates (e.g. 5 maps for period 2000–2025) and at multiple depths (Witjes et al. 2023). This means that the amount of produced data can be significant if predictions are also provided per depth. For example, to map soil organic carbon content (weight %), we can predict 23 annual maps at 4–5 standard depths resulting in over 180 images (assuming that we also produce prediction errors per pixel). AI4SoilHealth tasks T5.5 “Development of AI4SoilHealth present and future soil degradation products”, T5.6 “Development of AI4SoilHealth forecasting services”, and T5.7 “Development of AI4SoilHealth soil functions evidence / model chains” will also generate large amounts of predicted and/or simulated pan-EU variables i.e. results of predictions of future conditions, scenario testing, simulations, process-based modeling.\nThe following predictive soil modeling methods are considered for use so far for generating SHDC4EU:\n\n2D and 3D predictive soil mapping (static predictions, time is ignored).\n2D+T predictive soil mapping spacetime.\n3D+T predictive soil mapping spacetime.\nPredictive soil mapping focused on long-term indices.\nDerivation from primary data.\nDerivation using Pedo-transfer functions.\nDerivation / simulations using Mechanistic model (iterative).\nDerivation of future projected predictions (scenario testing).\n\nFor each derivation method we use sets of algorithms, usually implemented in python or R programming languages. These are documented in detail and eventually allow for complete reproducibility of results; most importantly we do a series of benchmarking (listed in further sections) to compare predictive performance and select the algorithm that is most accurate + most robust at the same time. In the tasks T5.5 Development of AI4SoilHealth present and future soil degradation products, T5.6 Development of AI4SoilHealth forecasting services and T5.7 Development of AI4SoilHealth soil functions evidence / model chains, also process-based modeling can be used, especially to generate potential soil ecosystem services etc. Such modeling is at the order-of-magnitude more computationally demanding than predictive mapping hence we expect that these outputs will be of limited spatial detail (1-km) and or available only for experimental testing.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#targeted-soil-and-soil-health-variables",
    "href": "1-specifications.html#targeted-soil-and-soil-health-variables",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "The general objective of the SHDC4EU is to serve best possible, most detailed, complete and consistent predictions of the the number of targeted soil health indicators in the EU Mission’s “Implementation Plan: A Soil Deal for Europe”:\n\npresence of pollutants, excess nutrients and salts,\nsoil organic carbon stock,\nsoil structure including soil bulk density and absence of soil sealing and erosion,\nsoil biodiversity,\nsoil nutrients and acidity (pH),\nvegetation cover,\nlandscape heterogeneity,\nforest cover.\n\nIf these are available for the complete land mask of pan-EU area, these can then be used to assess soil degradation state (e.g. salinization / sealing level and trends, concentration of excess nutrients and pollutants and trends, erosion state and trends, loss of SOC and trends etc), current soil properties and soil potential in terms of potential ecosystem services / potential SOC sequestration etc, potential productivity of soil, potential soil biodiversity etc. The working version of what we find as feasible to map at high spatial resolution is provided below.\nNote that some of the soil health indicators recommended by the European Commission / JRC, are not defacto soil variables (e.g. vegetation cover, landscape heterogeneity, forest cover) but will be generated in this project and integrated into SHDC4EU. Some potential options to represent the vegetation cover, landscape heterogeneity etc include:\n\nCanopy height, data already available for EU but could also be improved by outputs from the Open-Earth-Monitor project;\nGPP, Gross Primary Productivity (bimonthly and/or annual; based on the last 2–5yrs);\nLand cover and land use / cropping systems and applications (categories e.g. based on the EuroCrops and similar), producing cropping system maps for the EU;\n\nHowever, producing cropping system maps for the EU, even only for the recent year is not trivial and producing such data is at the moment optional.\nList of target soil variables that will be delivered in the SHDC4EU includes dynamic soil properties covering period 2000–2025+:\n\nSoil organic carbon density (kg/m3) ISO 10694. 1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year estimates;\nSoil carbon stock change (t/ha/yr) ISO 10694. 1996 for 0–20, 20–50, 50–100 cm depth intervals; long-term\nSoil pH in a suspension of soil in water (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 depth intervals, 5–year estimates;\nSoil pH measured in a CaCl2 solution (-) ISO 10390. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil total nitrogen content (dg/kg) ISO 11261:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil bulk density (t/m3) Adapted ISO 11272:2017 for 0–20, 20–50, 50–100 cm, 5–year;\nSoil texture fractions (sand, silt, clay) (g/g) ISO:11277 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nSoil WRB subgroup (factor / probs) based on the WRB2022 classification system for 0–200 cm, long-term;\nDepth to bedrock (cm) up to 200 cm, long-term;\nExtractable potassium content (mg/kg) USDA−NRCS, 2004 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nCarbonates content CaCO3 (g/g) ISO 10693:1995 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nExtractable Phosphorus content (Olsen) (mg/kg) ISO 11263. 1994 for 0–20, 20–50, 50–100 cm depth intervals, 5–year;\nMonthly Gross Primary Productivity (kg/ha/yr) FluxNET bi-monthly;\nBare Soil Fraction (m2/m2) Landsat-based, 5–year;",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#variable-registry",
    "href": "1-specifications.html#variable-registry",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "For each variable we provide a soil variable registry system, so that each variable should have unique (short-as-possible) code. As simple coding system for SHDC4EU is suggested where the variable name is a combination of the three components:\n\ngeneric variable name,\nspecific laboratory / field / O&M method (ISO standard or similar),\nmeasurement unit,\n\nFor example, for SOC content in weight percent we recommend using oc_iso.10694.1995_wpct (dry combustion) or oc_iso.17184.2014_wpct; for SOC in permiles you can use e.g. oc_iso.10694.1995_wpml. Note that here we use ISO code, however since ISO is commercial (proprietary) and probably not suited for open datasets, we recommend using the ISO code (allowed), but in fact pointing to a scientific reference i.e. a publication or PDF that is at the order of scale easier to obtain (instead of pointing to ISO website or similar). For example the variable oc_iso.10694.1995_wpct can be linked to the scientific reference Nelson et al. (1982) (also used by ISO).\nSoil variable names can be also provided in a shorter version (assuming that only 1 reference method exists) as a combination of the variable name and measurement unit code e.g. oc.wcpt. It is very important that all partners in the project consistently use the same codes, and if there are updates in the naming convention that they refer to the version of the code.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#spatio-temporal-reference",
    "href": "1-specifications.html#spatio-temporal-reference",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "The general interest of this project is to produce a time-series of predictions of key soil/vegetation/land use variables with either annual or monthly support and covering the period 1997–2022+ and at highest spatial resolution e.g. 30-m. For variables that vary by depth, we predict at 5 standard depths (0, 20, 50, 100 and 200 cm), then aggregate values to standard depth intervals:\n\n0–20 cm (topsoil) LUCAS standard;\n20–50 cm (subsoil1);\n50–100 cm (subsoil2);\n100–200 cm (subsoil3);\n\nTemporal support of predictions / simulations can be one of the following:\n\nLong-term primary soil properties and classes: e.g. soil types, soil water holding capacity;\nAnnual to 4/5–year values of primary soil and vegetation variables: e.g. annual bare soil fraction (index);\nBimonthly, monthly or seasonal values of soil and vegetation variables: e.g. bimonthly GPP.\nWeekly, 16–day values (original Landsat GLAD),\nDaily or hourly values (probably not of interest in this project).\n\nAggregated values of target soil and vegetation variables can also refer to some part of the distribution e.g. quantile e.g. P25, P50 (median / mean value) and P75; or P025 and P975 or the 95% probability range (corresponds to 2 standard deviations).",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#uncertainty-of-predictions",
    "href": "1-specifications.html#uncertainty-of-predictions",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "As an ideal case, each predictive mapping model should produce / provide also uncertainty per pixel (prediction error) and summary results of robust cross-validation. Two options are possible for predictive error mapping:\n\nProvide lower and upper quantiles p=0.025 and p=0.975 so that a 95% probability prediction interval (± 2 standard deviations) can be derived. For variables with skewed distribution / log-normally distributed it is recommended that the lower values are computed in the log-transformed space to avoid predicting negative values.\nProvide prediction error as 1 standard deviation: from this also prediction interval can be derived, assuming that the variable is normally distributed.\n\nAs a general recommendation we suggest using the Conformal Prediction method to produce prediction errors per pixel, best as implemented in the python libraries mappie and/or puncc.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#back-end-front-end-components",
    "href": "1-specifications.html#back-end-front-end-components",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "The back-end of the SHDC4EU is based on using open source software such as PostGIS/PostGreSQL Rio-tiler, FastAPI and S3 in the back-end; Vue.js, OpenLayers and Geoserver in front end. We are aiming at prioritizing / building upon simple scalable solutions built on top of existing open source solutions. Note that the most important about this back-end design is that the system is: (A) cloud-native i.e. building upon cloud-optimized solutions, (B) easy to extend, (C) focused on usability of data i.e. serving seamless layers (complete, consistent, documented, version-controlled with a live support channels).\nAll pan-European layers (COGs) produced in this project, including majority of covariate layers, are distributed through an existing infrastructure http://EcoDataCube.eu (maintained by OpenGeoHub foundation). This means that all layers are made available:\n\nFor viewing i.e. as Web Mapping Service (WMS) so it can be displayed using OpenLayers or similar;\nFor direct data access via Simple Storage Service (S3) / files registered via the SpatioTemporal Asset Catalog (STAC) via https://stac.ecodatacube.eu;\nBack-up copy available via Zenodo and the project NextCloud including via WebDAV.\n\nOpenGeoHub is responsible that the data is available openly in-real-time (i.e. through S3 service) up to 5 years after the end of project, and that a copy of the data is archived on Zenodo or similar.\nWP5 focuses exclusively on pan-EU modeling, however, we might also use national / pilot data to test methods and develop experimental solutions that potentially could have much higher impact. For this purpose it is crucial that all project partners have access to the NextCloud and can freely collaborate on data without a need to have multiple copies and installations.\nAll tabular vector data will be stored in a single PostGIS DB and where possible made available to project participants (a GeoPKG copy of the point / polygon data will also be made available via the project Nextcloud). For land mask / geospatial standards we rely on the Copernicus Land monitoring infrastructure so we also include Turkey / Western Balkans and Ukraine i.e. also all candidate countries. For data exchange and sharing we use Nextcloud or similar. This data is however not publicly available.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#data-exchange-formats",
    "href": "1-specifications.html#data-exchange-formats",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "All project partners agree to use standard formats to exchange data within the project and on the project websites. The recommended file formats for gridded spatial / spatiotemporal data include:\n\nCloud-Optimized GeoTIFF (*.tif).\nZarr (*.zarr) (also in combination with NetCDF (*.nc).\n\nFor vector geospatial data we recommend the following file formats:\n\nFor data subsets: GeoJSON and/or KML.\nFor cloud-data serving: Geoparquet (*.geoparquet) and/or lance.\nFor visualization: Mapbox Vector Tiles (*.mvt).\nFor GIS analysis: Geopackage (*.gpkg).\n\nFor tabular data, objects as Simple Features and similar, the following file formats are accepted for delivery:\n\nComma Separated Value (*.csv) and GeoCSV best compressed as *.csv.gz.\nR RDS (*.rds) and/or QS files that can be red and written in parallel.\nGeoJSON (*.json).\n\nFor geospatial data to be FAIR, it should at least pass the following checks:\n\nIt is decision-ready, or at least analysis-ready (complete consistent optimized);\nIt is available in a professional catalog e.g. STAC catalog and/or Geonetwork;\nIt comes with technical documentation (ideally a peer-reviewed publication) / links to Github / Gitlab where users can find technical explanation of how was the data produced;\nIt has a version and each version has unique DOI;\nIt can be accessed directly i.e. file URL is available for HTTP requests (through S3 or similar).\n\ngdal_translate in.vrt out.tif -of COG -co TILED=YES -co COPY_SRC_OVERVIEWS=YES -co COMPRESS=LZW\nEach COG are quality controlled using (1) COG validator, (2) by visual inspection, (3) random sampling point overlay to certify that &gt;99% of pixels are available. After the quality control, all produced global mosaics can be registered and uploaded to S3 storage or similar.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#disclaimer-for-data-software-products",
    "href": "1-specifications.html#disclaimer-for-data-software-products",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "All public releases of new data / software should point to the generic AI4SoilHealth disclaimer:\n\n“Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or European Commision. Neither the European Union nor the granting authority can be held responsible for them. The data is provided “as is”. AI4SoilHealth project consortium and its suppliers and licensors hereby disclaim all warranties of any kind, express or implied, including, without limitation, the warranties of merchantability, fitness for a particular purpose and non-infringement. Neither AI4SoilHealth Consortium nor its suppliers and licensors, makes any warranty that the Website will be error free or that access thereto will be continuous or uninterrupted. You understand that you download from, or otherwise obtain content or services through, the Website at your own discretion and risk.”",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#land-mask",
    "href": "1-specifications.html#land-mask",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "Considering the land mask for pan-EU (see figure), we try to closely match the data coverage of Copernicus pan-european i.e.  the official selection of countries listed here. Note the mask covers also all EU integration candidate countries, but can eventually also be subset to only European Union countries.\n\n\n\n\n\nLandmask for the SHDC for pan-EU.\n\n\n\n\nThere are a total of three landmask files available, each of which is aligned with the standard spatial/temporal resolution and sizes of SHDC4EU specifications. Additionally, these files include a corresponding look-up table that provides explanations for the values present in the raster data.\nTable: Technical description of the pan-EU land mask.\n\n\n\nLandmask\n\n\nPurpose/principle\n\n\nThe basic principle to create the land mask is to include as much as land as possible, to avoid missing any land pixels and ensure precise differentiation between land, ocean and inland water bodies.\n\nWhen generating the land mask, the two reference datasets in a way that:\n\n\nIf either of the two reference datasets identifies a pixel as land, it is considered a land pixel in our mask.\n\nRegarding ocean and inland water bodies, a pixel is classified as a water pixel only when both reference datasets confirm its identification as water.\n\n\n\n\n\n\nReference datasets\n\n\n\n\nWorldCover, 10 m resolution.\n\nEuroGlobalMap, with shapefiles of administrative boundaries, inland water bodies, ocean and landmask.\n\n\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution landmasks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “min” in GDAL. This “min” method allows taking the minimum values from the contributing pixels, to keep as much land as possible.\n\n\n\n\nResolution available\n\n\n10-m, 30-m, 100-m, 250-m, and 1-km resolution\n\n\n\n\nMask values\n\n\n\n\n10: not in the pan-EU area, i.e. out of mapping scope\n\n1: land\n\n2: inland water\n\n3: ocean\n\n\n\n\n\n\nISO-3166 country code mask\n\n\nPurpose/principle\n\n\nIn this mask, each country is assigned a unique value, which allows for the interpretation and analysis of data associated with a specific country. The values are assigned to each country according to iso-3166 country code, which can be found in the corresponding look-up table.\n\n\n\n\nReference datasets\n\n\nEuroGlobalMap country shapefile\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution masks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n\n\n\n\nResolution available\n\n\n10m, 30m and 100m\n\n\n\n\nMask values\n\n\nCan be found in the corresponding look-up table.\n\n\n\n\nNUTS-3 mask\n\n\nPurpose/principle\n\n\nIn this raster file, each unique NUT3 level area is assigned a unique value, which allows for the interpretation and analysis of data associated with specific NUTS-3 regions. Compared to ISO-3166 country code mask, NUTS-3 mask shows more details about regional administrative boundaries.\n\n\n\n\nReference datasets\n\n\nEuropean NUTS-3 shapefile\n\n\n\n\nMosaic/resampling method\n\n\nThe coarse resolution masks (&gt;10 m) are generated by resampling from the 10m resolution base map using resampling method “mode” in GDAL.\n\n\n\n\nResolution available\n\n\n10 m, 30 m and 100 m\n\n\n\n\nMask values\n\n\nCan be found in the corresponding look-up table.\n\n\n\nAn important point to consider is that the ISO-code country mask provides a wider geographical coverage compared to the NUTS3 mask. This extended coverage includes countries such as Ukraine and others that lie beyond the NUTS3 administrative boundaries. Both the land mask and administrative code mask are in an Equal-area projection, allowing for accurate area estimation and facilitating aggregation per political/administrative unit.\nThese masks are published and shared in a publicly available way on Zenodo. The working version (v0.1) is available from: https://doi.org/10.5281/zenodo.8171860. The layers can be opened directly in QGIS by copying links from Zenodo. The scripts used to generate these masks can be found in our project Gitlab or via the public Github. If any issue / problem is noticed, please report.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#layer-submission-process-and-quality-control",
    "href": "1-specifications.html#layer-submission-process-and-quality-control",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "Partners on consortium are invited to submit working versions of data and services / share preliminary outputs internally via NextCloud or similar. Folder structure and more detailed instructions are provided by the WP lead OpenGeoHub. It is important however to distinguish between (1) internal releases, (2) public releases (partners responsible) and (3) public releases approved by the consortium.\nAll WP5 participants are required to register project outputs and inform other project participants about the progress. For internal documents / draft versions of outputs the following submission principles need to be followed:\n\nFor internal datasets and documents use project repositories, Gitlab, NextCloud and Mattermost to inform parties about the progress and receive feedback;\nRegularly inform your WP/Task group about the progress and planned dates (best via Mattermost channel).\nTo receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\nFor each new version of the output specify in short terms what has been improved, what has changed from last version.\nTag people in Mattermost that you would like to request to review the outputs.\n\nFor official releases please refer to these guidelines to avoid any delays. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\nFirst register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the minimum requirements regarding file formats and file naming, documentation and metadata.\nMake sure that no parts of the submission have any copyright issues.\nMake sure that all contributors are credited in detail.\nMake sure you provide appropriate disclaimer and terms of use; we recommend using “No warranty” as default.\nMake sure the license is compatible to the AI4SoilHealth project following our Consortium Agreement;\nMake sure acknowledgement (https://cordis.europa.eu/project/id/101086179) is clearly provided.\nMake sure you provide contact information and instructions for users to provide feedback and request support.\nMake sure all project partners are informed about the date of release (through Gitlab and Mattermost).\nIf there are any deviations from specifications you have put in the implementation plan, please indicate such deviations in the project management system.\n\nFor quality assurance (public releases) we recommend the following three checks for any new significant datasets:\n\nPreliminary data and code shared with all WP members via NextCloud / code on Gitlab / Github. Internal check in co-development sessions.\nPeer-review publications (as agreed in the proposal, all major new data products should go through peer-review).\nData and code exposed for public comments (https://github.com/ai4soilhealth) including the social media, especially Mastodon, X, Linkedin etc.\n\nIn principle, the publication process should be non-bureaucratic, agile and should not limit intonation and experimentation. However, for public releases approved by the consortium e.g. major deliverables as specified in the Grant Agreement, it is advised that these are approved by the Executive Board of the project so that the new dataset / service is then also officially promoted through media channels, landing page etc.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#reproducible-research",
    "href": "1-specifications.html#reproducible-research",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "In order to ensure FAIR outputs, it is highly recommended that all production steps used to generate maps are documented in code, best as Rmarkdown / Python Jupyter computational notebooks. As agreed also in the project proposal, All empirical data obtained follows strict and validated processes of monitoring and evaluation to avoid any potential error. The data produced in AI4SoilHealth will be made available as open data, following the FAIR principle. The project Data Management Plan (DMP) is available and has even more instructions.\nAI4SoilHealth has agreed to provide open access (OA) to research outputs (e.g., publications, data, software) through deposition in trusted repositories. Partners provide OA for peer-reviewed scientific publications relating to their results. Authors of all peer-reviewed scientific publications stores them in an OA trusted repository, during and after the project’s life following Article 17 and Annex 5 of the General Assembly. The consortium members are encouraged to publish in the Open Research Europe data platform, specifically via the Zenodo community for AI4SoilHealth (https://zenodo.org/communities/ai4soilhealth/).\nBesides OA publication, the project aims for early and open sharing of the soil health data, and the research and technological developments  including open data, open standards, open source software, and open communication:\n\nOpen data: The AI4SoilHealth consortium builds solutions upon open datasets published using genuinely open data licenses.\n\nOpen standards (interoperability): The use of open standards prevents lock-in by, or dependency on any single data, software or service supplier. AI4SoilHealth fully adopts the principles of FAIR data management and open standards to enable interoperability of methods and services, both within the project and beyond.\nOpen source software: AI4SoilHealth plans to release the mobile phone application on Github under the MIT license. All major outputs of the project should be made available via the project github (https://github.com/ai4soilhealth).\nOpen communication: The consortium has experience with organizing live, open, and free discussion forums and workshops.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "2-points.html",
    "href": "2-points.html",
    "title": "Point data import and quality control",
    "section": "",
    "text": "Under construction\nThis section provides detailed steps implemented to import and bind all soil laboratory / soil site data.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Point data import and quality control"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Soil Health Data Cube for pan-EU (SHDC4EU) is imagined as a compilation of pan-EU layers organized into a cloud-optimized data cube and served through an API / as analysis or decision ready data (in later phases of the project this data will be accessible also via a mobile phone app and various web-GIS applications). This is one of the main deliverables of AI4SoilHealth WP5 (“Harmonised EU-wide soil monitoring tools and services”) and is intended to cover most of the pan-EU mapping and cloud data services efforts. This document describes general development principles and specifications of the SHDC4EU and envisages the proposed optimal usage of the data cube. To read and understand what a data cube is and which technology is used to optimize access and usability, please refer to the medium article.\n\n\n\nWe provide code and examples of how to generate so-called Analysis-Ready training data sets. Some minimum conditions for a data set to be analysis ready include:\n\nIt requires no special pre-processing to remove artifacts, harmonize values within columns, bind or subset data;\nIt comes with extensive metadata so that there is no mistake about how was the data collected, prepared and distributed and by whom;\nIt is ready in some standard format e.g. Cloud Optimized GeoTIFF; for vector data sets we recommend using (open) cloud-native data formats to distribute data either Geopackage, Geoparquet, Flatgeobuf or similar;",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#who-this-book-is-for",
    "href": "preface.html#who-this-book-is-for",
    "title": "Preface",
    "section": "",
    "text": "Soil Health Data Cube for pan-EU (SHDC4EU) is imagined as a compilation of pan-EU layers organized into a cloud-optimized data cube and served through an API / as analysis or decision ready data (in later phases of the project this data will be accessible also via a mobile phone app and various web-GIS applications). This is one of the main deliverables of AI4SoilHealth WP5 (“Harmonised EU-wide soil monitoring tools and services”) and is intended to cover most of the pan-EU mapping and cloud data services efforts. This document describes general development principles and specifications of the SHDC4EU and envisages the proposed optimal usage of the data cube. To read and understand what a data cube is and which technology is used to optimize access and usability, please refer to the medium article.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#data-and-services",
    "href": "preface.html#data-and-services",
    "title": "Preface",
    "section": "",
    "text": "We provide code and examples of how to generate so-called Analysis-Ready training data sets. Some minimum conditions for a data set to be analysis ready include:\n\nIt requires no special pre-processing to remove artifacts, harmonize values within columns, bind or subset data;\nIt comes with extensive metadata so that there is no mistake about how was the data collected, prepared and distributed and by whom;\nIt is ready in some standard format e.g. Cloud Optimized GeoTIFF; for vector data sets we recommend using (open) cloud-native data formats to distribute data either Geopackage, Geoparquet, Flatgeobuf or similar;",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Preface"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This document provides information about the Soil Health Data Cube for pan-EU (SHDC4EU). The main purpose of the SHDC is to be as a platform for more detailed computing i.e. to estimate trends in important soil health indicators (e.g. Bare Soil Fraction, vegetation cover, chemical soil properties and similar). The SHDC is available via S3 (Simple Storage Service) and STAC as open data, which means that any researcher across EU can access data directly using rstac or similar, and fetch values / aggregate per polygon or farm. list will be continuously updated and extended. This document is continuously updated and new layers are continuously added.\n\nSoil Health Data Cube for pan-EU (SHDC4EU) is compilation of pan-EU layers organized into a data cube and served through an API as analysis or decision ready data. It will be published via the EcoDataCube.eu infrastructure and served with unrestricted access as a genuine open data project comparable to Copernicus Land Monitoring services, Zenodo.org, Amazon AWS Open Data and similar. It aims at serving the EU Soil Observatory hosted by the European Commission JRC. We are preparing a complete, consistent Data Cube which will include EU-wide seasonal crop-type maps, primary soil properties, land degradation indices, terrain parameters, and similar EO products, all at 30-m resolution and encompassing the period 2000–2024+, climatic time-series images (rainfall, soil moisture, surface temperature) and projected crop models (typically only at 1-km spatial resolution). These will then be combined into a unified complete consistent data cube that can be used to run machine learning models to detect and examine relationships between the field-estimated and EO-based SHI. The general workflow of using the SHDC4EU, in a nutshell, is: (1) assess the management and eventual degradation history of a piece of land, (2) assess current properties and states, and (3) assess potential of soil + land under different (climate change) scenarios. This is the basic framework which other WPs can use as a computing engine on top of which to build their particular applications. For processing: (1) data is used to assess the history of each pixel in terms of positive and negative trends in GPP, vegetation cover, SOC and other soil properties, (2) the most recent (actual) soil properties and classes will be provided to individual pixels / farms, and (3) the land potential will be estimated by extrapolating process-based and/or ML models to future climate land-use scenarios.\n\n\nCompiled data (imported, standardized, quality-controlled) is available through a diversity of standard formats:\n\nGridded data as COG files (compressed Cloud Optimized GeoTIFFs);\nVector data as GPKG files (Geopackage file ready to be opened in QGIS);\nTabular data is available either as CSV files or as Google Sheets;\n\nAll files can be downloaded using the STAC browser and/or using embedded links provided in this document.\n\n\n\nAI4SoilHealth project aims at buidling open development communities where data and code is exchange seamlessly under the Open Data / Open Source licenses (e.g. CC-BY, CC-BY-SA, ODbL, MIT, GPL and simlar). We are open to hosting contributed geospatial data produced by 3rd parties assuming that some minimum requirements are met. Currently, the minimum requirements to submit a dataset for inclusion to Soil Health Data Cube are:\n\nPan-EU coverage (or at least aiming at the global coverage) AND,\n\nLicense and terms of use clearly specified AND,\n\nComplete and consistent metadata that can ensure correct standardization and harmonization steps AND,\n\nPredictions of soil properties are Complete Consistent Current and Correct and based on the most significant data AND,\n\nNo broken or invalid URLs,\n\nData sets that do NOT satisfy the above listed minimum requirements might be removed. If you discover an issue with license, data description or version number of a dataset, please open a Github issue.\nRecommended settings for all datasets are:\n\nPeer-reviewed versions of the datasets (i.e. a dataset accompanied with a peer-reviewed publication) should have the priority,\n\nRegister your dataset (use e.g. https://zenodo.org/communities/ai4soilhealth/) and assign a DOI to each version,\n\nProvide enough metadata so that it can be imported and bind with other data without errors,\n\nIf your dataset is a compilation of previously published datasets, please indicate in the description,\n\nInformation outdated or missing? Please open an issue or best do a correction and then a pull request.\n\n\n\nThis work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.\n\n\n\nUse liability: OpenGeoHub foundations cannot provide any warranty as to the accuracy, reliability, or completeness of furnished data. Users assume responsibility to determine the usability of these data. The user is responsible for the results of any application of this data for other than its intended purpose.\nDistribution liability: OpenGeoHub foundations make no warranty, expressed or implied, regarding these data, nor does the fact of distribution constitute such a warranty. OpenGeoHub foundations cannot assume liability for any damages caused by any errors or omissions in these data. If appropriate, OpenGeoHub foundations can only certify that the data it distributes are an authentic copy of the records that were accepted for inclusion in the OpenGeoHub foundations archives.\nThe code and data described in this tutorial has been submitted for scientific review. Errors and artifacts are still possible. In case you spot an issue or artifact in maps and/or code, please report here, many thanks in advance!\n\n\n\nTo cite this document please use:\n@book{shdc4eu_2024,\n  author       = {Hengl, T., Minarik, R., Tian, X., Parente, L., Ho, Y.-F., Consoli, D., Simoes, R., and contributors},\n  title        = {{Soil Health Data Cube for pan-EU technical manual}},\n  year         = {2024},\n  publisher    = {OpenGeoHub foundation},\n  address      = {Doorwerth},\n  version      = {v0.1},\n  doi          = {10.5281/zenodo.13838797},\n  url          = {https://shdc.ai4soilhealth.eu/}\n}\n\n\n\n\nAI4SoilHealth “Accelerating collection and use of soil health information using AI technology to support the Soil Deal for Europe and EU Soil Observatory” is one of a group of Horizon Europe funded projects which fit under the EU’s Soil Health Mission for 2030. The 8 Mission objectives include:\n\nReduce desertification\nConserve soil organic carbon stocks\nStop soil sealing and increase re-use of urban soils\nReduce soil pollution and enhance restoration\nPrevent erosion\nImprove soil structure to enhance soil biodiversity\nReduce the EU global footprint on soils\nImprove soil literacy in society\n\nThe EU-funded AI4SoilHealth project will co-design, create and maintain an open access Europe-wide digital infrastructure founded on advanced AI methods combined with new and deep soil health understanding and measures. The AI-based data infrastructure will evolve a Soil Digital Twin. The project will deliver a coherent Soil Health Index methodology, Rapid Soil Health Assessment Toolbox, AI4SoilHealth Data Cube for Europe, Soil-Health-Soil-Degradation-Monitor, and AI4SoilHealth API and mobile phone app. AI4SoilHealth will test the tools, collecting feedback from target users.\n\n\n\nAI4SoilHealth.eu project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179.\nFunded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or European Research Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#download-compiled-data",
    "href": "index.html#download-compiled-data",
    "title": "Welcome",
    "section": "",
    "text": "Compiled data (imported, standardized, quality-controlled) is available through a diversity of standard formats:\n\nGridded data as COG files (compressed Cloud Optimized GeoTIFFs);\nVector data as GPKG files (Geopackage file ready to be opened in QGIS);\nTabular data is available either as CSV files or as Google Sheets;\n\nAll files can be downloaded using the STAC browser and/or using embedded links provided in this document.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#add-your-own-data",
    "href": "index.html#add-your-own-data",
    "title": "Welcome",
    "section": "",
    "text": "AI4SoilHealth project aims at buidling open development communities where data and code is exchange seamlessly under the Open Data / Open Source licenses (e.g. CC-BY, CC-BY-SA, ODbL, MIT, GPL and simlar). We are open to hosting contributed geospatial data produced by 3rd parties assuming that some minimum requirements are met. Currently, the minimum requirements to submit a dataset for inclusion to Soil Health Data Cube are:\n\nPan-EU coverage (or at least aiming at the global coverage) AND,\n\nLicense and terms of use clearly specified AND,\n\nComplete and consistent metadata that can ensure correct standardization and harmonization steps AND,\n\nPredictions of soil properties are Complete Consistent Current and Correct and based on the most significant data AND,\n\nNo broken or invalid URLs,\n\nData sets that do NOT satisfy the above listed minimum requirements might be removed. If you discover an issue with license, data description or version number of a dataset, please open a Github issue.\nRecommended settings for all datasets are:\n\nPeer-reviewed versions of the datasets (i.e. a dataset accompanied with a peer-reviewed publication) should have the priority,\n\nRegister your dataset (use e.g. https://zenodo.org/communities/ai4soilhealth/) and assign a DOI to each version,\n\nProvide enough metadata so that it can be imported and bind with other data without errors,\n\nIf your dataset is a compilation of previously published datasets, please indicate in the description,\n\nInformation outdated or missing? Please open an issue or best do a correction and then a pull request.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Welcome",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Welcome",
    "section": "",
    "text": "Use liability: OpenGeoHub foundations cannot provide any warranty as to the accuracy, reliability, or completeness of furnished data. Users assume responsibility to determine the usability of these data. The user is responsible for the results of any application of this data for other than its intended purpose.\nDistribution liability: OpenGeoHub foundations make no warranty, expressed or implied, regarding these data, nor does the fact of distribution constitute such a warranty. OpenGeoHub foundations cannot assume liability for any damages caused by any errors or omissions in these data. If appropriate, OpenGeoHub foundations can only certify that the data it distributes are an authentic copy of the records that were accepted for inclusion in the OpenGeoHub foundations archives.\nThe code and data described in this tutorial has been submitted for scientific review. Errors and artifacts are still possible. In case you spot an issue or artifact in maps and/or code, please report here, many thanks in advance!",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "Welcome",
    "section": "",
    "text": "To cite this document please use:\n@book{shdc4eu_2024,\n  author       = {Hengl, T., Minarik, R., Tian, X., Parente, L., Ho, Y.-F., Consoli, D., Simoes, R., and contributors},\n  title        = {{Soil Health Data Cube for pan-EU technical manual}},\n  year         = {2024},\n  publisher    = {OpenGeoHub foundation},\n  address      = {Doorwerth},\n  version      = {v0.1},\n  doi          = {10.5281/zenodo.13838797},\n  url          = {https://shdc.ai4soilhealth.eu/}\n}",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about-ai4soilhealth",
    "href": "index.html#about-ai4soilhealth",
    "title": "Welcome",
    "section": "",
    "text": "AI4SoilHealth “Accelerating collection and use of soil health information using AI technology to support the Soil Deal for Europe and EU Soil Observatory” is one of a group of Horizon Europe funded projects which fit under the EU’s Soil Health Mission for 2030. The 8 Mission objectives include:\n\nReduce desertification\nConserve soil organic carbon stocks\nStop soil sealing and increase re-use of urban soils\nReduce soil pollution and enhance restoration\nPrevent erosion\nImprove soil structure to enhance soil biodiversity\nReduce the EU global footprint on soils\nImprove soil literacy in society\n\nThe EU-funded AI4SoilHealth project will co-design, create and maintain an open access Europe-wide digital infrastructure founded on advanced AI methods combined with new and deep soil health understanding and measures. The AI-based data infrastructure will evolve a Soil Digital Twin. The project will deliver a coherent Soil Health Index methodology, Rapid Soil Health Assessment Toolbox, AI4SoilHealth Data Cube for Europe, Soil-Health-Soil-Degradation-Monitor, and AI4SoilHealth API and mobile phone app. AI4SoilHealth will test the tools, collecting feedback from target users.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Welcome",
    "section": "",
    "text": "AI4SoilHealth.eu project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179.\nFunded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or European Research Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Welcome"
    ]
  },
  {
    "objectID": "1-specifications.html#predictive-soil-mapping-based-on-machine-learning-and-hpc",
    "href": "1-specifications.html#predictive-soil-mapping-based-on-machine-learning-and-hpc",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "WP5 — with the tasks T5.2 Development of AI4SoilHealth computing engine and T5.3 Development of AI4SoilHealth Indicators data cube) — aims at implementing fully automated predictive soil mapping at high spatial resolution using large datasets (e.g. whole of Europe at 30-m spatial resolution). Prepared harmonized point datasets are overlaid against time-series of EO images and/or static terrain-based or similar indices. These are then be used to predict values of target variables in 2D, 3D and 2D+T, 3D+T. The general workflow for predictive soil mapping based on Spatiotemporal Machine Learning and High Performance Computing (HPC) and assumes that all point data is analysis-ready, representative and correlates with the EO / covariate layers i.e. can be used to produce usable spatial predictions. Automated predictive soil mapping is currently implemented via the Python library scikit-map using OpenGeoHub’s High Performance Computing infrastructure, and other workflows implemented in C++. The overall soil mapping framework is referred to as the EO-soilmapper (see figure below).\n\n\n\n\n\nEO-soilmapper is the OpenGeoHub’s predictive soil mapping flagship product. Image source: Tian et al. (2024).\n\n\n\n\nThe key interest of WP5 is to produce pan-EU predictions at the highest possible spatial resolution and that can directly serve soil health assessment at continental scale. Some variables are, however, not available for the whole of pan-EU, and some point datasets might be produced experimentally and used for testing purposes only. Hence, within the WP5, all soil variables are modeled / mapped under one the following 3 tiers:\n\nTier 1: Pan-EU, production ready variables: usually based on LUCAS + national soil datasets;\nTier 2 (internal, under construction): National / in-situ data: Tested locally, then upscaled to whole of EU; usually based on collaboration of WP3,4,5,6;\nTier 3 (internal, under construction): Pan-EU, new experimental variables (currently not in the LUCAS soil and most likely can not be modeled/mapped across EU, but can only be used to assess soil health at site); based on the collaboration of WP3,4,5;\n\nWe expect that project partners + pilots feed data for Tier 2 and Tier 3. However, not everything can be mapped across the EU, hence some variables will eventually stay available only locally and a small selection of variables will be available in-situ only i.e. these will not be mapped at all.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#standard-spatialtemporal-resolutions-and-support-sizes",
    "href": "1-specifications.html#standard-spatialtemporal-resolutions-and-support-sizes",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "For the sake of consistency and compatibility, project participants should use standard spatial resolutions to deliver and exchange data. Recommended standard pixel sizes / resolutions:\nTable: Standard bounding box and spatial resolutions.\n\n\n\nEurope COG\nBounding box\n\n\nContinental EU COG based on Copernicus\nSpatial resolutions and image size EPSG:3035\n\n\n\n\nXmin = 900,000\n\nYmin = 899,000\n\nXmax = 7,401,000\n\nYmax = 5,501,000\n\n\n10m | 650,100L x 460,200P\n\n25m | 260,040L x 184,080P\n\n30m | 216,700P x 153,400L\n\n100m | 65,010P x 46,020L\n\n250m | 26,004P x 18,408L\n\n1km | 6501P x 4602L\n\n\n\n\n\n\n\n\nTiling system (100-km blocks) used in the EO-soilmapper to run processing",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#file-naming-convention",
    "href": "1-specifications.html#file-naming-convention",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "SHDC consistently uses the standard OpenLandMap file-naming convention to submit new data-sets and similar (compare e.g. with the MODIS file naming convention). This is to ensure consistency and ease of use within the AI4SoilHealth project, but also by the end-users. This applies especially to WP4, WP5 and WP6.\nThe OpenLandMap file-naming convention works with 10 fields that basically define the most important properties of the data (this way users can search files, prepare data analysis etc, without even needing to access or open files. The 10 fields include:\n\nGeneric variable name (needs to be unique and approved by the AI4SoilHealth Coordination team): lclu;\nVariable procedure combination i.e. method standard (standard abbreviation): luisa;\nPosition in the probability distribution / variable type: c;\nSpatial support (usually horizontal block) in m or km: 30m;\nDepth reference or depth interval e.g. below (“b”), above (“a”) ground or at surface (“s”): s;\nTime reference begin time (YYYYMMDD): 20210101;\nTime reference end time: 20211231;\nBounding box (2 letters max): eu;\nEPSG code: epsg.3035;\nVersion code i.e. creation date: v20221015;\n\nAn example of a file-name based on the description above:\nlclu_luisa_c_30m_s_20210101_20211231_eu_epsg.3035_v20221015.tif\nNote that this file naming convention has the following properties:\n\nLarge quantities of files can be easily sorted and searched (one line queries in Bash).\nFile-naming patterns can be used to seamlessly build virtual mosaics and composites.\n\nKey spatiotemporal properties of the data are available in the file name e.g. variable type, O&M method, spatial resolution, bounding box, projection system, temporal references. Users can program analysis without opening or testing files.\nVersioning system is ubiquitous.\nAll file-names are unique.\n\nGeonetwork and STAC will be further used to link the unique file names to: (1) WPs, deliverables, themes / keywords, (2) DOI’s, (3) project homepages, (4) contact pages for support and feedback. For keywords we recommend using the INSPIRE keywords. To confirm that metadata is complete and consistent, we recommend using the INSPIRE metadata validator and/or https://data.europa.eu/en validator.\nSome simple additional rules for generating the file name include:\n\nCodes and abbreviations should be human-readable as much as possible (hence short, but not too short!);\nUse only English-US (en-us) language e.g. for months use jan, feb etc;\nConsistently use UNICODE standard: small letters only, no blank spaces, no non-ASCII characters;\nLimit the total file name size in characters to 256;\nFor time reference do not extend beyond hour minute and timezone;\nFor bounding boxes use as much as possible the 2–letter unique country code; for continents use the Equi7 Grid code i.e. eu,\nFor method codes use as much as possible unique IDs from ISO - ICS;\nFor MODIS products use consistently the MODIS products codes e.g. MOD11A2 v061; For long-term aggregates of seasonal, monthly, weekly values use the period name at the end of the method names (#3) for example the long-term estimate of MODIS LST daytime temperature for month August:\n\nlst.d_mod11a2v061.aug_m_1km_s_20200101_20211231_eu_epsg.3035_v20221015.tif\nA list of vocabularies to be used as abbreviated names of variables is provided by OpenGeoHub. The same file-name convention described above can be also used for vector data (this would only have a different file extension) also.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "1-specifications.html#registering-project-outputs",
    "href": "1-specifications.html#registering-project-outputs",
    "title": "Soil Health Data Cube specifications",
    "section": "",
    "text": "All project participants are required to register project outputs and inform other project participants about the progress (via https://zenodo.org/communities/ai4soilhealth/). For internal documents / draft versions of outputs the following submission principles need to be followed:\n\nFor internal datasets and documents use project Gitlab and Mattermost to inform parties about the progress and receive feedback;\nRegularly inform your WP about the progress and planned dates (best via Mattermost channel).\nTo receive internal feedback please use project Gitlab; this way ALL project participants can follow discussion and can possibly help resolve bugs / open issues.\nFor each new version of the output specify in short terms what has been improved, what has changed from last version.\nTag people in Mattermost that you would like to request to review the outputs.\n\nFor official releases please refer to the Consortium Agreement for the correct procedure. For every public release of the new official AI4SoilHealth data set, functionality and/or software, the following procedure should be followed closely:\n\nFirst register the produced data set, library, front-end via project management system (Gitlab) and confirm that the submission follows the minimum requirements regarding file formats and file naming, documentation and metadata.\nMake sure that no parts of the submission have any copyright issues.\nMake sure that all contributors are credited in detail.\nMake sure you provide appropriate disclaimer and terms of use; we recommend using “No warranty” as default.\nMake sure the license is compatible to the AI4SoilHealth project following our Consortium Agreement;\nMake sure acknowledgement (see below) is clearly provided.\nMake sure you provide contact information and instructions for users to provide feedback and request support.\nMake sure all project partners are informed about the date of release (through Gitlab and Mattermost).\n\nSuggested 3rd party platforms for registering outputs (we recommend using multiple of the listed options e.g. a, b, c etc):\n\nDatasets: https://zenodo.org/communities/ai4soilhealth/ and/or https://onedata.org/; metadata entry via: Geonetwork and/or STAC; structured summary via: https://ai4soilhealth.eu, suggestion: registered on https://data.europa.eu/en,\nSoftware: Github (https://github.com/ai4soilhealth) -&gt; suggestion: always generate DOI and put on Zenodo.org; https://archive.softwareheritage.org/, structured summary via: https://ai4soilhealth.eu,\nTutorials / data catalogs: webpages on github / gitlab (Rbookdown, python books) see e.g. https://opengeohub.github.io/SoilSamples/, MKdocs see e.g. https://gee-community-catalog.org/;\nData portals / web-GUI’s: sub-domain under *.ai4soilhealth.eu; recommended portal: https://gkhub.earthobservations.org/; video-tutorial published via: https://av.tib.eu/,\n\nWe are planning to have our own installation of Gitlab, STAC browser, Geonetwork and Pretalx. Final workflow for registering outputs will be specified in the final version of the implementation plan.\nDuring publishing of outputs, and especially if you register outputs via 3rd party repos, it of utmost importance that all partners use the correct attribution and links to the project homepage:\n\nCorrect attribution: “The AI4SoilHealth project project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179.”;\nProject URL: https://cordis.europa.eu/project/id/101086179.\nProject homepage: https://ai4soilhealth.eu,\nCorrect default disclaimer is below.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Soil Health Data Cube specifications"
    ]
  },
  {
    "objectID": "2-layers.html",
    "href": "2-layers.html",
    "title": "Available layers",
    "section": "",
    "text": "This section systematically list all layers provided within the AI4SoilHealth Soil Health Data Cube for Europe. Note, the list if constantly updated. Some layers are available on Zenodo and on S3, but most layers are only available on S3. The total size of all data is at the order of 20 to 30TB.\nIn principle, the data in SHDC is licensed under CC-BY, and the code is licensed under the MIT License. Training points are used under various terms and conditions.\n\n\n\nThree Pan-EU land masks designed for different specific applications in the production of soil health data cube:\n\nLand mask: with values differentiating land, ocean, and inland water\nNUT-3 code map: with values differentiating administrative area at nut-3 level\nISO-3166 country code map: with values differentiating countries according to ISO-3166 standard The jupyter notebooks and bash files that are used to produce masks, merge tiles, reproject coordinate systems, and resample to another resolution.\n\nAll the landmasks are aligned with the standard spatial/temporal resolution and sizes indicated/recommended by AI4SoilHealth project, WP5. The coverage of these maps closely match the data coverage of https://land.copernicus.eu/pan-european i.e. the official selection of countries listed here: https://lanEEA39d.copernicus.eu/portal_vocabularies/geotags/eea39.\nThese masks are created by Xuemeng, Yu-Feng, and Martijn from OpenGeoHub. If you spot any problems in the land masks, or see any possible improvements in them, or have any questions, or etc…, just raise an issue here or send us emails! We appreciate any feedbacks that could refine these masks.\n\n\n\n\nThis data cube offers a time-series of Landsat-based spectral indices maps across continental Europe including Ukraine, the UK, and Turkey from 2000 to 2022. At a resolution of 30-meters, it includes bi-monthly, annual, and long-term analyses, focusing on key aspects of soil health such as vegetation cover, soil exposure, tillage practices, and crop intensity. Apart from direct monitoring, analysis, and verification of specific aspects of soil health, this data cube also provides important input for modeling and mapping soil properties. All the maps are aligned with the standard spatial/temporal resolution and sizes indicated/recomended by AI4SoilHealth project, WP5.\nPlease cite as:\n@Article{tian2024time,\nAUTHOR = {Tian, X. and Consoli, D. and Witjes, M. and Schneider, F. and Parente, L. and \\c{S}ahin, M. and Ho, Y.-F. and Mina\\v{r}\\'{\\i}k, R. and Hengl, T.},\nTITLE = {Time-series of Landsat-based bi-monthly and annual spectral indices for continental Europe for 2000--2022},\nJOURNAL = {Earth System Science Data Discussions},\nVOLUME = {2024},\nYEAR = {2024},\nPAGES = {1--49},\nURL = {https://essd.copernicus.org/preprints/essd-2024-266/},\nDOI = {10.5194/essd-2024-266}\n}\n\n\nThe corresponding folder provides:\n\nEssential code & data used to generate/analyze/visualize/upload the landsat-based spectral indices data cube,\nVisualization for selected indices.\n\nThe indices include:\n\nVegetation index: Normalized Difference Vegetation Index (NDVI), Soil Adjusted Vegetation Index (SAVI), and Fraction of Absorbed Photosynthetically Active Radiation (FAPAR).\nSoil exposure: Bare Soil Fraction (BSF).\nTillage and soil sealing: Normalized Difference Tillage Index (NDTI) and minimum Normalized Difference Tillage Index (minNDTI).\nCrop patterns: Number of Seasons (NOS) and Crop Duration Ratio (CDR).\nWater dynamics: Normalized Difference Snow Index (NDSI) and Normalized Difference Water Index (NDWI)\n\nGeneral steps of maps production are described below:\n\n\n\n\n\nGeneral workflow for processing Landsat-based spectral index predictors. Image source: Tian et al. (2024).\n\n\n\n\nA preview of the BSF (%) time series for the former Szczakowa sand mine, south Poland is shown below.\n\n\n\n\n\nThe top panel provides zoomed-in view of NDVI, NDWI and BSF trends for the former Szczakowa sand mine, south Poland. Image source: Tian et al. (2024).\n\n\n\n\n\n\n\nTo ensure accessibility and proper usage of the dataset, we have distributed the data across multiple platforms for different purposes:\n\nZenodo https://zenodo.org/communities/ai4soilhealth\n\nThis dataset is registered on Zenodo with preview visualization and a valid DOI: https://doi.org/10.5281/zenodo.10776891.\nDue to the storage limit of Zenodo in each bucket, uploading all data layers to Zenodo is impractical and not beneficial for users as it would be too distributed. Therefore, for bimonthly predictors, only data layers for the years 2000 and 2022 are uploaded. All the annual and long-term predictors are available, though.\n\nWasabi.com cloud\n\nThe complete dataset is hosted on Wasabi’s cloud in COG format, enabling efficient storage, retrieval, and secure data management.\nA comprehensive index of all the data layers stored and maintained on Wasabi.com is available through a navigation catalog in a Google Sheet, facilitating the indexing, finding, and downloading of all the predictor layers.\n\n\n\n\n\nThe SHDC data was created by Xuemeng, Davide, Leandro, and Yu-Feng from OpenGeoHub. If you spot any problems in the maps, or see any possible improvements in them, or see any potential collaborations, or etc…, just raise an issue here or send us emails! We appreciate any feedbacks/helps that could refine them.\n\n\n\n\n\n\nThe folder soil_property_model_pipeline contains scripts used to build predictive models for 10 key soil properties:\n\nSoil Organic Carbon (SOC)\nNitrogen (N)\nCarbonate (CaCO3)\nCation Exchange Capacity (CEC)\nElectrical Conductivity (EC)\npH in Water\npH in CaCl2 Solution\nBulk Density\nExtractable Phosphorus (P)\nExtractable Potassium (K)\n\nThe notebooks and their content:\n\nNotebooks (001 – 009)\n\nDesigned to test various steps in the predictive model building process\nExplore and validate different methodologies and approaches for model construction\n\nBenchmark Pipeline Script\n\nbenchmark_pipeline.py automates the entire model-building pipeline\nStreamlines the process based on the initial 10 notebooks\n\nProperty-Specific Modeling (010 – 011)\n\nNotebooks with indices 010 -- 011 loop the pipeline through different soil properties\nIdentify and optimize the best model for each property\n\nPrediction Interval Models (012 – 014)\n\nNotebooks with indices 012 -- 014 build models that estimate prediction intervals\nAdd a layer of uncertainty quantification to the predictions\n\n\n\n\n\nPredictions of soil properties are based on the following training and input data:\n\nThe training data includes comprehensive metadata (sampling year, depth, location) and quality scores for each measurement, covering the above mentioned 10 properties. Details can be found in AI4SH soil data harmonization specifications.\nThe features used for model fitting and map production contain around 450 covariate layers. Details can be found in AI4SH soil health data cube covariates preparation. These layers comply with the technical specifications outlined in D5.1: Soil Health Data Cube, ensuring they are well-suited for integration, cross-comparison, and subsequent map production. The covariate layers include a diverse range of geospatial layers detailing various environmental conditions, categorized into:\n\nClimate\nLandsat-based spectral indices\nParental material\nWater cycle\nTerrain\nHuman pressure factors\n\n\n\n\n\nA standardized pipeline has been developed to automate model development for predicting soil properties. This pipeline enhances model performance through hyper-parameter tuning, feature selection, and cross-validation. The process begins with inputting harmonized soil data, covariate paths, and a defined quality score threshold to ensure data reliability. The inputs, processing steps and outputs are (Tian et al. 2024?):\n\nInput Data Preparation:\n\nHarmonized soil data\nList of covariate paths\nQuality score threshold\n\nModel Candidates:\n\nArtificial Neural Network (ANN)\nRandom Forest (RF)\nLightGBM\nWeighted variants (excluding ANN due to scikit-learn limitations)\n\nProcessing steps:\n\nSeparate calibration, training and testing dataset:\n\nValidation Dataset: 5000 soil points selected from LUCAS through stratified random sampling.\nCalibration Dataset: 20% of remaining soil data points selected in a stratified manner from each spatial block (approx. 120 km grids).\nTraining Dataset: remaining 80% soil data points.\n\nCalibration using calibration dataset\n\nFeature Selection: Using a default Random Forest (RF) model from scikit-learn.\nHyper-parameter Tuning: Using HalvingGridSearch from scikit-learn.\n\nCross-validation of base models on training dataset\n\nSpatial Blocking Strategy: in each run, it is ensured that geographically proximate (approx. 120 km grids) soil points are not selected together.\nMethod: 5-fold spatially blocked cross-validation (CV).\nMetrics: Coefficient of determination (R2), Root Mean Square Error (RMSE), Concordance Correlation Coefficient (CCC), and computation time.\n\nTesting on individual validation dataset\n\nAll 5 candidate models are trained on the whole training dataset.\nAnd then being tested on the individual validation dataset, to get a set of objective metrics.\n\n\nIntemediate outputs during process:\n\nProduces calibration and training datasets\nTrained models\nSorted feature importance\nPerformance metrics and accuracy plots\n\nFinal Model\n\nSelection: Model with the best overall performance across metrics for both CV and individual validation.\nTraining: Trained on the complete dataset of soil points using optimized features and parameters.\nQuantile Regression Model: A quantile model will be trained with same parameters on the complete data set to estimate prediction intervals.\nMap Production: Fully trained model used for soil property prediction and uncertainty map production.\n\n\n\n\n\nThese maps are created by Xuemeng, Rolf, Davide, Leandro, Robert and Yu-Feng from OpenGeoHub. If you spot any problems in the maps, or see any possible improvements in them, or see any potential collaborations, or etc…, just raise an issue here or send us emails! We appreciate any feedbacks/helps that could refine them.\n\n\n\n\n\n\nThe folder SOCD_map contains scripts used to test, train, evaluate predictive models for soil organic carbon density (SOCD, kg/m3) based on:\n\n22,428 lab measurements with both SOC content (g/kg) and fine earth (size &lt; 2mm) bulk density.\na wide range of environmental covariates, especially the time series of 30m Landsat-based spectral indices.\n\nThe scripts used to generate the figures in the paper are also included.\nPlease cite as:\n@Article{tian2025socd,\nAUTHOR = {Tian, X. and {de Bruin}, S. and Simoes, R. and Isik, M.S. and Mina\\v{r}\\'{\\i}k, R. and Ho, Y.-F. and Sahin, M. and Herold, M. and Consoli and Hengl, T.},\nTITLE = {Spatiotemporal prediction of soil organic carbon density for Europe (2000--2022) in 3D+T based on Landsat-based spectral indices time-series},\nJOURNAL = {PeerJ},\nVOLUME = {in review},\nYEAR = {2024?},\nPAGES = {1--49},\nDOI = {10.21203/rs.3.rs-5128244/v1}\n}\n\n\n\n\n\n\nThe folder WRB_map contains scripts used to test, train, evaluate predictive models to map soil types based on the IUSS World Reference Base classification system. For this we used (Minařı́k et al. 2024?):\n\ncca 19,000 training points with ground observation of soil types;\na wide range of environmental covariates, especially the time series of 30-m Landsat-based spectral indices;\n\nThe scripts used to generate the figures in the paper are also included.\nPlease cite as:\n@Article{minarik2025wrb,\nAUTHOR = {Mina\\v{r}\\'{\\i}k, R. and Hengl, T. and Simoes, R. and Isik, M.S. and Ho, Y.-F. and Tian, X.},\nTITLE = {Soil type (World Reference Base) map of Europe based on Ensemble Machine Learning and multiscale EO data},\nJOURNAL = {PeerJ},\nVOLUME = {in review},\nYEAR = {2024?},\nPAGES = {1--32},\nDOI = {xx}\n}\n\n\n\n\nThe production of these data layers are parts of AI4SoilHealth project. The AI4SoilHealth project project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or European Commision. Neither the European Union nor the granting authority can be held responsible for them. The data is provided “as is”. AI4SoilHealth project consortium and its suppliers and licensors hereby disclaim all warranties of any kind, express or implied, including, without limitation, the warranties of merchantability, fitness for a particular purpose and non-infringement. Neither AI4SoilHealth Consortium nor its suppliers and licensors, makes any warranty that the Website will be error free or that access thereto will be continuous or uninterrupted. You understand that you download from, or otherwise obtain content or services through, the Website at your own discretion and risk.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Available layers"
    ]
  },
  {
    "objectID": "2-layers.html#pan-eu-landmask",
    "href": "2-layers.html#pan-eu-landmask",
    "title": "Available layers",
    "section": "",
    "text": "Three Pan-EU land masks designed for different specific applications in the production of soil health data cube:\n\nLand mask: with values differentiating land, ocean, and inland water\nNUT-3 code map: with values differentiating administrative area at nut-3 level\nISO-3166 country code map: with values differentiating countries according to ISO-3166 standard The jupyter notebooks and bash files that are used to produce masks, merge tiles, reproject coordinate systems, and resample to another resolution.\n\nAll the landmasks are aligned with the standard spatial/temporal resolution and sizes indicated/recommended by AI4SoilHealth project, WP5. The coverage of these maps closely match the data coverage of https://land.copernicus.eu/pan-european i.e. the official selection of countries listed here: https://lanEEA39d.copernicus.eu/portal_vocabularies/geotags/eea39.\nThese masks are created by Xuemeng, Yu-Feng, and Martijn from OpenGeoHub. If you spot any problems in the land masks, or see any possible improvements in them, or have any questions, or etc…, just raise an issue here or send us emails! We appreciate any feedbacks that could refine these masks.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Available layers"
    ]
  },
  {
    "objectID": "2-layers.html#landsat-based-spectral-indices-data-cube",
    "href": "2-layers.html#landsat-based-spectral-indices-data-cube",
    "title": "Available layers",
    "section": "",
    "text": "This data cube offers a time-series of Landsat-based spectral indices maps across continental Europe including Ukraine, the UK, and Turkey from 2000 to 2022. At a resolution of 30-meters, it includes bi-monthly, annual, and long-term analyses, focusing on key aspects of soil health such as vegetation cover, soil exposure, tillage practices, and crop intensity. Apart from direct monitoring, analysis, and verification of specific aspects of soil health, this data cube also provides important input for modeling and mapping soil properties. All the maps are aligned with the standard spatial/temporal resolution and sizes indicated/recomended by AI4SoilHealth project, WP5.\nPlease cite as:\n@Article{tian2024time,\nAUTHOR = {Tian, X. and Consoli, D. and Witjes, M. and Schneider, F. and Parente, L. and \\c{S}ahin, M. and Ho, Y.-F. and Mina\\v{r}\\'{\\i}k, R. and Hengl, T.},\nTITLE = {Time-series of Landsat-based bi-monthly and annual spectral indices for continental Europe for 2000--2022},\nJOURNAL = {Earth System Science Data Discussions},\nVOLUME = {2024},\nYEAR = {2024},\nPAGES = {1--49},\nURL = {https://essd.copernicus.org/preprints/essd-2024-266/},\nDOI = {10.5194/essd-2024-266}\n}\n\n\nThe corresponding folder provides:\n\nEssential code & data used to generate/analyze/visualize/upload the landsat-based spectral indices data cube,\nVisualization for selected indices.\n\nThe indices include:\n\nVegetation index: Normalized Difference Vegetation Index (NDVI), Soil Adjusted Vegetation Index (SAVI), and Fraction of Absorbed Photosynthetically Active Radiation (FAPAR).\nSoil exposure: Bare Soil Fraction (BSF).\nTillage and soil sealing: Normalized Difference Tillage Index (NDTI) and minimum Normalized Difference Tillage Index (minNDTI).\nCrop patterns: Number of Seasons (NOS) and Crop Duration Ratio (CDR).\nWater dynamics: Normalized Difference Snow Index (NDSI) and Normalized Difference Water Index (NDWI)\n\nGeneral steps of maps production are described below:\n\n\n\n\n\nGeneral workflow for processing Landsat-based spectral index predictors. Image source: Tian et al. (2024).\n\n\n\n\nA preview of the BSF (%) time series for the former Szczakowa sand mine, south Poland is shown below.\n\n\n\n\n\nThe top panel provides zoomed-in view of NDVI, NDWI and BSF trends for the former Szczakowa sand mine, south Poland. Image source: Tian et al. (2024).\n\n\n\n\n\n\n\nTo ensure accessibility and proper usage of the dataset, we have distributed the data across multiple platforms for different purposes:\n\nZenodo https://zenodo.org/communities/ai4soilhealth\n\nThis dataset is registered on Zenodo with preview visualization and a valid DOI: https://doi.org/10.5281/zenodo.10776891.\nDue to the storage limit of Zenodo in each bucket, uploading all data layers to Zenodo is impractical and not beneficial for users as it would be too distributed. Therefore, for bimonthly predictors, only data layers for the years 2000 and 2022 are uploaded. All the annual and long-term predictors are available, though.\n\nWasabi.com cloud\n\nThe complete dataset is hosted on Wasabi’s cloud in COG format, enabling efficient storage, retrieval, and secure data management.\nA comprehensive index of all the data layers stored and maintained on Wasabi.com is available through a navigation catalog in a Google Sheet, facilitating the indexing, finding, and downloading of all the predictor layers.\n\n\n\n\n\nThe SHDC data was created by Xuemeng, Davide, Leandro, and Yu-Feng from OpenGeoHub. If you spot any problems in the maps, or see any possible improvements in them, or see any potential collaborations, or etc…, just raise an issue here or send us emails! We appreciate any feedbacks/helps that could refine them.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Available layers"
    ]
  },
  {
    "objectID": "2-layers.html#predictive-models-for-soil-properties",
    "href": "2-layers.html#predictive-models-for-soil-properties",
    "title": "Available layers",
    "section": "",
    "text": "The folder soil_property_model_pipeline contains scripts used to build predictive models for 10 key soil properties:\n\nSoil Organic Carbon (SOC)\nNitrogen (N)\nCarbonate (CaCO3)\nCation Exchange Capacity (CEC)\nElectrical Conductivity (EC)\npH in Water\npH in CaCl2 Solution\nBulk Density\nExtractable Phosphorus (P)\nExtractable Potassium (K)\n\nThe notebooks and their content:\n\nNotebooks (001 – 009)\n\nDesigned to test various steps in the predictive model building process\nExplore and validate different methodologies and approaches for model construction\n\nBenchmark Pipeline Script\n\nbenchmark_pipeline.py automates the entire model-building pipeline\nStreamlines the process based on the initial 10 notebooks\n\nProperty-Specific Modeling (010 – 011)\n\nNotebooks with indices 010 -- 011 loop the pipeline through different soil properties\nIdentify and optimize the best model for each property\n\nPrediction Interval Models (012 – 014)\n\nNotebooks with indices 012 -- 014 build models that estimate prediction intervals\nAdd a layer of uncertainty quantification to the predictions\n\n\n\n\n\nPredictions of soil properties are based on the following training and input data:\n\nThe training data includes comprehensive metadata (sampling year, depth, location) and quality scores for each measurement, covering the above mentioned 10 properties. Details can be found in AI4SH soil data harmonization specifications.\nThe features used for model fitting and map production contain around 450 covariate layers. Details can be found in AI4SH soil health data cube covariates preparation. These layers comply with the technical specifications outlined in D5.1: Soil Health Data Cube, ensuring they are well-suited for integration, cross-comparison, and subsequent map production. The covariate layers include a diverse range of geospatial layers detailing various environmental conditions, categorized into:\n\nClimate\nLandsat-based spectral indices\nParental material\nWater cycle\nTerrain\nHuman pressure factors\n\n\n\n\n\nA standardized pipeline has been developed to automate model development for predicting soil properties. This pipeline enhances model performance through hyper-parameter tuning, feature selection, and cross-validation. The process begins with inputting harmonized soil data, covariate paths, and a defined quality score threshold to ensure data reliability. The inputs, processing steps and outputs are (Tian et al. 2024?):\n\nInput Data Preparation:\n\nHarmonized soil data\nList of covariate paths\nQuality score threshold\n\nModel Candidates:\n\nArtificial Neural Network (ANN)\nRandom Forest (RF)\nLightGBM\nWeighted variants (excluding ANN due to scikit-learn limitations)\n\nProcessing steps:\n\nSeparate calibration, training and testing dataset:\n\nValidation Dataset: 5000 soil points selected from LUCAS through stratified random sampling.\nCalibration Dataset: 20% of remaining soil data points selected in a stratified manner from each spatial block (approx. 120 km grids).\nTraining Dataset: remaining 80% soil data points.\n\nCalibration using calibration dataset\n\nFeature Selection: Using a default Random Forest (RF) model from scikit-learn.\nHyper-parameter Tuning: Using HalvingGridSearch from scikit-learn.\n\nCross-validation of base models on training dataset\n\nSpatial Blocking Strategy: in each run, it is ensured that geographically proximate (approx. 120 km grids) soil points are not selected together.\nMethod: 5-fold spatially blocked cross-validation (CV).\nMetrics: Coefficient of determination (R2), Root Mean Square Error (RMSE), Concordance Correlation Coefficient (CCC), and computation time.\n\nTesting on individual validation dataset\n\nAll 5 candidate models are trained on the whole training dataset.\nAnd then being tested on the individual validation dataset, to get a set of objective metrics.\n\n\nIntemediate outputs during process:\n\nProduces calibration and training datasets\nTrained models\nSorted feature importance\nPerformance metrics and accuracy plots\n\nFinal Model\n\nSelection: Model with the best overall performance across metrics for both CV and individual validation.\nTraining: Trained on the complete dataset of soil points using optimized features and parameters.\nQuantile Regression Model: A quantile model will be trained with same parameters on the complete data set to estimate prediction intervals.\nMap Production: Fully trained model used for soil property prediction and uncertainty map production.\n\n\n\n\n\nThese maps are created by Xuemeng, Rolf, Davide, Leandro, Robert and Yu-Feng from OpenGeoHub. If you spot any problems in the maps, or see any possible improvements in them, or see any potential collaborations, or etc…, just raise an issue here or send us emails! We appreciate any feedbacks/helps that could refine them.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Available layers"
    ]
  },
  {
    "objectID": "2-layers.html#m-resolution-maps-of-socd-and-prediction-uncertainty-for-europe-20002022-in-3dt",
    "href": "2-layers.html#m-resolution-maps-of-socd-and-prediction-uncertainty-for-europe-20002022-in-3dt",
    "title": "Available layers",
    "section": "",
    "text": "The folder SOCD_map contains scripts used to test, train, evaluate predictive models for soil organic carbon density (SOCD, kg/m3) based on:\n\n22,428 lab measurements with both SOC content (g/kg) and fine earth (size &lt; 2mm) bulk density.\na wide range of environmental covariates, especially the time series of 30m Landsat-based spectral indices.\n\nThe scripts used to generate the figures in the paper are also included.\nPlease cite as:\n@Article{tian2025socd,\nAUTHOR = {Tian, X. and {de Bruin}, S. and Simoes, R. and Isik, M.S. and Mina\\v{r}\\'{\\i}k, R. and Ho, Y.-F. and Sahin, M. and Herold, M. and Consoli and Hengl, T.},\nTITLE = {Spatiotemporal prediction of soil organic carbon density for Europe (2000--2022) in 3D+T based on Landsat-based spectral indices time-series},\nJOURNAL = {PeerJ},\nVOLUME = {in review},\nYEAR = {2024?},\nPAGES = {1--49},\nDOI = {10.21203/rs.3.rs-5128244/v1}\n}",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Available layers"
    ]
  },
  {
    "objectID": "2-layers.html#m-resolution-maps-of-soil-types-wrb",
    "href": "2-layers.html#m-resolution-maps-of-soil-types-wrb",
    "title": "Available layers",
    "section": "",
    "text": "The folder WRB_map contains scripts used to test, train, evaluate predictive models to map soil types based on the IUSS World Reference Base classification system. For this we used (Minařı́k et al. 2024?):\n\ncca 19,000 training points with ground observation of soil types;\na wide range of environmental covariates, especially the time series of 30-m Landsat-based spectral indices;\n\nThe scripts used to generate the figures in the paper are also included.\nPlease cite as:\n@Article{minarik2025wrb,\nAUTHOR = {Mina\\v{r}\\'{\\i}k, R. and Hengl, T. and Simoes, R. and Isik, M.S. and Ho, Y.-F. and Tian, X.},\nTITLE = {Soil type (World Reference Base) map of Europe based on Ensemble Machine Learning and multiscale EO data},\nJOURNAL = {PeerJ},\nVOLUME = {in review},\nYEAR = {2024?},\nPAGES = {1--32},\nDOI = {xx}\n}",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Available layers"
    ]
  },
  {
    "objectID": "2-layers.html#disclaimer",
    "href": "2-layers.html#disclaimer",
    "title": "Available layers",
    "section": "",
    "text": "The production of these data layers are parts of AI4SoilHealth project. The AI4SoilHealth project project has received funding from the European Union’s Horizon Europe research an innovation programme under grant agreement No. 101086179. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or European Commision. Neither the European Union nor the granting authority can be held responsible for them. The data is provided “as is”. AI4SoilHealth project consortium and its suppliers and licensors hereby disclaim all warranties of any kind, express or implied, including, without limitation, the warranties of merchantability, fitness for a particular purpose and non-infringement. Neither AI4SoilHealth Consortium nor its suppliers and licensors, makes any warranty that the Website will be error free or that access thereto will be continuous or uninterrupted. You understand that you download from, or otherwise obtain content or services through, the Website at your own discretion and risk.",
    "crumbs": [
      "Project repositories",
      "Soil Health Data Cube components",
      "Available layers"
    ]
  },
  {
    "objectID": "3-points.html",
    "href": "3-points.html",
    "title": "Point Data Import and Quality Control",
    "section": "",
    "text": "Last update 19 November 2024\nThis section outlines the detailed procedures used to import and bind all soil laboratory and site data. The major components of the process are shown in Figure Figure 1, and are further explained in the corresponding subsections.",
    "crumbs": [
      "Project repositories",
      "Data use tutorials",
      "Point Data Import and Quality Control"
    ]
  },
  {
    "objectID": "3-points.html#materials",
    "href": "3-points.html#materials",
    "title": "Point data import and quality control",
    "section": "",
    "text": "This section outlines the detailed procedures used to import and bind all soil laboratory and site data. The following materials are involved in this process:\n\n\n\nInput: A folder named raw data is used to store raw data collected from multiple sources.\nOutput: A folder named data holds processed data, which includes intermediate data products and the final harmonized dataset.\nCode: A folder named soil-data contains Jupyter notebooks used for harmonizing the data. More details can be found in our GitHub repository.\n\n\n\n\nThe Google Sheet serves as the central platform for managing and documenting all soil data. It consists of multiple sheets, each serving specific functions: 1. Overview Sheet: Contains high-level information including data sources, data availability across sources and properties, and the spatio-temporal distribution of the whole dataset.\n\nProperty Sheets:\n\nEach sheet is named after a soil property involved in the harmonization process.\nIncludes details for each soil property such as measurement methods, units, and quality scores indicating the comparability of measurements to the standard LUCAS methodology.\nFeatures conversion equations used to standardize soil point data measurements to LUCAS points, along with their references, where applicable.",
    "crumbs": [
      "Project repositories",
      "Data use tutorials",
      "Point data import and quality control"
    ]
  },
  {
    "objectID": "3-points.html#standard-format-of-the-harmonized-soil-point-dataset",
    "href": "3-points.html#standard-format-of-the-harmonized-soil-point-dataset",
    "title": "Point data import and quality control",
    "section": "",
    "text": "The harmonized soil point dataset is presented in a tabular format. Each row in the table represents a sample collected from a specific depth and location in a given year. The table is organized into columns that store two main types of information about each sample:\n\n\n\nlat and lon: Latitude and longitude of the sample location.\ntime: Sampling year (finest granularity available).\nhzn_top and hzn_btm: Top and bottom of the soil horizon layer from which the sample is taken.\nid: Unique sample identifier; cross-reference with ref.\nref: Name of the data source.\nnuts0: Country where the data originates.\n\n\n\n\n\nocd (oc_iso.10694.1995.mg.cm3): Organic carbon density.\nsoc (oc_iso.10694.1995.wpct): Soil organic carbon.\ntotal.n (n.tot_iso.13878.1998.wpct): Total nitrogen.\ncarbonates (caco3_iso.10693.1995.wpct): Carbonate content.\nph.h2o (ph.h2o_iso.10390.2021.index): pH in H2O.\nph.cacl2 (ph.cacl2_iso.10390.2021.index): pH in CaCl2.\nbulk.density (bd.core_iso.11272.2017.g.cm3): Bulk density.\nextractable.p (p.ext_iso.11263.1994.mg.kg): Extractable phosphorus.\nextractable.k (k.ext_usda.nrcs.mg.kg): Extractable potassium.\ncec (cec.ext_iso.11260.1994.cmol.kg): Cation exchange capacity.\nec (ec_iso.11265.1994.ms.m): Electrical conductivity.\nsand (sand.tot_iso.11277.2020.wpct): Sand content.\nsilt (silt.tot_iso.11277.2020.wpct): Silt content.\nclay (clay.tot_iso.11277.2020.wpct): Clay content.\n\n\n\n\n\nEach measurement is assigned a quality score ranging from 0 to 10, where 10 represents the highest quality. This score reflects the comparability of the measurement to corresponding LUCAS data.",
    "crumbs": [
      "Project repositories",
      "Data use tutorials",
      "Point data import and quality control"
    ]
  },
  {
    "objectID": "3-points.html#lucas-points",
    "href": "3-points.html#lucas-points",
    "title": "Point data import and quality control",
    "section": "",
    "text": "This description, written in October 2024, is based on the LUCAS soil survey dataset downloaded earlier the same year. The LUCAS dataset is the cornerstone of the soil health data cube (SHDC) for Europe, organized by the Joint Research Centre (JRC). It represents the most comprehensive and consistent soil property dataset on a European scale (Orgiazzi et al., 2018).\n\n\n\nSub-surveys:\n\nLUCAS 2009\nLUCAS 2015\nLUCAS 2018\n\nSupplementary surveys:\n\nLUCAS 2012, mainly from Bulgaria and Romania\nLUCAS Switzerland\n\n\n\n\n\nThese properties are measured repeatedly across all sub-surveys: Organic carbon (soc), Nitrogen (total.n), Carbonate (Carbonates), pH in CaCl2 (ph.cacl2), pH in H2O (ph.h2o), Extractable phosphorus (extractable.p) and Extractable potassium (extractable.k). This repeat sampling setup in LUCAS, sampling at the same location across different years (sub-surveys), enables time series analysis of these properties. However, it’s important to note that the exact sampling locations vary slightly from survey to survey. As stated in the Data Evaluation of LUCAS Soil Survey Laboratory Data, 2020: &gt; “The sites are not marked and one may assume that the repeated samples are taken in the vicinity of the place where the previous sample was taken. Therefore, it should be interpreted as a repeated sample from an area, not a point.”\nOther properties are not consistently included in every survey. Soil texture components—clay, silt, and sand—are considered static; thus, they were measured only during LUCAS 2009/2012, LUCAS Switzerland, and partially in LUCAS 2015, when the point was first included in the LUCAS survey. Bulk density (bulk.density) measurements began with LUCAS 2018. Electrical conductivity (ec) is available for LUCAS 2015, LUCAS Switzerland, and LUCAS 2018, and Cation Exchange Capacity (cec) data are available only for LUCAS 2009/2012.",
    "crumbs": [
      "Project repositories",
      "Data use tutorials",
      "Point data import and quality control"
    ]
  },
  {
    "objectID": "3-points.html#general",
    "href": "3-points.html#general",
    "title": "Point Data Import and Quality Control",
    "section": "",
    "text": "This section outlines the detailed procedures used to import and bind all soil laboratory and site data. The major components of the process are shown in Figure: Workflow Diagram.\n\n\nPoint soil datasets are organized systematically to facilitate easy access and management. All raw data files are stored in a main folder named “raw data”. Within this folder, there are sub-folders for each country or region, named using the NUTS0 code (e.g., DE for Germany, UK for the United Kingdom, etc.).\n\n\n\nThe Google Sheet serves as the central platform for managing and documenting all soil data. It consists of multiple sheets, each serving specific functions:\n\nOverview Sheet: High-level information including data sources, data availability across sources and properties, and the spatio-temporal distribution of the entire dataset.\nProperty Sheets:\n\nEach sheet is named after a soil property involved in the harmonization process.\nDetails measurement methods, units, and quality scores indicating comparability of measurements to the standard LUCAS methodology.\nLists conversion equations used to standardize soil point data measurements to LUCAS points, along with references.\n\n\n\n\n\nThe code section includes Jupyter notebooks used for importing, quality control, harmonizing, and binding datasets into a single harmonized soil dataset. For the complete code, visit the GitHub repository.\n\n\nThe standardization of format (e.g., arrangement of rows/columns, naming conventions) is done individually for each dataset. Here’s an example using the GEMAS dataset.\nFirst import necessary libraries and define input and output paths.\n\n\nCode\n# Initialization Cell: Import libraries and set paths\nimport pandas as pd\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas as gpd\n\ninput_path = '/home/xuemeng/work_xuemeng/ai4sh_data.harmo/raw_data'\noutput_path = '/home/xuemeng/work_xuemeng/ai4sh_data.harmo/data_v2'\n\n\nThen load GEMAS data, check its format and content.\n\n\nCode\ngema = gpd.read_file(f'{input_path}/EU/GEMAS/GEMAS.csv')\ngema.head()\n\n\n\n\n\n\n\n\n\nID\nCOUNTRY\nUHDICM\nLHDICM\nC_ID\nTYPE\nXCOO\nYCOO\nXLAEA\nYLAEA\n...\nCd\nCu\nPb\nZn\nC_tot\nTOC\nCEC\npH_CaCl2\nLOI\ngeometry\n\n\n\n\n0\n3001\nGER\n0\n20\n39\nAp\n10.6667\n52.4167\n4366358.988\n3256570.511\n...\n0.212\n3.47\n14.91\n19.2\n1.36\n1.3\n9.3\n6.05\n3.67\nNone\n\n\n1\n3002\nSKA\n0\n20\n20\nAp\n19.608\n48.5683\n5028057.752\n2874475.41\n...\n0.1599\n4.94\n20.38\n50\n2.09\n2\n11.1\n4.9\n8.12\nNone\n\n\n2\n3003\nEST\n0\n20\n9\nAp\n22.5333\n58.58\n5046996.613\n4006167.894\n...\n0.1781\n6.49\n10.59\n44.1\n2.86\n2.8\n13.9\n6.2\n7.33\nNone\n\n\n3\n3004\nLIT\n0\n20\n9\nAp\n24.9375\n55.4631\n5258480.127\n3693116.32\n...\n0.1404\n5.22\n7.67\n19.4\n1.79\n1.8\n15.6\n6.81\n5.66\nNone\n\n\n4\n3005\nNOR\n0\n20\n9\nAp\n25.0675\n70.5008\n4884643.708\n5324544.679\n...\n0.358\n6.81\n6.99\n32.4\n10.4\n11\n24.7\n4.48\n22.82\nNone\n\n\n\n\n5 rows × 24 columns\n\n\n\nWhen converting GEMAS dataset to standard format, a temporary DataFrame is created to store the harmonized version of the GEMAS dataset. The soil property data is systematically organized into three components for each property available in the GEMAS dataset: 1. measured values; 2. measurement methods; 3. units.\n\n\nCode\ntemp = pd.DataFrame()\n\ntemp['ph.cacl2'] = gema['pH_CaCl2']  # unit info is not necessary for pH, therefore not recorded\ntemp['ph.cacl2_method'] = '0.01 M CaCl2'\n\ntemp['clay'] = gema['clay']\ntemp['silt'] = gema['silt']\ntemp = temp[['clay', 'silt']].apply(pd.to_numeric)\ntemp['sand'] = 100 - temp['clay'] - temp['silt']\ntemp['clay_unit'] = '%'\ntemp['sand_unit'] = '%'\ntemp['silt_unit'] = '%'\ntemp['clay_method'] = '&lt;2 μm, DIN-EN 725-5 (04/2007) / ISO 13320 (10/2009) by laser diffractometry.'\ntemp['sand_method'] = '20-2000 μm, DIN-EN 725-5 (04/2007) / ISO 13320 (10/2009) by laser diffractometry.'\ntemp['silt_method'] = '2-20 μm, DIN-EN 725-5 (04/2007) / ISO 13320 (10/2009) by laser diffractometry.'\n\ntemp['soc'] = gema['TOC']\ntemp['soc_unit'] = '%'\ntemp['soc_method'] = 'ISO 10694, Soil quality – Determination of organic and total carbon after dry combustion.'\n\ntemp['cec'] = gema['CEC']  # meq/100g = cmol/kg\ntemp['cec_unit'] = 'meq+/100 g'\ntemp['cec_method'] = 'Silver-thiourea method.'\n\ntemp['lc_survey'] = gema['TYPE']\ntemp.loc[temp['lc_survey'] == 'Gr', 'lc_survey'] = 'permanent grassland'\ntemp.loc[temp['lc_survey'] == 'Ap', 'lc_survey'] = 'arable land'\n\n\nAnother key component is meta information, which contains key details such as the sampling location, time, depth, NUTS0 region, reference information (to identify the data source), and unique identifiers (used within each data source to distinguish individual measurements).\n\n\nCode\ntemp['lat'] = gema['YCOO']  # coordinates\ntemp['lon'] = gema['XCOO']\ntemp['time'] = 2008   # time, GEMAS soil survey is conducted in 2008\ntemp['hzn_top'] = gema['UHDICM']   # top and bottom depth of the surveyed soil layers\ntemp['hzn_btm'] = gema['LHDICM']\ntemp['ref'] = 'gemas'   # reference column\ntemp['id'] = gema['ID']   # id used for each \"ref\"\n\n# Map country codes to NUTS0\ncountry_to_nuts0 = {\n    'GER': 'DE', 'SKA': 'SK', 'EST': 'EE', 'LIT': 'LT', 'NOR': 'NO',\n    'PTG': 'PT', 'POL': 'PL', 'SWE': 'SE', 'DEN': 'DK', 'ITA': 'IT',\n    'FRA': 'FR', 'FIN': 'FI', 'UKR': 'UA', 'CRO': 'HR', 'HEL': 'EL',\n    'HUN': 'HU', 'SPA': 'ES', 'CYP': 'CY', 'BEL': 'BE', 'UNK': 'UK',\n    'LAV': 'LV', 'SIL': 'SI', 'BUL': 'BG', 'SRB': 'RS', 'CZR': 'CZ',\n    'BOS': 'BA', 'FOM': 'MK', 'AUS': 'AT', 'NEL': 'NL', 'SLO': 'SK',\n    'IRL': 'IE', 'MON': 'ME', 'LUX': 'LU'\n}\ntemp['nuts0'] = gema['COUNTRY'] \ntemp['nuts0'] = gema['COUNTRY'].map(country_to_nuts0)\n\n\nBefore saving, the data is preliminarily cleaned and filtered using metadata to ensure all measurements have valid location, time, and depth information. Additionally, only measurements taken after the year 2000 are retained, aligning with the temporal scope of our modeling and mapping efforts.\n\n\nCode\nprint(f'{len(temp)} data in total')\n\nna = temp['time'].isna().sum()\nprint(f'{na} data with no time info')\n\nna = temp.loc[temp['time']&lt;2000]\nprint(f'{len(na)} data sampled before year 2000')\n\nif 'hzn_dep' in temp.columns:\n    na = temp['hzn_dep'].isna().sum()\nelse:\n    na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\nprint(f'{na} data with no depth info')\n\nna = len(temp[temp['lat'].isna() | temp['lon'].isna()])\nprint(f'{na} data with no coordinate info')\n\nif 'hzn_dep' in temp.columns:\n    temp = temp.dropna(subset=['time','hzn_dep','lat','lon'])\nelse:\n    temp = temp.dropna(subset=['time','hzn_btm','hzn_top','lat','lon'])\n\ntemp = temp.loc[temp['time']&gt;=2000]\nprint(f'{len(temp)} data left in total after filtering')\n\n\n4132 data in total\n0 data with no time info\n0 data sampled before year 2000\n0 data with no depth info\n0 data with no coordinate info\n4132 data left in total after filtering\n\n\nFinally, ensure that the data is saved securely in the correct folder.\n\n\nCode\ntemp.to_parquet(f'{output_path}/gemas_harmonized_l1.pq')\nprint(temp.shape)\n\n\n(4132, 24)\n\n\n\n\n\nThe harmonization of measured values aims to minimize discrepancies introduced by differing measurement methods across datasets. Each property is harmonized individually to ensure comparability to LUCAS, the benchmark standard. Below is an example of harmonizing SOC data.\n\n\n\n\nThe harmonized soil point dataset is presented in a tabular format. Each row in the table represents a sample collected from a specific depth and location in a given year. The table is organized into columns that store two main types of information about each sample:\n\nMeta Information\n\nlat and lon: Latitude and longitude of the sample location.\ntime: Sampling year (finest granularity available).\nhzn_top and hzn_btm: Top and bottom of the soil horizon layer from which the sample is taken.\nid: Unique sample identifier; cross-reference with ref.\nref: Name of the data source.\nnuts0: Country where the data originates.\n\nSoil Property Information\n\nocd (oc_iso.10694.1995.mg.cm3): Organic carbon density.\nsoc (oc_iso.10694.1995.wpct): Soil organic carbon.\ntotal.n (n.tot_iso.13878.1998.wpct): Total nitrogen.\ncarbonates (caco3_iso.10693.1995.wpct): Carbonate content.\nph.h2o (ph.h2o_iso.10390.2021.index): pH in H2O.\nph.cacl2 (ph.cacl2_iso.10390.2021.index): pH in CaCl2.\nbulk.density (bd.core_iso.11272.2017.g.cm3): Bulk density.\nextractable.p (p.ext_iso.11263.1994.mg.kg): Extractable phosphorus.\nextractable.k (k.ext_usda.nrcs.mg.kg): Extractable potassium.\ncec (cec.ext_iso.11260.1994.cmol.kg): Cation exchange capacity.\nec (ec_iso.11265.1994.ms.m): Electrical conductivity.\nsand (sand.tot_iso.11277.2020.wpct): Sand content.\nsilt (silt.tot_iso.11277.2020.wpct): Silt content.\nclay (clay.tot_iso.11277.2020.wpct): Clay content.\n\nMeasurement Quality Score\n\nEach measurement is assigned a quality score ranging from 0 to 10, where 10 represents the highest quality. This score reflects the comparability of the measurement to corresponding LUCAS data.",
    "crumbs": [
      "Project repositories",
      "Data use tutorials",
      "Point Data Import and Quality Control"
    ]
  },
  {
    "objectID": "3-points.html#raw-soil-data",
    "href": "3-points.html#raw-soil-data",
    "title": "Point Data Import and Quality Control",
    "section": "Raw Soil Data",
    "text": "Raw Soil Data\nPoint soil datasets are organized systematically to facilitate easy access and management. All raw data files are stored in a main folder named “raw data”. Within this folder, there are sub-folders for each country or region, named using the NUTS0 code (e.g., DE for Germany, UK for the United Kingdom, etc.).",
    "crumbs": [
      "Project repositories",
      "Data use tutorials",
      "Point Data Import and Quality Control"
    ]
  },
  {
    "objectID": "3-points.html#harmonization-sheet",
    "href": "3-points.html#harmonization-sheet",
    "title": "Point Data Import and Quality Control",
    "section": "Harmonization Sheet",
    "text": "Harmonization Sheet\nThe Google Sheet serves as the central platform for managing and documenting all soil data. It consists of multiple sheets, each serving specific functions:\n\nOverview Sheet: High-level information including data sources, data availability across sources and properties, and the spatio-temporal distribution of the entire dataset.\nProperty Sheets: Each sheet is named after a soil property involved in the harmonization process, see the example of soc in Figure Figure 2 sheet.\n\nsrc: Indicates the origin or source from which the data is obtained.\nmethod description: Provides the original description of the measurement method as documented, detailing how the data was measured.\ndata count: The number of data entries available for this measurement method from the specified source.\nquality note: A summarized interpretation of the method description, prepared to assist in assigning an appropriate quality flag.\nquality score: A designated score indicating the reliability of the data, helping determine whether it is suitable for use.\nconversion formula: Specifies any necessary conversion formula to standardize the data.\nconversion reference: Cites the literature or source that supports the choice of the conversion formula.\nnotes: Any supplementary information or remarks relevant to the data or its context.\n\n\n\n\n\n\n\n\nFigure 2: soc harmonization sheet",
    "crumbs": [
      "Project repositories",
      "Data use tutorials",
      "Point Data Import and Quality Control"
    ]
  },
  {
    "objectID": "3-points.html#code",
    "href": "3-points.html#code",
    "title": "Point Data Import and Quality Control",
    "section": "Code",
    "text": "Code\nThe code section includes Jupyter notebooks used for importing, quality control, harmonizing, and binding datasets into a single harmonized soil dataset. For the complete code, visit the GitHub repository.\n\nFormat Standardization: GEMAS Example\nThe standardization of format (e.g., arrangement of rows/columns, naming conventions) is done individually for each dataset. Here’s an example using the GEMAS dataset.\nFirst import necessary libraries and define input and output paths.\n\n\nCode\n# Initialization Cell: Import libraries and set paths\nimport pandas as pd\nimport os\nimport sys\nimport numpy as np\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas as gpd\nfrom eumap.misc import find_files, nan_percentile, GoogleSheet, ttprint\nmodule_path = '/home/xuemeng/work_xuemeng/ai4sh_data.harmo/soil-data/code/' # Add the folder containing harmonization_tool_kit.py to the system path\nif module_path not in sys.path:\n    sys.path.append(module_path)\nfrom harmonization_tool_kit import plot_subplots_histogram, conversion_and_score, plot_spatial_distribution\n\ninput_path = '/home/xuemeng/work_xuemeng/ai4sh_data.harmo/raw_data'\noutput_path = '/home/xuemeng/work_xuemeng/ai4sh_data.harmo/data_v2'\n\n\nThen load GEMAS data, check its format and content.\n\n\nCode\ngema = gpd.read_file(f'{input_path}/EU/GEMAS/GEMAS.csv')\ngema.head()\n\n\n\n\n\n\n\n\n\nID\nCOUNTRY\nUHDICM\nLHDICM\nC_ID\nTYPE\nXCOO\nYCOO\nXLAEA\nYLAEA\n...\nCd\nCu\nPb\nZn\nC_tot\nTOC\nCEC\npH_CaCl2\nLOI\ngeometry\n\n\n\n\n0\n3001\nGER\n0\n20\n39\nAp\n10.6667\n52.4167\n4366358.988\n3256570.511\n...\n0.212\n3.47\n14.91\n19.2\n1.36\n1.3\n9.3\n6.05\n3.67\nNone\n\n\n1\n3002\nSKA\n0\n20\n20\nAp\n19.608\n48.5683\n5028057.752\n2874475.41\n...\n0.1599\n4.94\n20.38\n50\n2.09\n2\n11.1\n4.9\n8.12\nNone\n\n\n2\n3003\nEST\n0\n20\n9\nAp\n22.5333\n58.58\n5046996.613\n4006167.894\n...\n0.1781\n6.49\n10.59\n44.1\n2.86\n2.8\n13.9\n6.2\n7.33\nNone\n\n\n3\n3004\nLIT\n0\n20\n9\nAp\n24.9375\n55.4631\n5258480.127\n3693116.32\n...\n0.1404\n5.22\n7.67\n19.4\n1.79\n1.8\n15.6\n6.81\n5.66\nNone\n\n\n4\n3005\nNOR\n0\n20\n9\nAp\n25.0675\n70.5008\n4884643.708\n5324544.679\n...\n0.358\n6.81\n6.99\n32.4\n10.4\n11\n24.7\n4.48\n22.82\nNone\n\n\n\n\n5 rows × 24 columns\n\n\n\nWhen converting GEMAS dataset to standard format, a temporary DataFrame is created to store the harmonized version of the GEMAS dataset. The soil property data is systematically organized into three components for each property available in the GEMAS dataset: 1. measured values; 2. measurement methods; 3. units.\n\n\nCode\ntemp = pd.DataFrame()\n\ntemp['ph.cacl2'] = gema['pH_CaCl2'] # unit info is not necessary for pH, therefore not recorded\ntemp['ph.cacl2_method'] = '0.01 M CaCl2'\n\ntemp['clay'] = gema['clay']\ntemp['silt'] = gema['silt']\ntemp = temp[['clay','silt']].apply(pd.to_numeric)\ntemp['sand'] = 100-temp['clay']-temp['silt']\ntemp['clay_unit'] = '%'\ntemp['sand_unit'] = '%'\ntemp['silt_unit'] = '%'\ntemp['clay_method'] = '&lt;2 μm, DIN-EN 725-5 (04/2007) / ISO 13320 (10/2009) bylaser diffractometry.'\ntemp['sand_method'] = '20-2000 μm, DIN-EN 725-5 (04/2007) / ISO 13320 (10/2009) bylaser diffractometry.'\ntemp['silt_method'] = '2-20 μm, DIN-EN 725-5 (04/2007) / ISO 13320 (10/2009) bylaser diffractometry.'\n\ntemp['soc'] = gema['TOC'] \ntemp['soc_unit'] = '%'\ntemp['soc_method'] = 'ISO standard 10694, Soilquality – determination of organic and total carbon after dry combustion'\n\ntemp['cec'] = gema['CEC'] # meq/100g = cmol/kg\ntemp['cec_unit'] = 'meq+/100 g'\ntemp['cec_method'] = 'silver-thiourea method'\n\n\ntemp['lc_survey'] = gema['TYPE']\ntemp.loc[temp['lc_survey']=='Gr','lc_survey'] = 'permanent grassland'\ntemp.loc[temp['lc_survey']=='Ap','lc_survey'] = 'arable land'\n\n\nAnother key component is meta information, which contains key details such as the sampling location, time, depth, NUTS0 region, reference information (to identify the data source), and unique identifiers (used within each data source to distinguish individual measurements).\n\n\nCode\ntemp['lat'] = gema['YCOO']  # coordinates\ntemp['lon'] = gema['XCOO']\ntemp['time'] = 2008   # time, GEMAS soil survey is conducted in 2008\ntemp['hzn_top'] = gema['UHDICM']   # top and bottom depth of the surveyed soil layers\ntemp['hzn_btm'] = gema['LHDICM']\ntemp['ref'] = 'gemas'   # reference column\ntemp['id'] = gema['ID']   # id used for each \"ref\"\n\n# Map country codes to NUTS0\ncountry_to_nuts0 = {\n    'GER': 'DE', 'SKA': 'SK', 'EST': 'EE', 'LIT': 'LT', 'NOR': 'NO',\n    'PTG': 'PT', 'POL': 'PL', 'SWE': 'SE', 'DEN': 'DK', 'ITA': 'IT',\n    'FRA': 'FR', 'FIN': 'FI', 'UKR': 'UA', 'CRO': 'HR', 'HEL': 'EL',\n    'HUN': 'HU', 'SPA': 'ES', 'CYP': 'CY', 'BEL': 'BE', 'UNK': 'UK',\n    'LAV': 'LV', 'SIL': 'SI', 'BUL': 'BG', 'SRB': 'RS', 'CZR': 'CZ',\n    'BOS': 'BA', 'FOM': 'MK', 'AUS': 'AT', 'NEL': 'NL', 'SLO': 'SK',\n    'IRL': 'IE', 'MON': 'ME', 'LUX': 'LU'\n}\ntemp['nuts0'] = gema['COUNTRY'] \ntemp['nuts0'] = gema['COUNTRY'].map(country_to_nuts0)\n\n\nBefore saving, the data is preliminarily cleaned and filtered using metadata to ensure all measurements have valid location, time, and depth information. Additionally, only measurements taken after the year 2000 are retained, aligning with the temporal scope of our modeling and mapping efforts.\n\n\nCode\nprint(f'{len(temp)} data in total')\n\nna = temp['time'].isna().sum()\nprint(f'{na} data with no time info')\n\nna = temp.loc[temp['time']&lt;2000]\nprint(f'{len(na)} data sampled before year 2000')\n\nif 'hzn_dep' in temp.columns:\n    na = temp['hzn_dep'].isna().sum()\nelse:\n    na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\nprint(f'{na} data with no depth info')\n\nna = len(temp[temp['lat'].isna() | temp['lon'].isna()])\nprint(f'{na} data with no coordinate info')\n\nif 'hzn_dep' in temp.columns:\n    temp = temp.dropna(subset=['time','hzn_dep','lat','lon'])\nelse:\n    temp = temp.dropna(subset=['time','hzn_btm','hzn_top','lat','lon'])\n\ntemp = temp.loc[temp['time']&gt;=2000]\nprint(f'{len(temp)} data left in total after filtering')\n\n\n4132 data in total\n0 data with no time info\n0 data sampled before year 2000\n0 data with no depth info\n0 data with no coordinate info\n4132 data left in total after filtering\n\n\nFinally, ensure that the data is saved securely in the correct folder.\n\n\nCode\ntemp.to_parquet(f'{output_path}/gemas.showcase_harmonized_l1.pq')\nprint(temp.shape)\n\n\n(4132, 24)\n\n\n\n\nMerge datasets.\nOnce all raw datasets are harmonized into a standard format, they will be merged into a single, unified dataset for further checks and harmonization.\nThe standardized datasets are saved using the naming pattern {raw dataset name}_harmonized_l1.pq, with which we could easily parse all the standardized datasets and then merge them together.\n\n\nCode\ndata_path = '/home/xuemeng/work_xuemeng/ai4sh_data.harmo/data_v2'  # define path of operation\n\n# parse the paths to all standarized datasets\ndef find_soil_dataset(directory):\n    listt = []\n    for dirpath,_,filenames in os.walk(directory):\n        for f in filenames:\n            if 'harmonized_l1.pq' in f:\n                listt.append(os.path.abspath(os.path.join(dirpath, f)))\n    return listt\n\npath_list = find_soil_dataset(data_path)\n\ndata_list = []\n# read in all standarized datasets and merge\nfor ip in path_list:\n    temp = pd.read_parquet(ip) #, dtype=dtype_dict\n    print('Dataset: ', ip.split('/')[-1].split('_')[0], 'with ', len(temp.columns), ' data records.')\n    data_list.append(temp)\n    \nprint()    \ndata = pd.concat(data_list)\n\n# unify the data format in each column\nmeta_cols = ['id', 'lat', 'lon', 'time', 'hzn_top', 'hzn_btm', 'ref', 'nuts0']  # columns to indicate meta information\nprop_cols = ['soc', 'carbonates', 'total.n', 'ph.h2o', 'ph.cacl2', 'bulk.density.tot', 'bulk.density.fe',\n             'clay', 'silt', 'sand', 'extractable.p', 'extractable.k','cec','ec','coarse.mass','coarse.vol']  # columns to indicate property values\nunit_cols = [i+'_unit' for i in prop_cols]  # columns to indicate the unit information for each property\nmtod_cols = [i+'_method' for i in prop_cols]  # columns to indicate the method used for each measurements\n\nnum_cols = ['lat', 'lon', 'time', 'hzn_top', 'hzn_btm'] + prop_cols\nfor col in num_cols:\n    data[col] = pd.to_numeric(data[col], errors='coerce')    \n    \nstr_cols = ['id'] + unit_cols + mtod_cols\nfor col in str_cols:\n    data[col] = data[col].astype(str)\n   \n# show some example rows \nsampled_rows = (\n    data[['ref','id']+prop_cols+unit_cols+mtod_cols].groupby('ref')\n    .apply(lambda x: x.sample(n=1))  # Sample one row per 'ref' group\n    .sample(n=5, random_state=42)   # Randomly choose 5 rows from the sampled groups\n    .reset_index(drop=True)         # Reset index for a clean DataFrame\n)\n\nsampled_rows\n\n\nDataset:  spain.ParcelasINES with  20  data records.\nDataset:  ukceh with  22  data records.\nDataset:  slovenia with  36  data records.\nDataset:  croatia with  45  data records.\nDataset:  gemas.showcase with  24  data records.\nDataset:  swiss.nabo with  50  data records.\nDataset:  geocradle with  21  data records.\nDataset:  estonia.kese with  25  data records.\nDataset:  foregs with  14  data records.\nDataset:  lucas with  57  data records.\nDataset:  germany with  39  data records.\nDataset:  portugal with  43  data records.\nDataset:  wales with  26  data records.\nDataset:  marsoc with  20  data records.\nDataset:  czech with  19  data records.\nDataset:  basque with  46  data records.\nDataset:  castilla.y.leon with  26  data records.\nDataset:  hunssd with  34  data records.\nDataset:  spain.ParcelasCOS with  17  data records.\nDataset:  bro.bhrp.nl with  23  data records.\nDataset:  gemas with  24  data records.\nDataset:  nl.bis with  32  data records.\nDataset:  sodah with  38  data records.\n\n\n\n\n\n\n\n\n\n\nref\nid\nsoc\ncarbonates\ntotal.n\nph.h2o\nph.cacl2\nbulk.density.tot\nbulk.density.fe\nclay\n...\nbulk.density.fe_method\nclay_method\nsilt_method\nsand_method\nextractable.p_method\nextractable.k_method\ncec_method\nec_method\ncoarse.mass_method\ncoarse.vol_method\n\n\n\n\n0\nBRO BHR-P\nbro_id: BHR000000346040, interval id: 12439\n3.300000\nNaN\nNaN\nNaN\nNaN\n1.421\nNaN\nNaN\n...\nnan\nNone\nNone\nNone\nnan\nnan\nnan\nnan\nNone\nnan\n\n\n1\nbis.anatol\nBPK - 334958\n4.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.00\n...\nNone\nfield estimation - pipette method - &lt;2 μm, mas...\nNone\nNone\nNone\nnan\nNone\nnan\nnan\nnan\n\n\n2\nParcelasINES\n16311\n0.590000\nNaN\nNaN\nNaN\nNaN\n1.310\nNaN\n24.26\n...\nnan\nContenido en Arcilla (&lt;0,002 mm)\nnan\nnan\nnan\nnan\nnan\nnan\nLaboratory-analyzed percentage by weight of co...\nnan\n\n\n3\nCastilla.y.Leon\nAcor - ACOR201608425\n1.850476\n5.384615\n-9999.0\nNaN\nNaN\n-9999.000\nNaN\n6.00\n...\nnan\nnan\nnan\nnan\nOlsen\nnan\nnan\nnan\nnan\nnan\n\n\n4\nforegs\nN25E13T2\n3.500000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n5 rows × 50 columns\n\n\n\nCheck once again on the validity of meta columns, if not, drop the corresponding rows.\n\n\nCode\n# time info\nprint('Check time info')\nold_time = data.loc[data['time']&lt;2000]\nprint(f'{len(old_time)} data sampled before year 2000')\n\nna_time = data.loc[data['time'].isna()]\nprint(f'{len(na_time)} data with invalid temporal information')\nprint()\n\n# coordinate\nprint('Check coordinates info')\nlen_ori = len(data)\n\ninf_condition = data['lat'] &gt; 75\nzero_condition = data['lat'] &lt;= 0\nnan_condition = (data['lat'].isna() | data['lon'].isna())\n\ncombined_condition = nan_condition | inf_condition | zero_condition\n\nprint(f\"{nan_condition.sum()} rows with nan coordinates, from {data.loc[nan_condition, 'ref'].unique()}\")\nprint(f\"{inf_condition.sum()} rows with inf coordinates, from {data.loc[inf_condition, 'ref'].unique()}\")\nprint(f\"{zero_condition.sum()} rows with zero coordinates, from \\n{data.loc[zero_condition, 'ref'].unique()}\")\n\ndata = data.loc[~combined_condition].reset_index(drop=True)\n\nprint(f\"Original length was {len_ori}, cleaned length is {len(data)}\")\nprint()\n\n# depth\nprint('Check depth info')\ndata.loc[data['hzn_dep'].isna(),'hzn_dep'] = (data.loc[data['hzn_dep'].isna(),'hzn_top']+data.loc[data['hzn_dep'].isna(),'hzn_btm'])/2 # derive mean depth info from bottom and top depth of layers\ndata = data.drop(columns = ['hzn_top','hzn_btm'])\nna_depth = data.loc[data['hzn_dep'].isna()]\nprint(f'{len(na_depth)} data with invalid depth information')\nprint()\n\n\nCheck time info\n0 data sampled before year 2000\n0 data with invalid temporal information\n\nCheck coordinates info\n2 rows with nan coordinates, from ['gemas']\n1246 rows with inf coordinates, from ['ParcelasINES' 'ukceh' 'thuenen.bze.lw']\n3 rows with zero coordinates, from \n['LUCAS']\nOriginal length was 413960, cleaned length is 412709\n\nCheck depth info\n0 data with invalid depth information\n\n\n\nDrop duplicates and save the data.\n\n\nCode\ndata_cleaned = data.drop_duplicates()\n\nduplicates = data.duplicated(keep=False)\nduplicated_df = data[duplicates]\n\ndup_src = duplicated_df['ref'].unique()\nprint(f'{len(duplicated_df)} duplicated rows, from {dup_src} datasets')\nprint(f'left {len(data_cleaned)} rows from origianl {len(data)} rows')\n\ndata.to_parquet(f'{data_path}/soil.showcase_meta.cleaned_l2.pq')\n\n\n10664 duplicated rows, from ['ParcelasINES' 'ukceh' 'multione' 'gemas' 'MarSOC' 'BRO BHR-P' 'SoDaH'] datasets\nleft 406300 rows from origianl 412709 rows\n\n\n\n\nQuality control: SOC example\nQuality control refers to the harmonization of measured values, involving the cleaning of erroneous and imcomplete data, conversion of data to a consistent unit, removing measurements that are neither comparable nor convertible to benchmark, and applying conversion formulas to transform convertible soil data to align with the bemchmark. The harmonization is performed property by property, using LUCAS as the benchmark. Below is an example of harmonizing SOC data.\n\n\nCode\n# Initialize the paths, variables and read in the merged dataset\nprop = 'soc'\nunit = f'{prop}_unit'\nmtod = f'{prop}_method'\n\ndata_path = '/home/xuemeng/work_xuemeng/ai4sh_data.harmo/data_v2'\ndata = pd.read_parquet(f'{data_path}/soil.showcase_meta.cleaned_l2.pq')\ndata.loc[:, :] = data.replace('None','nan')\nnone_exists = (data == 'None').any().any()  # make sure all the invalid values are represented by 'nan'\n\nsampled_rows = (\n    data[['ref', 'id', 'nuts0', prop, unit, mtod]].groupby('ref')\n    .apply(lambda x: x.sample(n=1))  # Sample one row per 'ref' group\n    .sample(n=5, random_state=42)   # Randomly choose 5 rows from the sampled groups\n    .reset_index(drop=True)         # Reset index for a clean DataFrame\n)\n\nsampled_rows\n\n\n\n\n\n\n\n\n\nref\nid\nnuts0\nsoc\nsoc_unit\nsoc_method\n\n\n\n\n0\nBRO BHR-P\nbro_id: BHR000000346063, interval id: 12551\nNL\n1.900000\n%\norganic matter, NEN5754v2014plusWENR - verhitt...\n\n\n1\nbis.anatol\nBPK - 20702\nNL\nNaN\nnan\nnan\n\n\n2\nParcelasINES\n2565\nES\n0.880000\n%\nWalkley-Black, OM in fine soil (&lt; 2 mm) is div...\n\n\n3\nCastilla.y.Leon\nInea - INEA20200355\nES\n2.015815\n%\nSOM, Materia Orgánica Porcentaje\n\n\n4\nforegs\nN27E12T1\nEL\n1.610000\n%\ntotal combustion in a 99.99 % oxygen flow at 1...\n\n\n\n\n\n\n\nFirstly, we standardize all measurements to g/kg for soc values.\n\n\nCode\n# check all the possible units of soc measurements we have\nprint(f'{prop} units: {data[unit].unique()}')\n\n# convert % to g/kg\ndata.loc[data[unit].isin(['%', '% KA']),prop] = data.loc[data[unit].isin(['%', '% KA']),prop]*10 # % -&gt; g/kg\n\n# now check which data source has unit = 'nan' \nmask_unit = (data[unit]=='nan') & (data[prop].notna())\nnan_ref = data.loc[mask_unit,'ref'].unique().tolist()\nprint(f'nan unit data from {nan_ref}')\n\n# let's check the distribution of the nan unit data, compare them with LUCAS, to see what's the unit of them\nplot_subplots_histogram(data, prop, 'ref', filt = nan_ref+['LUCAS'], value_range=None, bins=30)\n\nprint('Based on the range of values, it appears that the unit for the nan unit data is %.')\nmask_unit = (data[unit]=='nan') & (data[prop].notna())\ndata.loc[mask_unit,prop] = data.loc[mask_unit,prop]*10 # transform to g/kg\n\n\nsoc units: ['%' 'g/kg' 'nan' '% KA']\nnan unit data from ['geocradle', 'Czech']\n\n\n\n\n\n\n\n\n\nBased on the range of values, it appears that the unit for the nan unit data is %.\n\n\nThen we use the property harmonization information from Harmonization sheet to assign quality score and convert the convertable/comparable values to be comparable to LUCAS soc values.\n\n\nCode\n# read in the soc harmonization sheet\nkey_file = '/home/xuemeng/work_xuemeng/ai4sh_data.harmo/gaia-319808-913d36b5fca4.json'\nurl = 'https://docs.google.com/spreadsheets/d/1J652XU_VWmbm1uLmeywlF6kfe7fUD5aJrfAIK97th1E/edit#gid=1254448729'\ngsheet = GoogleSheet(key_file, url)\nmdf = gsheet.soc\n\n# Use this function to convert all convertible measurements and assign quality scores to each measurement. \n# After the conversion, a new quality score column 'soc_qa' is added.\ndff = conversion_and_score(data, mdf, prop) \n\nsampled_rows = (\n    dff[['ref', 'id', 'time','hzn_dep', f'{prop}_original', prop, f'{prop}_qa',mtod,'conversion formula', 'conversion ref',unit]].groupby('ref')\n    .apply(lambda x: x.sample(n=1))  # Sample one row per 'ref' group\n    .sample(n=5, random_state=42)   # Randomly choose 5 rows from the sampled groups\n    .reset_index(drop=True)         # Reset index for a clean DataFrame\n)\n\nsampled_rows\n\n\nMethod in GSheet for soc updated!\n['y = 1.3*x' 'y=x' 'y = 1.3*x/2' '' 'y=x/2' 'y = 1.15*x/2' nan\n 'y = 1.05*x' 'y = 0.965*x' 'y = 1.15*x']\n\n\n\n\n\n\n\n\n\nref\nid\ntime\nhzn_dep\nsoc_original\nsoc\nsoc_qa\nsoc_method\nconversion formula\nconversion ref\nsoc_unit\n\n\n\n\n0\nBRO BHR-P\nbro_id: BHR000000346025, interval id: 12362\n2014.0\n15.0\n39.0\n39.00\n2.0\norganic matter, NEN5754v2014plusWENR - verhitt...\n\n\n%\n\n\n1\nbis.anatol\nBPK - 329987\n2016.0\n12.5\n60.0\n60.00\n2.0\nfield estimation - SOM measured by LOI at 550°...\n\n\n%\n\n\n2\nParcelasINES\n22162\n2015.0\n5.0\n23.8\n30.94\n4.0\nWalkley-Black, OM in fine soil (&lt; 2 mm) is div...\ny = 1.3*x\n\n%\n\n\n3\nCastilla.y.Leon\nAsaja-SO - 2014_0320\n2014.0\n12.5\n14.3\n14.30\n1.0\nSOM, Materia Orgánica Porcentaje\n\n\n%\n\n\n4\nforegs\nN34E06T1\n2001.0\n12.5\nNaN\nNaN\n5.0\ntotal combustion in a 99.99 % oxygen flow at 1...\ny=x\n\n%\n\n\n\n\n\n\n\nOnce the conversion is complete, the data is cleaned based on its possible range. For soc, values must be positive and less than 1000.\n\n\nCode\n# Define the property column, assuming it's in a variable `prop`\nlower_limit, upper_limit = 0, 1000\n\n# Remove negative values (below lower limit) and print summary\nnan_counts_lower = dff[dff[prop]&lt; lower_limit].groupby('ref').size()\nprint(\"\\nNaN dff count per 'ref' from negative values:\")\nfor ref, count in nan_counts_lower.items():\n    print(ref, count)\n    \n# Remove values above the upper limit and print summary\nnan_counts_upper = dff[dff[prop]&gt; upper_limit].groupby('ref').size()\nprint(\"\\nNaN dff count per 'ref' from values exceeding upper limit:\")\nfor ref, count in nan_counts_upper.items():\n    print(ref, count)\n    \ndff.loc[dff[prop] &lt; lower_limit, prop] = np.nan\ndff.loc[dff[prop] &gt; upper_limit, prop] = np.nan\n\nplot_subplots_histogram(dff, prop, 'ref', filt=None, value_range=[lower_limit, upper_limit], bins=40)\n\n\n\nNaN dff count per 'ref' from negative values:\nCastilla.y.Leon 24\nCzech 956\nLUCAS 15\n\nNaN dff count per 'ref' from values exceeding upper limit:\nMarSOC 1\nestonia.kese 25\n\n\n\n\n\n\n\n\n\nMeasurements with a quality score below 2 (indicating they are not comparable to LUCAS) are set to NaN. Finally, all unnecessary columns are dropped, keeping only the quality score column.\n\n\nCode\ndff.loc[dff[f'{prop}_qa']&lt;=2,prop] = np.nan\ndff = dff.drop(columns=['src','conversion formula', 'conversion ref',f'{prop}_original',unit,mtod])\ndff.to_parquet(f'{data_path}/soil.showcase_soc.cleaned.o1_l2.pq')\n\nplot_spatial_distribution(dff, title = 'harmonized soc')",
    "crumbs": [
      "Project repositories",
      "Data use tutorials",
      "Point Data Import and Quality Control"
    ]
  },
  {
    "objectID": "3-points.html#harmonized-soil-dataset",
    "href": "3-points.html#harmonized-soil-dataset",
    "title": "Point Data Import and Quality Control",
    "section": "Harmonized soil dataset",
    "text": "Harmonized soil dataset\nThe harmonized soil point dataset is presented in a tabular format. Each row in the table represents a sample collected from a specific depth and location in a given year. The table is organized into columns that store two main types of information about each sample:\n\nMeta Information\n\nlat and lon: Latitude and longitude of the sample location.\ntime: Sampling year (finest granularity available).\nhzn_dep: Derived from the mean of top and bottom of the sampled soil horizon layer.\nref: Name of the data source.\nid: Unique sample identifier; cross-reference with ref.\nnuts0: Country where the data originates.\n\nSoil Property Information\n\nsoc (oc_iso.10694.1995.wpct): Soil organic carbon.\nsand (sand.tot_iso.11277.2020.wpct): Sand content.\nsilt (silt.tot_iso.11277.2020.wpct): Silt content.\nclay (clay.tot_iso.11277.2020.wpct): Clay content.\ncoarse.mass: coarse fraction in mass.\ncoarse.vol: coarse fraction in volume.\nbulk.density.tot (bd.core_iso.11272.2017.g.cm3): Bulk density of total bulk sample.\nbulk.density.fe: Bulk density of fine earth (size &lt; 2mm).\nocd (oc_iso.10694.1995.mg.cm3): Organic carbon density.\nph.h2o (ph.h2o_iso.10390.2021.index): pH in H2O.\ntotal.n (n.tot_iso.13878.1998.wpct): Total nitrogen.\ncarbonates (caco3_iso.10693.1995.wpct): Carbonate content.\nph.cacl2 (ph.cacl2_iso.10390.2021.index): pH in CaCl2.\nextractable.p (p.ext_iso.11263.1994.mg.kg): Extractable phosphorus.\nextractable.k (k.ext_usda.nrcs.mg.kg): Extractable potassium.\ncec (cec.ext_iso.11260.1994.cmol.kg): Cation exchange capacity.\nec (ec_iso.11265.1994.ms.m): Electrical conductivity.\n\nMeasurement Quality Score\n\nEach measurement is assigned a quality score ranging from 0 to 5, where 5 represents the highest quality. This score reflects the comparability of the measurement to corresponding LUCAS data. Details of quality score see in Harmonization sheet - ReadMe.",
    "crumbs": [
      "Project repositories",
      "Data use tutorials",
      "Point Data Import and Quality Control"
    ]
  }
]